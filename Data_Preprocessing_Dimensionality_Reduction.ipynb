{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5effec6-ad52-4769-8447-4aac8e0cf62b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains all of the scripts necessary to preprocess the raw datasets for the dimensionality reduction (DR) part of the project. The notebook follows the following 4 steps:\n",
    "\n",
    "Step 1: Isolates the features and participants of interest from each dataset containing the clinical, demographic, sMRIs, fMRI, and dMRI, see the participants section of the methodology for inclusion and exclusion criteria and Table 1 for more information on the features. The functions clean_dMRI, clean_sMRI_vol, clean_sMRI_thickness, under the 'Cleaning Functions' header are created to clean each of the datasets and are combined in the function make_base_df to create the base HC & pwMS and pwMS only datasets.\n",
    "\n",
    "Step 2: Splits the newly created dataframes into train and test subjects (80%-20%, respectively). Stratified subject split is used to obtain a repreentative distribution of either the HC and pwMS by creating a new temporary variable with the following classes: HC, RRMS, SPMS, and PPMS, to represent the new splits. The functions get_test_subjects and make_train_test_split are used to obtain the train and test subject IDs such that there is no data leakage between the train and test dataset and only participants belonging to one of the classes are selected for the test set.\n",
    "\n",
    "Step 3: Missing values are imputed with either 1 of 3 methods (Time, Time + Type, and Time + Neighbor), see the 'Missing Values' subsection of the Methodology for more information on these imputation methods. The function split_df_by_time splits the previously created train and test base dataframes into 1 unique session per dataframe and returns these splits in a list. Each unique session dataframe is then passed on to the impute_df function which fills the missing value with the specified imputatin method. The sessions for the test datasets are merged to obtain a dataframe containing all the sessions. In combination with the imputated dataframes with unique sessions, the test dataframes are saved and reserved for later use in the dimensionality reduction methods as they will not undergo oversampling. \n",
    "\n",
    "Step 4: The class imbalance between the MS phenotypes and HC / pwMS is fixed using Synthetic Minority Over-sampling Technique (SMOTE), more information with regards to SMOTE can be found in the subsubsection 'Class Imbalance' of the methodology. The lists of unique session dataframes created in the previous imputation step are converted from an X row per participants for the X sessions to a 1 row per participant format. The datasets are then used to synthesize new participants in the class_rebalancing_SMOTE function, using SMOTE based on the MS phenotypes column for MS datasets and the HC + MS phenotypes (HC, RRMS, SPMS, PPMS) for the HC + pwMS datasets, which are added to the existing participants. The new oversampled dataframes are then reformated to the original X rows per participant for the X sessions and saved in the reformat_SMOTE_dfs function. A combined session version of the oversampled dataset is also created and saved. This is repeated for all versions of the MS and ALL train datasets obtained from the different imputation methods.\n",
    "\n",
    "These datasets will then be used in part 1 of the project for dimensionality reduction with PCA, tSNE, UMAP, and TPHATE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b45f60",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ef863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc188d3",
   "metadata": {},
   "source": [
    "# Step 1: Prepare & Merge the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b088766",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf89a88-e6ab-4af0-98aa-bae7fac6d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dMRI(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df :(dataframe) dataset with dMRI data\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Sets up the dMRI dataset for merging with others in the make_base_df function. Isolates the no lession zscores \n",
    "    (FD, FDC, and logFC), subject IDs and sessions number.\n",
    "    \"\"\"\n",
    "    # Create a list with all columns beginning with FD or Logfc in the df\n",
    "    all_dMRI_cols = [col for col in df.columns if col.startswith('FD') or col.startswith('Logfc')]\n",
    "    \n",
    "    # Keep only columns with _nolesion_zscores\n",
    "    keep_dMRI_cols = [col for col in all_dMRI_cols if col.endswith('_nolesion_zscores')]\n",
    "\n",
    "    # Get the columns to remove based on wanted columns\n",
    "    rm_cols = [col for col in df.columns if col not in ['PRESGENE_ID', 'Time'] + keep_dMRI_cols]\n",
    "\n",
    "    # Remove the unwanted columns from the df\n",
    "    output_df = df.drop(columns = rm_cols, axis = 1)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f56046-b90b-44e9-8124-cd2376f37915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sMRI_vol(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df :(dataframe) dataset with demographic, clinical and structural MRI dataset.\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Sets up the sMRI (volumes) dataset for merging with others in the make_base_df function. \n",
    "    Retains the clinical, demographic, and sMRI volume features. Removes rows and columns with at least 90% of missing values.\n",
    "    \"\"\"\n",
    "    # Make a copy of the input df to prevent editing the original df\n",
    "    working_df = df.copy()\n",
    "    \n",
    "    #Remove raw crossectional measures\n",
    "    clin_demo = ['id_presgene','programs_label_code','edss', 'mstype_code', 'subject_type_code',\n",
    "                 'age',\t'sex',\t'average_cognition', 'HC_CI_CP']\n",
    "    \n",
    "    vol_long_corr = [col for col in working_df.columns if 'Vol_long_all_corrected' in col and 'Mask' not in col and 'BrainSeg' not in col]\n",
    "\n",
    "    working_df = working_df[[col for col in working_df.columns if col in clin_demo + vol_long_corr + ['lesion_volume_mm3']]]\n",
    "\n",
    "    # Replace other missing type notation with the most common MStype\n",
    "    working_df['mstype_code'] = working_df['mstype_code'].apply(lambda x: 3 if x > 8 else x)\n",
    "\n",
    "    # Remove columns filled with at least 90% missing values or zeros:\n",
    "    nan_prct = working_df.isnull().mean()\n",
    "    zr_prct = (working_df == 0).mean()\n",
    "    prct = nan_prct+zr_prct\n",
    "    nozr_df = working_df.loc[:, prct < 0.9]\n",
    "\n",
    "    zr_col = prct[prct >= 0.9].index.tolist()\n",
    "    print(\"Columns removed:\", zr_col)\n",
    "\n",
    "    # Remove rows filled with at leasst 90% missing values or zeros and any of their occurances:\n",
    "    na_prct_row = nozr_df.isnull().mean(axis=1)\n",
    "    zr_prct_row = (nozr_df == 0).mean(axis=1)\n",
    "    prct_row = na_prct_row + zr_prct_row\n",
    "    zr_row = nozr_df.loc[prct_row >= 0.8, 'id_presgene'].tolist()\n",
    "    nozr_df =  nozr_df[~nozr_df['id_presgene'].isin(zr_row)]\n",
    "\n",
    "    print(\"Rows with ID removed:\", zr_row)\n",
    "\n",
    "    # Encode the HC_CI_CP to numerical labels\n",
    "    nozr_df['HC_CI_CP'] = nozr_df['HC_CI_CP'].fillna(9999)\n",
    "    nozr_df['HC_CI_CP'] = nozr_df['HC_CI_CP'].replace({'HC': 0, 'CP': 1, 'CI': 2})\n",
    "    nozr_df['HC_CI_CP'] = nozr_df['HC_CI_CP'].replace(9999, np.nan)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    rename_dict = {'id_presgene':'PRESGENE_ID',\n",
    "                   'programs_label_code': 'Time',\n",
    "                   'edss': 'EDSS',\n",
    "                   'mstype_code':'MStype',\n",
    "                   'subject_type_code':'MS',\n",
    "                   'age': 'Age',\n",
    "                   'sex': 'Sex',\n",
    "                   'average_cognition':'BL_Avg_cognition'}\n",
    "                          \n",
    "    nozr_df = nozr_df.rename(columns = rename_dict)\n",
    "\n",
    "    print(f\"The sMRI volumes dataset is ready for merging\")\n",
    "    \n",
    "    return nozr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507d62a-bfc7-4d5a-bc89-eaf7bca70f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_PRESGENE(df_list):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df_list : (list of dataframes) list containing 2 sMRI (thickness) datasets representing left & right hemisphere measurements\n",
    "    OUTPUT: 2 dataframes of left and right hemisphere data.\n",
    "    DESCRIPTION: Reformats the ID column to match that of the PRESGENE_ID, and identifies the left and right hemisphere dataset.\n",
    "    \"\"\"\n",
    "    lh_df = None\n",
    "    rh_df = None\n",
    "    for df in df_list:\n",
    "        old_id = df.columns[0]\n",
    "        id_col = 'PRESGENE_ID'\n",
    "        # Restructure ID column\n",
    "        df = df.rename(columns = {df.columns[0] : id_col})\n",
    "        df[id_col] = df[id_col].astype(str)\n",
    "        df[id_col] = df[id_col].replace('^sub-', '', regex=True)\n",
    "        df[id_col] = df[id_col].str[:4]+ '_' + df[id_col].str[4:]\n",
    "\n",
    "        # Remove the BrainSegVolNotVent & eTIV columns:\n",
    "        df.drop(columns = ['BrainSegVolNotVent', 'eTIV'], inplace=True)\n",
    "\n",
    "        if 'lh' in old_id:\n",
    "            lh_df = df\n",
    "        elif 'rh' in old_id:\n",
    "            rh_df = df\n",
    "        else:\n",
    "            print('Neither left nor right hemisphere files were provided.')\n",
    "        \n",
    "    return lh_df, rh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a150d7-b95d-40dd-82a7-a7f3d1a5d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sMRI_thickness(df_list, num_ses):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df_list : (nested list of dfs) Each session has its own sublist containing the left and right hemisphere sMRI thickness \n",
    "    data within the main list.\n",
    "    num_ses : (integer) number of sessions to include in the merging of the sMRI thickness datasets\n",
    "    OUTPUT: 1 dataframe\n",
    "    DESCRIPTION: Sets up the sMRI (thickness) individual datasets (per session and brain hemisphere) for merging with others in the \n",
    "    make_base_df function. Uses the parameter num_ses session to combine the left and right hemisphere data up to the desired session\n",
    "    (baseline = 1, five year follow-up = 2, ten year follow-up = 3).\n",
    "    \"\"\"\n",
    "    # Initialise the output variable\n",
    "    output_df = None\n",
    "\n",
    "    # Iterate over the list to reformat the ID column and merge the left and right hemisphere measurements up to desired session\n",
    "    for i in range(0, num_ses):\n",
    "        # Reformat\n",
    "        presgene_lh, presgene_rh = format_to_PRESGENE(df_list[i])\n",
    "        \n",
    "        # merge lh & rh datasets & add time column\n",
    "        lh_rh_df = pd.merge(presgene_lh, presgene_rh, on=['PRESGENE_ID'], how='inner')\n",
    "        lh_rh_df['Time'] = i + 1\n",
    "\n",
    "        # Update output_df to baseline session for the first iteration\n",
    "        if i == 0:\n",
    "            # if baseline (_00)\n",
    "            output_df = lh_rh_df\n",
    "        \n",
    "        # Combine previous output_df with left & right hemisphere obtained for current session\n",
    "        else:\n",
    "            # Find common participant IDs between the 2 sessions\n",
    "            common_ids = pd.merge(output_df[['PRESGENE_ID']], lh_rh_df[['PRESGENE_ID']], on='PRESGENE_ID')\n",
    "\n",
    "            # Filter the dataframes to only include the selected participants\n",
    "            base_filtered = output_df[output_df['PRESGENE_ID'].isin(common_ids['PRESGENE_ID'])]\n",
    "            new_filtered = lh_rh_df[lh_rh_df['PRESGENE_ID'].isin(common_ids['PRESGENE_ID'])]\n",
    "\n",
    "            # Concantenate  the previous dataframe with the new session dataset\n",
    "            output_df = pd.concat([base_filtered, new_filtered], ignore_index=True)\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df235a-5e99-4f01-ab57-f6fb2105391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fECM(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframe) dataset with fECM data\n",
    "    OUTPUT: 1 dataframe\n",
    "    DESCRIPTION: Isolates the z-scores features and creates 'Time' column (session indicator) based on feature name (bl = 1, fu = 2).\n",
    "    \"\"\"\n",
    "    # Remove all non-zscore columns from the df\n",
    "    rm_non_z = [col for col in df.columns if not col.startswith('Zscore_') and col != 'PRESGENE_ID']\n",
    "    temp_df =  df.drop(columns = rm_non_z)\n",
    "    \n",
    "    # Make baseline and future columns list\n",
    "    bl_col_names = temp_df.filter(regex='_bl_').columns.tolist()\n",
    "    fu_col_names = temp_df.filter(regex='_fu_').columns.tolist()\n",
    "\n",
    "    # Reformat the baseline and future columns\n",
    "    df_BL = temp_df[bl_col_names + ['PRESGENE_ID']]\n",
    "    df_FU = temp_df[fu_col_names + ['PRESGENE_ID']]\n",
    "\n",
    "    df_BL.columns = df_BL.columns.str.replace('_bl_', '_')\n",
    "    df_FU.columns = df_FU.columns.str.replace('_fu_', '_')\n",
    "\n",
    "    # Create the Time column to label the sessions\n",
    "    df_BL.insert(0, 'Time', 1, False)\n",
    "    df_FU.insert(0, 'Time', 2, False)\n",
    "\n",
    "    # Combine the baseline and future sessions\n",
    "    output_df = pd.concat([df_BL, df_FU], ignore_index=True)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581fbb68",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_df(dMRI, sMRI_vol, sMRI_ls, fECM, num_ses):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dMRI: (dataframe) diffusion MRI dataset.\n",
    "    sMRI_vol: (dataframe) structural MRI dataset with volumetric, clinical and demographic data.\n",
    "    sMRI_ls : (nested list of dataframes) sublists containing the thickness structural MRIs at one of the sessions for the left and \n",
    "    right hemisphere of the brain\n",
    "    fECM : (dataframe) functional MRI dataset.\n",
    "    num_ses: (integer) the number of the highest session desired in the dataset. (1-3)\n",
    "    OUTPUT: 2 dataframes. The first with pwMS only and the second with HC + pwMS participants.\n",
    "    DESCRIPTION: Cleans and merges the sMRI thickness, sMRI volumes, dMRI and fMRI datasets up to the desired session number for the \n",
    "    base MS (pwMS only) and ALL participants (HC + pwMS) datasets.\n",
    "    \"\"\"\n",
    "    # Clean the datasets\n",
    "    dMRI_df = clean_dMRI(dMRI)\n",
    "    vol_df = clean_sMRI_vol(sMRI_vol)\n",
    "    fECM_df = clean_fECM(fECM)\n",
    "    thickness_df = clean_sMRI_thickness(sMRI_ls, num_ses)\n",
    "\n",
    "    # Merge the dMRI, fMRI & volume sMRI measures:\n",
    "    dsMRI_df = pd.merge(vol_df, dMRI_df, on=['PRESGENE_ID', 'Time'])\n",
    "    dsfMRI_df = pd.merge(dsMRI_df, fECM_df, on=['PRESGENE_ID', 'Time'])\n",
    "\n",
    "    # Find their common Subjects & corresponding sess\n",
    "    common_ids = pd.merge(dsfMRI_df[['PRESGENE_ID', 'Time']], thickness_df[['PRESGENE_ID', 'Time']], on=['PRESGENE_ID', 'Time'])\n",
    "    \n",
    "    # Filter the data frames to include only common subjects\n",
    "    dsfMRI_filtered = dsfMRI_df[dsfMRI_df[['PRESGENE_ID', 'Time']].apply(tuple, 1).isin(common_ids.apply(tuple, 1))]\n",
    "    thick_filtered = thickness_df[thickness_df[['PRESGENE_ID', 'Time']].apply(tuple, 1).isin(common_ids.apply(tuple, 1))]\n",
    "\n",
    "    # Combine the presgene & dMRI datasets\n",
    "    output_df = pd.merge(dsfMRI_filtered, thick_filtered, on=['PRESGENE_ID','Time'], how = 'inner')\n",
    "\n",
    "    # Create the ALL particpants base dataframe & remove pwMS specific features\n",
    "    output_all = output_df.drop(columns = ['MStype','lesion_volume_mm3'])\n",
    "\n",
    "    # Create the MS particpants base dataframe & remove irrelevant features for pwMS\n",
    "    output_ms = output_df[output_df['MS'] == 1]\n",
    "    output_ms = output_ms.drop(columns = 'MS')\n",
    "    \n",
    "    print(f'Merging completed')\n",
    "    return output_ms, output_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55de00b-a8d0-42df-b6b4-0877383ac37e",
   "metadata": {},
   "source": [
    "### Import the datasets\n",
    "\n",
    "Import the dataset containing the sMRI (thickness and volumes), clinical, demographic, dMRI and fMRI data. Group the sMRI thickness by session based on the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a21fe-123a-493c-a411-fbbd93755641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the clean_presgene_whole function on the presgene_whole dataframe.\n",
    "dMRI = pd.read_excel('data/Presgene_wholeBL_forFBA_metrics_JHU_forLME_zscores.xlsx')\n",
    "\n",
    "# Load the structural MRI dataset\n",
    "volume_df = pd.read_excel('updated_data/prograMS_database_v1.9_reduced_kayna.xlsx')\n",
    "\n",
    "# Load the functional MRI (fECM) dataset\n",
    "fECM = pd.read_excel('data/Presgene_longitudinal_marijn_fECM.xlsx')\n",
    "\n",
    "# Load the structural MRI datasets\n",
    "# Create the sublists for session grouping\n",
    "Y00, Y05, Y10 = list(), list(), list()\n",
    "\n",
    "sMRIs_path = 'data/sMRIs'\n",
    "for filename in os.listdir(sMRIs_path):\n",
    "    # Read and store each file in its appropriate list\n",
    "    file_path = os.path.join(sMRIs_path, filename)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    if 'Y00' in filename:\n",
    "        Y00.append(df)\n",
    "    elif 'Y05' in filename:\n",
    "        Y05.append(df)\n",
    "    elif 'Y10' in filename:\n",
    "        Y10.append(df)\n",
    "\n",
    "# Create the final dMRI list with all sessions\n",
    "sMRI_ls = [Y00, Y05, Y10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7957c80-7b6d-4de2-ae46-6e9d279afb2b",
   "metadata": {},
   "source": [
    "### Run the merging function (optional saving)\n",
    "\n",
    "Cell blocks to run the functions of Step 1 of this notebook, and save the newly created dataframes optionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abba0b-6c8e-4288-be66-640114aad30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the base function\n",
    "MS_base2, ALL_base2 = make_base_df(dMRI, volume_df, sMRI_ls, fECM, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc3722-5a6b-4b13-b6df-70d0b8b8a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the base dataframes\n",
    "for df in [MS_base2, ALL_base2]:\n",
    "    # Find the highest included session\n",
    "    ses = max(df['Time'].value_counts().index)\n",
    "\n",
    "    # Identify the participants in the df\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "    # Save accordingly as csv files\n",
    "    if label_col[0] == 'MStype':   \n",
    "        df.to_csv(f'updated_data/base/MS_no_dMRI_base{ses}.csv', index = False)\n",
    "    else:\n",
    "       df.to_csv(f'updated_data/base/ALL_no_dMRI_base{ses}.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a14620",
   "metadata": {},
   "source": [
    "# Step 2: Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7f90a-b225-40cb-916e-fd7091db528d",
   "metadata": {},
   "source": [
    "## Generate the train and test subject IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1276c4-91a2-4187-a8fd-3b14fbfeea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_subjects(ms_df, all_df, file_path):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ms_df : (dataframe) base pwMS dataset\n",
    "    all_df : (dataframe) base HC + pwMS dataset\n",
    "    file_path : (string) file path for saving the test subjects arrays\n",
    "    OUTPUT: 2 series of PRESGENE IDs (strings) for pwMS and HC + pwMS participants\n",
    "    DESCRIPTION: Splits the subjects into train and test subjects with stratified sampling using an egineered feature combining the\n",
    "    MS phenotypes and HC/pwMS features for HC + pwMS dataset and the MS phenotype feture for the pwMS dataset.\n",
    "    \"\"\"\n",
    "    # find the max session value in the dataset \n",
    "    max_ses = max(ms_df['Time'].value_counts().index)\n",
    "\n",
    "    # Isolate the HC participants at the max session from the HC + pwMS dataset\n",
    "    hc_subset = all_df[all_df['MS'] == 2]\n",
    "    hc_subset = hc_subset[hc_subset['Time'] == max_ses]\n",
    "    hc_subset = hc_subset[['PRESGENE_ID', 'Time', 'MS']]\n",
    "    \n",
    "    # Isolate the MS participants at the max session from the pwMS dataset\n",
    "    ms_subset = ms_df[['PRESGENE_ID', 'Time', 'MStype']]\n",
    "    ms_subset = ms_subset[ms_subset['Time'] == max_ses]\n",
    "    \n",
    "    # Remove pwMS with missing values or incorrect MStypes\n",
    "    ms_subset = ms_subset.dropna(subset=['MStype'])\n",
    "    ms_subset = ms_subset[ms_subset['MStype'] <= 3]\n",
    "    \n",
    "    # Merge the HC and pwMS datasets, fill misssing values in Stype column (HC values) with 0\n",
    "    working_df = pd.concat([hc_subset, ms_subset], axis=0)\n",
    "    working_df['MStype'] = working_df['MStype'].fillna(0)\n",
    "\n",
    "    # Run train_test_split() function on the IDs, 80% train  20% test\n",
    "    train_id_all, test_id_all, _, _ = train_test_split(working_df['PRESGENE_ID'], working_df['MStype'], \n",
    "                                                        test_size = 0.2, stratify = working_df['MStype'], random_state = 42)\n",
    "\n",
    "    train_id_ms, test_id_ms, _, _ = train_test_split(ms_subset['PRESGENE_ID'], ms_subset['MStype'], \n",
    "                                                    test_size = 0.2, stratify = ms_subset['MStype'], random_state = 42)\n",
    "\n",
    "    # Indicate if partcipants of the train dataset appear in the test serries.\n",
    "    print(f'There are {train_id_all.isin(test_id_all).sum()} subjects of the ALL train split in the ALL test split and {test_id_all.isin(train_id_all).sum()} subjects of the ALL test split in the ALL train split.')\n",
    "    print(f'There are {train_id_ms.isin(test_id_ms).sum()} subjects of the MS train split in the MS test split and {test_id_ms.isin(train_id_ms).sum()} subjects of the MS test split in the MS train split.')\n",
    "    \n",
    "    # Export the test arrays of subjects as pickle files\n",
    "    test_id_all.to_pickle(f'{file_path}/test_PRESGENE_ID_ALL{max_ses}.pkl')\n",
    "    test_id_ms.to_pickle(f'{file_path}/test_PRESGENE_ID_MS{max_ses}.pkl')\n",
    "    \n",
    "    return test_id_ms, test_id_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a569594-96f3-4916-9129-3db7232462da",
   "metadata": {},
   "source": [
    "## Split the dataset between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_split(df, test_subs, file_path):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df : (dataframe)\n",
    "    file_path : (string) file path for saving the test subjects arrays\n",
    "    test_subs : (series of strings) series of the test subjects IDs obtained from get_test_subjects function\n",
    "    output: 2 subsets of the input df with only the train or test subjects\n",
    "    DESCRIPTION: Split the input dataframe into the training and testing dataset\n",
    "    \"\"\"\n",
    "    # Get the max session of the df of interest\n",
    "    max_ses = max(df['Time'].value_counts().index)\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "    if test_subs is None:\n",
    "        # Get the appropriate .pkl file\n",
    "        sub = 'ALL' if label_col[0] == 'MS' else 'MS'\n",
    "        \n",
    "        # Load the test ids .pkl file corresponding to the session & dataset\n",
    "        with open (f'{file_path}/test_PRESGENE_ID_{sub}{max_ses}.pkl', 'rb') as file:\n",
    "            test_subs = pickle.load(file)\n",
    "    \n",
    "    # Get the train and test subsets\n",
    "    test_df = df[df['PRESGENE_ID'].isin(test_subs)]\n",
    "    train_df = df[~df['PRESGENE_ID'].isin(test_subs)]\n",
    "\n",
    "    # Reset the indexing of the train & test dfs\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Indicate if partcipants of the train dataset appear in the test dataset.\n",
    "    print(f\"There are {train_df[['PRESGENE_ID']].isin(test_df[['PRESGENE_ID']]).sum()} subjects of the train split in the test split and {test_df[['PRESGENE_ID']].isin(train_df[['PRESGENE_ID']]).sum()} subjects of the test split in the train split.\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc95b0-ea53-424a-867f-3818c108b25f",
   "metadata": {},
   "source": [
    "## Run the train test split functions\n",
    "\n",
    "First obtain the test participant IDs, then get the train and test datasets based on the test participant IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5a49a-22b0-43fd-83ca-fccacd188518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test subject IDs\n",
    "testid_MS2, testid_ALL2 = get_test_subjects(MS_base2, ALL_base2, 'updated_data/test')\n",
    "\n",
    "# Get the train and test dataset for the ALL and MS datasets (2 methods shown below)\n",
    "ALL_train_base2, ALL_test_base2 = make_train_test_split(ALL_base2, testid_ALL2, None)\n",
    "MS_train_base2, MS_test_base2 = make_train_test_split(MS_base2, None, 'updated_data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3462bab",
   "metadata": {},
   "source": [
    "# Step 3: Imputing the dataframes (all/type/neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34761dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_time(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframe) dataset with data of 2 or more sessions\n",
    "    OUTPUT: list of dataframes eaach with data for a unique session\n",
    "    DESCRIPTION: splits the input dataset into unique sessions according to its max session\n",
    "    \"\"\"\n",
    "    # Find the max session\n",
    "    max_ses = max(df['Time'].value_counts().index)\n",
    "    \n",
    "    # Initialise output variables\n",
    "    output_list = []\n",
    "    \n",
    "    # Get session 1\n",
    "    t1_df = df[df['Time'] == 1]\n",
    "    output_list.append(t1_df)\n",
    "    \n",
    "    # Get session 2 & remove columns not measured in session 2 (presgene_TBSS)\n",
    "    t2_df = df[df['Time'] == 2]\n",
    "    output_list.append(t2_df)\n",
    "    \n",
    "    # Create session 3 or higher dataset if specified\n",
    "    if max_ses >= 3:\n",
    "        for i in range(3,max_ses+1):\n",
    "            t_df = df[df['Time'] == max_ses]\n",
    "            output_list.append(t_df)\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68692cfb-a477-4328-a52d-86fab61f1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_df(df, impute_param):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df : (DataFrame)\n",
    "    impute_param : (string) imputation type, possible values are: 'all', 'type', or 'neighbor'\n",
    "    OUTPUT: DataFrame with no missing values\n",
    "    DESCRIPTION: Imputes the missing values of categorical and numerical features based on the chosen imputation method. \n",
    "    \"\"\"\n",
    "    # Identify label column, categorical and continuous columns\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')][0]\n",
    "    cat_cols = ['Sex', 'HC_CI_CP']\n",
    "    num_cols = [col for col in df.columns if col not in ['Time', label_col, 'PRESGENE_ID', 'Age_group'] + cat_cols]\n",
    "\n",
    "    # Save and remove ID column\n",
    "    PRESGENE_IDs = df[['PRESGENE_ID']]\n",
    "    noID_df = df.drop(columns='PRESGENE_ID')\n",
    "\n",
    "    # Ensure numerical columns are numeric\n",
    "    for col in num_cols:\n",
    "        noID_df[col] = pd.to_numeric(noID_df[col], errors='coerce')\n",
    "    \n",
    "    # Handle HC-specific logic\n",
    "    if df['PRESGENE_ID'].str.startswith('HC').any():\n",
    "        noID_df.loc[noID_df['MS'] == 2, 'EDSS'] = 999\n",
    "\n",
    "    # Copy DataFrame for imputation\n",
    "    imputed_df = noID_df.copy()\n",
    "\n",
    "    if impute_param == 'all': # Impute missing values (per session only).\n",
    "        # Impute NaN for categorical columns with mode.\n",
    "        for col in cat_cols:\n",
    "            if imputed_df[col].isna().any():\n",
    "                imputed_df[col] = noID_df[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "        \n",
    "        # Impute NaN for continuous columns with the mean.\n",
    "        for col in num_cols:\n",
    "            if imputed_df[col].isna().any():\n",
    "                imputed_df[col] = noID_df[col].transform(lambda x: x.fillna(x.mean()))\n",
    "            \n",
    "\n",
    "    if impute_param == 'type':\n",
    "        # Impute categorical columns with mode by group\n",
    "        for col in cat_cols:\n",
    "            if imputed_df[col].isna().any():\n",
    "                global_mode = noID_df[col].mode().iloc[0]  # Fallback mode\n",
    "                imputed_df[col] = imputed_df.groupby(label_col)[col].transform(\n",
    "                    lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else global_mode))\n",
    "                 \n",
    "        # Impute numerical columns with mean by group\n",
    "        for col in num_cols:\n",
    "            if imputed_df[col].isna().any():\n",
    "                global_mean = noID_df[col].mean()  # Fallback mean\n",
    "                imputed_df[col] = imputed_df.groupby(label_col)[col].transform(\n",
    "                    lambda x: x.fillna(x.mean()).fillna(global_mean))\n",
    "\n",
    "    elif impute_param == 'neighbor': # Impute missing values per session and neighbour.\n",
    "        # Initialize KNNImputers\n",
    "        mode_imputer = KNNImputer(n_neighbors=5, weights=\"distance\", metric=\"nan_euclidean\") # Mode\n",
    "        mean_imputer = KNNImputer(n_neighbors=5) # Mean\n",
    "        \n",
    "        # Impute NaN \n",
    "        imputed_cats_df = pd.DataFrame(mode_imputer.fit_transform(imputed_df[cat_cols]), columns = cat_cols, index=imputed_df.index)\n",
    "        imputed_df[cat_cols] = imputed_cats_df\n",
    "        \n",
    "        imputed_nums_df = pd.DataFrame(mean_imputer.fit_transform(imputed_df[num_cols]), columns = num_cols, index=imputed_df.index)\n",
    "        imputed_df[num_cols] = imputed_nums_df\n",
    "\n",
    "    # Apply rounding and ensure EDSS values are of the correct values and format\n",
    "    imputed_df['EDSS'] = imputed_df['EDSS'].apply(\n",
    "        lambda x: 0 if x < 0.5 else (1 if x < 1.5 else round(x * 2) / 2 if x != 999 else 999))\n",
    "    imputed_df[['Time', 'Sex', label_col, 'HC_CI_CP']] = imputed_df[['Time', 'Sex', label_col, 'HC_CI_CP']].round().astype(int)\n",
    "\n",
    "    # Reintroduce ID column\n",
    "    output_df = pd.concat([PRESGENE_IDs, imputed_df], axis=1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_imputed(ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_df : (list of dataframes) list of dataframes with unique sessions values\n",
    "    OUTPUT: 1 Dataframe\n",
    "    DESCRIPTION: Merges the unique sessions dataset to obtain 1 dataset with all sessions in it.\n",
    "    \"\"\"\n",
    "    # Initialisation of output list    \n",
    "    output_df = None\n",
    "\n",
    "    # Find the number of sessions to merge\n",
    "    max_ses = len(ls_df)\n",
    "\n",
    "    # Identify common columns between the first two dataframes\n",
    "    common_columns = ls_df[0].columns.intersection(ls_df[1].columns)\n",
    "    temp_df1 = ls_df[0][common_columns]\n",
    "    temp_df2 = ls_df[1][common_columns]\n",
    "    \n",
    "    # Check if there are more than 2 dataframes to combine\n",
    "    if max_ses > 2:\n",
    "        concat_extras = []\n",
    "        for i in range(2, max_ses):\n",
    "            # Find common columns between the existing common columns and the nth dataframe\n",
    "            common_columns = common_columns.intersection(ls_df[i].columns)\n",
    "            temp_df = ls_df[i][common_columns]\n",
    "            concat_extras.append(temp_df)\n",
    "        # Combine all subsets\n",
    "        output_df = pd.concat([temp_df1, temp_df2]+concat_extras, axis=0)\n",
    "    \n",
    "    else: # if only 2 dataframes to combine\n",
    "        # Combine only the first two subsets\n",
    "        output_df = pd.concat([temp_df1, temp_df2], axis=0)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49e709-3bcf-4bdc-bb0b-74be457e80aa",
   "metadata": {},
   "source": [
    "## Run impute_df and save for the test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241d284-0d2a-4ab0-af50-bc92bbf1b37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_file_path = 'updated_data/test'\n",
    "\n",
    "# Iterate over the test datasets for imputation and saving\n",
    "for test_df in [ALL_test_base2, MS_test_base2]:\n",
    "    for imp_method in ['all', 'type', 'neighbor']:\n",
    "        # Split by sessions\n",
    "        unique_tp_ls = split_df_by_time(test_df)\n",
    "\n",
    "    \n",
    "        # Initialise unique sessions list & impute missing values for each df\n",
    "        imp_ls = []\n",
    "        for tp_df in unique_tp_ls:\n",
    "            print('unique_tp_ls', tp_df.shape)\n",
    "            imp_tp_df = impute_df(tp_df, imp_method)\n",
    "            imp_ls.append(imp_tp_df)\n",
    "    \n",
    "        # Merge the unique sessions imputed datasets\n",
    "        imp_df = merge_time_imputed(imp_ls)\n",
    "        saving_ls = [imp_df] + imp_ls\n",
    "    \n",
    "        # Save the imputed test imputed datasets to .csv (no oversampling for test dfs)\n",
    "        sub = 'ALL' if 'MS' in saving_ls[0].columns else 'MS'\n",
    "        max_ses = len(imp_ls)\n",
    "        ses = ['', '00', '05', '10']\n",
    "        for i, df in enumerate(saving_ls):\n",
    "            df.to_csv(f'{save_file_path}/{imp_method}/{sub}test_{ses[i]}{max_ses}_{imp_method}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399757b0",
   "metadata": {},
   "source": [
    "# Step 4: Handling Class Imbalances with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f01b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets session row wise per subjects (1 row per subjects)\n",
    "def merge_time_by_rows(ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_df : (nested list of dataframes) nested list containing the imputed datasets, the first list for the MS unique sessions datasets \n",
    "    and a second for ALL unique sessions datasets.\n",
    "    output: list of 2 dataframes for \n",
    "    DESCRIPTION: Combines the unique sessions dataframes of each sublist of the input list to obtain 1 row per subject.\n",
    "    \"\"\"\n",
    "    # Initialise the output list \n",
    "    output_ls = []\n",
    "\n",
    "    # Iterate over the particpant specific lists (MS & ALL)\n",
    "    for idls, ls in enumerate(ls_df):\n",
    "        # define the base (baseline session) dataset\n",
    "        base_df = ls[0]\n",
    "        base_df = ls[0].rename(columns={col: col + f'_ses{str(int(list(base_df[\"Time\"].unique())[0]))}' for col in ls[0].columns if col not in ['PRESGENE_ID', 'Sex']})\n",
    "\n",
    "        # Iterate over the range of dfs in the sublist\n",
    "        for idx in range(1, len(ls)):\n",
    "            working_df = ls[idx]\n",
    "            working_ses = int(list(working_df['Time'].unique())[0])\n",
    "            \n",
    "            # Remove the sex column as it does not change over the sessions\n",
    "            working_df = working_df[[col for col in working_df.columns if col != 'Sex']]\n",
    "\n",
    "            # Rename the columns in common\n",
    "            working_df = working_df.rename(columns={col: col + f'_ses{str(working_ses)}' for col in working_df.columns if col != 'PRESGENE_ID'})\n",
    "\n",
    "            # Combine the base and the new time point\n",
    "            base_df = pd.merge(base_df, working_df, on='PRESGENE_ID', how='left')\n",
    "\n",
    "        # Add the newly reformated df to the output list\n",
    "        output_ls.append(base_df)\n",
    "\n",
    "    return output_ls[0], output_ls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dac090-7876-4be2-98fa-76be8d251f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_SMOTE_dfs(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframes) dataframe with the 1 row per participant format created by the functionmerge_time_by_rows\n",
    "    output: list with first a dataframe of the merged sessions with X rows per participant per X sessions, followed by the unique sessions\n",
    "    datasets.\n",
    "    DESCRIPTION: Splits the input dataframe into unique sessions, remove the session specific suffix from the column names, return a \n",
    "    list of the unique sessions dataset and a version with the merged sessions.\n",
    "    \"\"\"\n",
    "    # Reset index to use it as 'PRESGENE_ID' Substitute\n",
    "    df_idx = df.reset_index(drop = False)\n",
    "\n",
    "    # Ensure MStype or MS (categorical columns) are whole values\n",
    "    label_cols = [col for col in df_idx.columns if col.startswith('MS')]\n",
    "    for col in label_cols:\n",
    "        df_idx[col] = df_idx[col].round().astype(int)\n",
    "\n",
    "    # Find the number of sessions included in the dataset\n",
    "    sessions = [col for col in df.columns if col.startswith('Time')]\n",
    "\n",
    "    # Initialise the output list for unique session datasets\n",
    "    unique_ses_ls = []\n",
    "\n",
    "    # Iterate over the sessions to get the unique sessions dataframes\n",
    "    for ses in range(1, len(sessions) + 1):\n",
    "        ses_cols = [col for col in df_idx.columns if f'_ses{ses}' in col]\n",
    "        working_df = df_idx[['index', 'Sex'] + ses_cols]\n",
    "        working_df.columns = working_df.columns.str.replace(f'_ses{ses}', '', regex=False)\n",
    "        \n",
    "        unique_ses_ls.append(working_df)\n",
    "\n",
    "    # Combine the unique session data frames into 1 multi-session dataframe\n",
    "    combined_df = pd.concat(unique_ses_ls, ignore_index=True)\n",
    "\n",
    "    return [combined_df] + unique_ses_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf9869-7fbb-402e-b3bd-7969bf0f5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_rebalancing_SMOTE(ls_ls_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_df : (nested list of dataframes) nested list containing the imputed datasets, the first list for the MS unique sessions datasets \n",
    "    and a second for ALL unique sessions datasets.\n",
    "    OUTPUT: lists of dataframes for the MS and ALL unique sessions datasets\n",
    "    DESCRIPTION: Uses SMOTE to fix class imbalance in the MS phenotypes and HC + MS phenotypes in the pwMS dataset and HC + pwMS dataset,\n",
    "    respectively. \n",
    "    \"\"\"\n",
    "    # Combine the unique sessions row-wise (1 row per subjects)\n",
    "    MS_df, ALL_df = merge_time_by_rows(ls_ls_df)\n",
    "\n",
    "    # Find frequency of RRMS (most frequent is RRMS with MStype = 2)\n",
    "    resample_size = MS_df['MStype_ses1'].value_counts().max() * 2\n",
    "    \n",
    "    ## OVERSAMPLING pwMS DATASET\n",
    "    \n",
    "    # Make the sampling strategy for pwMS\n",
    "    sampling_strategy_ms = {label: resample_size for label in MS_df['MStype_ses1'].unique()}\n",
    "\n",
    "    # Remove the ID col and isolate the stratifying feature col\n",
    "    X_ms = MS_df.drop(['MStype_ses1', 'PRESGENE_ID'], axis=1)\n",
    "    y_ms = MS_df['MStype_ses1']\n",
    "    \n",
    "    # Initialise & apply SMOTE with Dynamic number of neighbours ( 3 or smallest class)\n",
    "    min_samples_class_ms = y_ms.value_counts().min()\n",
    "    smote_MS = SMOTE(sampling_strategy=sampling_strategy_ms, k_neighbors=min(3, min_samples_class_ms - 1), random_state=42) \n",
    "    X_SMOTE_ms, y_SMOTE_ms = smote_MS.fit_resample(X_ms, y_ms)\n",
    "\n",
    "    # Create the output MS df wiht the SMOTE oversampled data\n",
    "    output_MS = pd.DataFrame(X_SMOTE_ms, columns = X_ms.columns)\n",
    "    output_MS = pd.concat([output_MS, pd.Series(y_SMOTE_ms, name='MStype_ses1')], axis=1)\n",
    "\n",
    "    ## OVERSAMPLING ALL PARTICIPANTS DATASET\n",
    "    \n",
    "    # Add the MStype column to the ALL participants df for resampling of MStypes & HC/MS. \n",
    "    temp_ALL_mstype = ALL_df.merge(MS_df[['PRESGENE_ID', 'MStype_ses1']], on='PRESGENE_ID', how = 'left')\n",
    "    temp_ALL_mstype['MStype_ses1'] = temp_ALL_mstype['MStype_ses1'].fillna(0)\n",
    "\n",
    "    # Make the sampling strategy for ALL\n",
    "    sampling_strategy_all = {label: resample_size for label in temp_ALL_mstype['MStype_ses1'].unique()}\n",
    "\n",
    "    # Remove the ID col and isolate the stratifying feature col\n",
    "    X_all = temp_ALL_mstype.drop(['MStype_ses1', 'PRESGENE_ID'], axis=1)\n",
    "    y_all = temp_ALL_mstype['MStype_ses1']\n",
    "    \n",
    "    # Initialise & apply SMOTE with Dynamic number of neighbours ( 3 or smallest class)\n",
    "    min_samples_class_all = y_all.value_counts().min()\n",
    "    smote_ALL = SMOTE(sampling_strategy=sampling_strategy_all, k_neighbors=min(3, min_samples_class_all - 1), random_state=42) \n",
    "    X_SMOTE_all, y_SMOTE_all = smote_ALL.fit_resample(X_all, y_all)\n",
    "\n",
    "    # Create the output ALL df wiht the SMOTE oversampled data\n",
    "    output_ALL = pd.DataFrame(X_SMOTE_all, columns = X_all.columns)\n",
    "\n",
    "    # Reformat the extrapolated EDSS values\n",
    "    edss_cols = [col for col in output_MS.columns if col.startswith('EDSS')]\n",
    "    for col in edss_cols:\n",
    "        output_MS[col] = output_MS[col].apply(lambda x: 999 if x >= 10.5 else \n",
    "                                              (0 if x < 0.5 else \n",
    "                                               (1 if x < 1.5 else \n",
    "                                                round(x * 2) / 2 if x != 999 else 999)))\n",
    "        output_ALL[col] = output_ALL[col].apply(lambda x: 999 if x >= 10.5 else \n",
    "                                                (0 if x < 0.5 else \n",
    "                                                 (1 if x < 1.5 else \n",
    "                                                  round(x * 2) / 2 if x != 999 else 999)))\n",
    "\n",
    "    # Convert the stratifying features to integers (in case of conversion)\n",
    "    output_MS[['MStype_ses1']] ==  output_MS[['MStype_ses1']].round().astype(int)\n",
    "    output_ALL[['MS_ses1']] ==  output_ALL[['MS_ses1']].round().astype(int)\n",
    "\n",
    "    # Reformat the dataset from 1 row per subject to n rows per n sessions per subject.\n",
    "    MS_output_ls = reformat_SMOTE_dfs(output_MS)\n",
    "    ALL_output_ls = reformat_SMOTE_dfs(output_ALL)\n",
    "\n",
    "    return MS_output_ls, ALL_output_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ebae8-a668-432a-a58c-9431574a5c0a",
   "metadata": {},
   "source": [
    "## Run the SMOTE oversampling on the train datasets & save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fc973-7dc9-44bd-9461-54c3b05846d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_file_path = 'updated_data/oversampled'\n",
    "all_train_base = ALL_train_base2\n",
    "ms_train_base = MS_train_base2\n",
    "\n",
    "# Iterate over the 3 possible imputation methods\n",
    "for imp_method in ['all', 'type', 'neighbor']:\n",
    "    # Iterate over the MS and ALL train datasets for imputation\n",
    "    train_imp_tp_ls = []\n",
    "    for run, train_df in enumerate([ms_train_base, all_train_base]):\n",
    "        # Split by sessions\n",
    "        unique_tp_ls = split_df_by_time(train_df)\n",
    "    \n",
    "        # Initialise unique sessions list & impute missing values for each df\n",
    "        imp_ls = []\n",
    "        for tp_df in unique_tp_ls:\n",
    "            imp_tp_df = impute_df(tp_df, imp_method)\n",
    "            imp_ls.append(imp_tp_df)\n",
    "    \n",
    "        # Add the imputations to the MS or ALL list \n",
    "        train_imp_tp_ls.append(imp_ls)\n",
    "\n",
    "    # Use SMOTE for class rebalancing of the MS and ALL datasets\n",
    "    MS_smote_ls, ALL_smote_ls = class_rebalancing_SMOTE(train_imp_tp_ls)\n",
    "\n",
    "    # Save the merged and unique sessions class rebalanced train datasets\n",
    "    max_ses = len(unique_tp_ls)\n",
    "    ses = ['', '00', '05', '10']\n",
    "    for idx, df in enumerate(MS_smote_ls):\n",
    "        if ses[idx] == '':\n",
    "            df.to_csv(f'{save_file_path}/{imp_method}/MStrain_{max_ses}_{imp_method}.csv', index = False)\n",
    "            ALL_smote_ls[idx].to_csv(f'{save_file_path}/{imp_method}/ALLtrain_{max_ses}_{imp_method}.csv', index = False)\n",
    "        else:\n",
    "            df.to_csv(f'{save_file_path}/{imp_method}/MStrain_{ses[idx]}{max_ses}_{imp_method}.csv', index = False)\n",
    "            ALL_smote_ls[idx].to_csv(f'{save_file_path}/{imp_method}/ALLtrain_{ses[idx]}{max_ses}_{imp_method}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
