{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b0ce27-ca11-4810-a354-ec0ab831e8b2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the scripts needed to run the nested cross-validation (nCV) mentioned in the method of part 2 of the project. The notebook contains a variety of functions needed by the nCV from data cleaning to machine learning algorithm (MLA) hyperparameter tuning and plotting the receiver operator characteristic plot of the outer runs. The functions in the notebook can be grouped into the following 5 categories and consequent 5 main steps:\n",
    "\n",
    "Step 1: The libraries needed to run the LSTM and other dataset-altering functions are imported alongside the datasets containing the  sMRI volume measurements, the sMRI thickness measurements, the dMRI, the clinical, and the demographic data. The dMRI data is however only used if the first 2 sessions (Y00 and Y05) are intended to be used for training and evaluating the model as it does not currently have any available data for the third session (Y10).\n",
    "\n",
    "Step 2: The first 5 functions: get_dMRI, clean_sMRI_vol, PRESGENE_ID_for_thickness, clean_sMRI_thickness, and make_base_df are used to create the base dataset from the input datasets the steps taken to clean these including removing rows or columns with more than 90% of missing values, isolate features of interest and participants based on the target variable and combining the datasets with the desired number of sessions. Once the base dataset is obtained, the missing values are filled with the Time + Type imputation method using the functions split_df_by_time and impute_df. As in part 1 of the project, the overall class imbalance is handled using Synthetic Minority Over-sampling Technique (SMOTE) by reshaping the dataset to 1 row per subject in the function merge_time_by_rows, applying the SMOTE algorithm to generate new participant data based on HC / MS phenotype in the function class_rebalancing_SMOTE, and then reformat the dataset back to n rows per participant for the n sessions in the function reformat_SMOTE_dfs. Depending on the desired transformation method (None or UMAP), the function apply_unique_UMAP is used to dimensionally reduce the dataset to 2 Features with the hyperparameters (hps) of the algorithm (perplexity, minimum distance, and metric) tuned with the mode of the hps obtained in part 1 of the project. The final step in preparing the dataset is feature engineering if the desired target variable is bi_EDSS, tri_EDSS, worsening_EDSS, or RR_to_SP, then the feature is created in this function. Otherwise, the function ensures that the target variable has the correct format. The new feature is then added to the dataframe or the existing column is updated in the dataframe.\n",
    "\n",
    "Step 3: Encompasses all of the functions needed for the LSTM to run successfully. The dataset is reformated to a 1 row per participant format in prep_LSTM_data, then either the function rdm_oversampling is used to apply random oversampling to the dataset when the target variable is categorical or bootstrapping is used to increase the sample size with duplicates. The tune_lstm is used to tune the LSTM hps in the inner loop of the nCV and either the lstm_regressor_outer or the lstm_classifier_outer functions are used to build the LSTM in the function run_lstm with the hps found during tuning in the outer loop of the nCV. The function evaluate_score is used to compare the loss functions across the inner and outer runs of the LSTM and update the hps or model for the corresponding best score. Finally, the function AUCROC is used to obtain the AUCROC plot during the outer runs of the nCV.\n",
    "\n",
    "Step 4: Contains the function for the nCV and groups the previously mentioned functions into a nCV pipeline in the function CV_pipeline. After the creation of the base dataset with the functions mentioned in step 2, get_unique_subs is used to obtain a list of the participants representative of the target variable that stratifiedKfold can use. For example, continuous target variables (e.g.: average cogntion) are binned into 3 classes so that each range of values is evenly represented in each fold. These alterations are temporary and only used for stratifying the dataset when obtaining the folds. Following imputation and SMOTE, a version of the dataset with all combined sessions is obtained with the function merge_time_imputed so that it i. may also be used as a training dataset and ii. be used for feature engineering in the overarching function apply_transform_labeled which also performs data transformation on each of the train and test dataset for the given nCV fold. As demonstrated in Figure 2 of the paper, the nCV pipeline repeats the previous steps as it iterates through the inner folds of the outer folds. In the end, a dictionary of the inner and outer evaluation scores and best hps are returned, in addition to AUCROC plots being saved (for categorical target variables), and the best performing-model.\n",
    "\n",
    "Step 5: This final step provides a few cells to run the nCV with the possibility to choose the parameter values: transformation (RAW/UMAP), number of sessions to include in the training data (n_ses), session to use as test set (n_test), number of outer nCV runs (n_outer), and number of inner nCV runs (n_inner). A list of the target/dependent variable ('EDSS', 'MS', 'MStype', 'BL_Avg_cognition', 'HC_CI_CP','bi_EDSS', 'tri_EDSS', 'worsening_EDSS', 'RR_to_SP'), is also provided and the cell is set up such that we can iterate through the list of target variables with the nCV being repeated for each of the variables and the outputs of the function CV_pipeline being saved and added to a master dictionary for easy access to the combined target variables for analysis.\n",
    "\n",
    "This pipeline demonstrates how the results for part 2 of the project were obtained. The LSTM was isolated from the other MLAs as it requires a different version of certain libraries as explained in the LSTM subsection of the methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3ccc8-4627-4a94-ad5c-f72c09a7e973",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572c773-dc98-4421-a18d-fb878c6d6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Dimensionalty Reduction\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Machine learning algorithms general\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "# machine learning LSTM specific\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import Objective\n",
    "from tensorflow.keras.models import save_model, model_from_json, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddbf90-8e1d-4de8-8a56-08b415ad3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the diffusion MRI datasets\n",
    "dMRI = pd.read_excel('data/Presgene_wholeBL_forFBA_metrics_JHU_forLME_zscores.xlsx')\n",
    "\n",
    "# Load the structural MRI dataset (volumes)\n",
    "volume_df = pd.read_excel('updated_data/prograMS_database_v1.9_reduced_kayna.xlsx')\n",
    "\n",
    "# Create the sublists to group the sMRI (thickness) time points together\n",
    "Y00, Y05, Y10 = list(), list(), list()\n",
    "\n",
    "# Load the structural MRI datasets (thickness)\n",
    "sMRIs_path = 'data/sMRIs'\n",
    "for filename in os.listdir(sMRIs_path):\n",
    "    # Read and store each file in its appropriate list\n",
    "    file_path = os.path.join(sMRIs_path, filename)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    if 'Y00' in filename:\n",
    "        Y00.append(df)\n",
    "    elif 'Y05' in filename:\n",
    "        Y05.append(df)\n",
    "    elif 'Y10' in filename:\n",
    "        Y10.append(df)\n",
    "\n",
    "# Create the final sMRI list with all time points\n",
    "sMRI_ls = [Y00, Y05, Y10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ffe3b-9e1c-412b-adee-a39732c95d66",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the Data for the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f2160-c456-41e9-b646-498088a4d4cc",
   "metadata": {},
   "source": [
    "## Functions for the base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dab0e5-d26a-4f20-adf2-a4e12bfc13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dMRI(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df :(dataframe) dataset with dMRI data\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Sets up the dMRI dataset for merging with others in the make_base_df function. Isolates the no lession zscores \n",
    "    (FD, FDC, and logFC), subject IDs and sessions number.\n",
    "    \"\"\"\n",
    "    # Create a list with all columns begining with FD or Logfc in the df\n",
    "    all_dMRI_cols = [col for col in df.columns if col.startswith('FD') or col.startswith('Logfc')]\n",
    "    \n",
    "    # Keep only columns with _nolesion_zscores\n",
    "    keep_dMRI_cols = [col for col in all_dMRI_cols if col.endswith('_nolesion_zscores')]\n",
    "\n",
    "    # Get the columns to remove (not the selected FD/Logfc, PRESGENE)ID and Time) columns\n",
    "    rm_cols = [col for col in df.columns if col not in ['PRESGENE_ID', 'Time'] + keep_dMRI_cols]\n",
    "\n",
    "    # Remove the unwanted columns from the df\n",
    "    output_df = df.drop(columns = rm_cols, axis = 1)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fd313-5783-469d-819c-9584a07a35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sMRI_vol(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df :(dataframe) dataset with demographic, clinical and structural MRI dataset.\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Sets up the sMRI (volumes) dataset for merging with others in the make_base_df function. \n",
    "    Retains the clinical, demographic, and sMRI volume features. Removes rows and columns with at least 90% of missing values.\n",
    "    \"\"\"\n",
    "    # Make a copy of the input df to prevent editing the original df\n",
    "    working_df = df.copy()\n",
    "    \n",
    "    #Remove raw crossectional measures\n",
    "    clin_demo = ['id_presgene','programs_label_code','edss', 'mstype_code', 'subject_type_code',\n",
    "                 'age',\t'sex',\t'average_cognition', 'HC_CI_CP']\n",
    "    \n",
    "    vol_long_corr = [col for col in working_df.columns if 'Vol_long_all_corrected' in col and 'Mask' not in col and 'BrainSeg' not in col]\n",
    "\n",
    "    working_df = working_df[[col for col in working_df.columns if col in clin_demo + vol_long_corr + ['lesion_volume_mm3']]]\n",
    "\n",
    "    # Replace other missing type notation with the most common MStype\n",
    "    mstype_mode = working_df['mstype_code'].mode(dropna=True)[0]\n",
    "    working_df['mstype_code'] = working_df['mstype_code'].apply(lambda x: mstype_mode if x not in {1, 2, 3, None} else x)\n",
    "\n",
    "    # Remove columns filled with at least 90% missing values or zeros:\n",
    "    nan_prct = working_df.isnull().mean()\n",
    "    zr_prct = (working_df == 0).mean()\n",
    "    prct = nan_prct+zr_prct\n",
    "    nozr_df = working_df.loc[:, prct < 0.8]\n",
    "\n",
    "    zr_col = prct[prct >= 0.8].index.tolist()\n",
    "    print(f\"Columns: {zr_col} have more than 80% missing data and were removed\")\n",
    "\n",
    "    # Remove rows filled with at leasst 90% missing values or zeros and any of their occurances:\n",
    "    na_prct_row = nozr_df.isnull().mean(axis=1)\n",
    "    zr_prct_row = (nozr_df == 0).mean(axis=1)\n",
    "    prct_row = na_prct_row + zr_prct_row\n",
    "    zr_row = nozr_df.loc[prct_row >= 0.8, 'id_presgene'].tolist()\n",
    "    nozr_df =  nozr_df[~nozr_df['id_presgene'].isin(zr_row)]\n",
    "\n",
    "    print(f\"Rows with ID {zr_row} have more than  80% missing data and were removed\")\n",
    "\n",
    "    # Encode the HC_CI_CP to numerical labels\n",
    "    nozr_df.loc[:, 'HC_CI_CP'] = nozr_df.loc[:, 'HC_CI_CP'].fillna(9999)\n",
    "    nozr_df.loc[:,'HC_CI_CP'] = nozr_df.loc[:,'HC_CI_CP'].replace({'HC': 0, 'CP': 1, 'CI': 2}).infer_objects(copy=False)\n",
    "    nozr_df.loc[:, 'HC_CI_CP'] = nozr_df.loc[:,'HC_CI_CP'].replace(9999, np.nan).infer_objects(copy=False)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    rename_dict = {'id_presgene':'PRESGENE_ID',\n",
    "                   'programs_label_code': 'Time',\n",
    "                   'edss': 'EDSS',\n",
    "                   'mstype_code':'MStype',\n",
    "                   'subject_type_code':'MS',\n",
    "                   'age': 'Age',\n",
    "                   'sex': 'Sex',\n",
    "                   'average_cognition':'BL_Avg_cognition'}\n",
    "                          \n",
    "    nozr_df = nozr_df.rename(columns = rename_dict)\n",
    "    \n",
    "    return nozr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401a5bb-6479-48d6-8c8c-1e405022e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRESGENE_ID_for_thickness(df_list):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df_list : (list of dataframes) list containing 2 sMRI (thickness) datasets representing left & right hemisphere measurements\n",
    "    OUTPUT: 2 dataframes of left and right hemisphere data.\n",
    "    DESCRIPTION: Reformats the ID column to match that of the PRESGENE_ID, and identifies the left and right hemisphere dataset.\n",
    "    \"\"\"\n",
    "    lh_df, rh_df = None, None\n",
    "\n",
    "    for df in df_list:\n",
    "        old_id = df.columns[0]\n",
    "        id_col = 'PRESGENE_ID'\n",
    "        # Restructure ID column\n",
    "        df = df.rename(columns = {df.columns[0] : id_col})\n",
    "        df[id_col] = df[id_col].astype(str)\n",
    "        df[id_col] = df[id_col].replace('^sub-', '', regex=True)\n",
    "        df[id_col] = df[id_col].str[:4]+ '_' + df[id_col].str[4:]\n",
    "\n",
    "        # Remove the BrainSegVolNotVent & eTIV columns:\n",
    "        df = df.drop(columns = ['BrainSegVolNotVent', 'eTIV'])\n",
    "\n",
    "        if 'lh' in old_id:\n",
    "            lh_df = df\n",
    "        elif 'rh' in old_id:\n",
    "            rh_df = df\n",
    "        else:\n",
    "            print('Neither left nor right hemisphere files were provided.')\n",
    "\n",
    "    return lh_df, rh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd805ad-147f-46ff-8d97-efed5b849e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sMRI_thickness(df_list, num_ses):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df_list : (nested list of dfs) Each session has its own sublist containing the left and right hemisphere sMRI thickness \n",
    "    data within the main list.\n",
    "    num_ses : (integer) number of sessions to include in the merging of the sMRI thickness datasets\n",
    "    OUTPUT: 1 dataframe\n",
    "    DESCRIPTION: Sets up the sMRI (thickness) individual datasets (per session and brain hemisphere) for merging with others in the \n",
    "    make_base_df function. Uses the parameter num_ses session to combine the left and right hemisphere data up to the desired session\n",
    "    (baseline = 1, five year follow-up = 2, ten year follow-up = 3).\n",
    "    \"\"\"\n",
    "    output_df = None\n",
    "\n",
    "    for i in range(0, num_ses):\n",
    "        # Rename the subjects to match the PRSEGENE structure:\n",
    "        presgene_lh, presgene_rh = PRESGENE_ID_for_thickness(df_list[i])\n",
    "        \n",
    "        # merge lh & rh datasets & add time column\n",
    "        lh_rh_df = pd.merge(presgene_lh, presgene_rh, on=['PRESGENE_ID'], how='inner')\n",
    "        lh_rh_df['Time'] = i + 1\n",
    "\n",
    "        if i == 0: # if baseline (Y00)\n",
    "            output_df = lh_rh_df\n",
    "            \n",
    "        else:\n",
    "            # Combine the time points vertically, only keeping subjects common to all desired tps. \n",
    "            common_ids = pd.merge(output_df[['PRESGENE_ID']], lh_rh_df[['PRESGENE_ID']], on='PRESGENE_ID')\n",
    "            base_filtered = output_df[output_df['PRESGENE_ID'].isin(common_ids['PRESGENE_ID'])]\n",
    "            new_filtered = lh_rh_df[lh_rh_df['PRESGENE_ID'].isin(common_ids['PRESGENE_ID'])]\n",
    "\n",
    "            output_df = pd.concat([base_filtered, new_filtered], ignore_index=True)\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd5cfb-a3a2-4bdc-a253-78789e355307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_df(presgene_df, sMRI_vol, sMRI_ls, num_tps, task_ft):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dMRI: (dataframe) diffusion MRI dataset.\n",
    "    sMRI_vol: (dataframe) structural MRI dataset with volumetric, clinical and demographic data.\n",
    "    sMRI_ls : (nested list of dataframes) sublists containing the thickness structural MRIs at one of the sessions for the left and \n",
    "    right hemisphere of the brain\n",
    "    num_ses: (integer) the number of the highest session desired in the dataset. (1-3)\n",
    "    task_ft : (string) current target variable name, determines if base is pwMS only dataset or HC + pwMS dataset\n",
    "    OUTPUT: 1 dataframe\n",
    "    DESCRIPTION: Cleans and merges the sMRI thickness, sMRI volumes, and dMRI (if num_tps < 3) datasets up to the desired session number \n",
    "    for the current target variable\n",
    "    \"\"\"\n",
    "    # Clean the structural MRI data\n",
    "    vol_df = clean_sMRI_vol(sMRI_vol)\n",
    "    thickness_df = clean_sMRI_thickness(sMRI_ls, num_tps)\n",
    "\n",
    "    if isinstance(presgene_df, pd.DataFrame) and num_tps < 3: \n",
    "        dMRI_df = get_dMRI(presgene_df)\n",
    "        # Merge the dMRI & volume sMRI measures:\n",
    "        vol_df = pd.merge(vol_df, dMRI_df, on=['PRESGENE_ID', 'Time'])\n",
    "\n",
    "    # Find their common Subjects & corresponding tps\n",
    "    common_ids = pd.merge(vol_df[['PRESGENE_ID', 'Time']], thickness_df[['PRESGENE_ID', 'Time']], on=['PRESGENE_ID', 'Time'])\n",
    "    \n",
    "    # Filter the data frames to include only common subjects\n",
    "    pres_filtered = vol_df[vol_df[['PRESGENE_ID', 'Time']].apply(tuple, 1).isin(common_ids.apply(tuple, 1))]\n",
    "    dMRI_filtered = thickness_df[thickness_df[['PRESGENE_ID', 'Time']].apply(tuple, 1).isin(common_ids.apply(tuple, 1))]\n",
    "\n",
    "    # Combine the presgene & dMRI datasets\n",
    "    output_df = pd.merge(pres_filtered, dMRI_filtered, on=['PRESGENE_ID','Time'], how = 'inner')\n",
    "\n",
    "    # Remove the column of noninterest based on the wanted dataset\n",
    "    output_df = output_df.drop(columns = ['lesion_volume_mm3'])\n",
    "\n",
    "    # Remove HC if the task is for pwMS only\n",
    "    if task_ft in ['MStype', 'bi_EDSS', 'tri_EDSS', 'worsening_EDSS', 'EDSS', 'RR_to_SP']:\n",
    "        output_df = output_df[output_df['MS'] == 1]\n",
    "\n",
    "    print(f'Merging completed, base dataset for {task_ft} up to time point {output_df[\"Time\"].max()} is ready!')\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43509c3b-0ef3-4c58-8633-a004fb46d281",
   "metadata": {},
   "source": [
    "## Functions for dataset imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219ee22-724c-41ec-87f2-d97b8877c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_time(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframe) dataset with data of 2 or more sessions\n",
    "    OUTPUT: list of dataframes eaach with data for a unique session\n",
    "    DESCRIPTION: splits the input dataset into unique sessions according to its max session\n",
    "    \"\"\"\n",
    "    # Find highest session number and initialise output variables\n",
    "    max_tp = max(df['Time'].value_counts().index)\n",
    "    output_list = []\n",
    "    \n",
    "    # Get baseline\n",
    "    t1_df = df[df['Time'] == 1]\n",
    "    output_list.append(t1_df)\n",
    "    \n",
    "    # Get session Y05 & remove columns not measured in this session\n",
    "    t2_df = df[df['Time'] == 2]\n",
    "    output_list.append(t2_df)\n",
    "    \n",
    "    # Create dataset for sessions  greater than Y05\n",
    "    if max_tp >= 3:\n",
    "        for i in range(3,max_tp+1):\n",
    "            t_df = df[df['Time'] == max_tp]\n",
    "            output_list.append(t_df)\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a33cac-6413-4576-a02a-29a5da572607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_df(df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df : (dataframe)\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Imputes the categorical and numerical features of the given dataframe with the 'Time + Type' imputation method\n",
    "    \"\"\"\n",
    "    # Split the dataframe by time point:\n",
    "    tp_split_ls = split_df_by_time(df)\n",
    "\n",
    "    # initialise the output list for the imputed unique sessions datasets\n",
    "    imp_split_ls = []\n",
    "\n",
    "    for df in tp_split_ls:\n",
    "        # Create the lists of categorical and continuous features & get the label column.\n",
    "        label_col = ['MStype']\n",
    "        cat_cols = ['Sex', 'HC_CI_CP']\n",
    "        num_cols = [col for col in df.columns if col not in ['Time', label_col[0], 'PRESGENE_ID', 'Age_group'] + cat_cols]\n",
    "\n",
    "        # Save ID columns\n",
    "        PRESGENE_IDs = df[['PRESGENE_ID']]\n",
    "        noID_df = df.drop(columns = 'PRESGENE_ID')\n",
    "        \n",
    "        # Ensure numerical columns have numeric type\n",
    "        for col in num_cols:\n",
    "            noID_df[col] = pd.to_numeric(noID_df[col], errors='coerce')\n",
    "\n",
    "        imputed_df = noID_df.copy()\n",
    "\n",
    "        # Impute NaN for MStype columns.\n",
    "        imputed_df['MStype'] = imputed_df['MStype'].fillna(noID_df['MStype'].mode()[0])\n",
    "\n",
    "        # Impute NaN for categorical columns.\n",
    "        for col in cat_cols:\n",
    "            imputed_df[col] = imputed_df.groupby(label_col, observed=True)[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    \n",
    "        # Impute NaN for continuous columns.\n",
    "        for col in num_cols:\n",
    "            imputed_df[col] = imputed_df.groupby(label_col, group_keys = True)[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "        # Ensure correct class for MS based on MStype value after imputation\n",
    "        imputed_df['MS'] = imputed_df['MStype'].apply(lambda x: 2 if x == 0 else 1)\n",
    "        \n",
    "        # Apply rounding and ensure correct values in EDSS\n",
    "        imputed_df['EDSS'] = imputed_df['EDSS'].apply(lambda x: 0 if x < 0.5 else (1 if x < 1.5 else round(x * 2) / 2 if x != 999 else 999))\n",
    "        imputed_df[['Time', 'Sex', label_col[0], 'HC_CI_CP']] = imputed_df[['Time', 'Sex', label_col[0], 'HC_CI_CP']].round().astype(int)\n",
    "        working_df = pd.concat([PRESGENE_IDs, imputed_df], axis = 1)\n",
    "\n",
    "        imp_split_ls.append(working_df)\n",
    "    \n",
    "    return imp_split_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97755649-ac6f-4da8-809e-041a64b7cbc7",
   "metadata": {},
   "source": [
    "## Functions for fixing class imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc1b49-066b-44cc-92df-2ee0ab76712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_by_rows(ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_df : (list of dataframes) list of dataframes with unique sessions\n",
    "    OUTPUT: list of dataframes\n",
    "    DESCRIPTION: Merges the datasets such that each participant has 1 row instead of n rows for the n sessions.\n",
    "    \"\"\"\n",
    "    # Preserve `Sex` and `MS` columns only in the first DataFrame\n",
    "    for idx in range(1, len(ls_df)):\n",
    "        ls_df[idx] = ls_df[idx].drop(['Sex', 'MS'], axis=1, errors='ignore')\n",
    "\n",
    "    # Combine the time point split datasets row-wise\n",
    "    concat_df = pd.merge(ls_df[0], ls_df[1], on='PRESGENE_ID', how='left', \n",
    "                         suffixes=('_tp1', '_tp2'))\n",
    "\n",
    "    # If `tp3` is included\n",
    "    if len(ls_df) == 3:\n",
    "        df3 = ls_df[2].rename(columns={col: col + '_tp3' for col in ls_df[2].columns if col != 'PRESGENE_ID'})\n",
    "        concat_df = pd.merge(concat_df, df3, on='PRESGENE_ID', how='left')\n",
    "\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4f19e-890c-4ad5-9812-9a4854d15d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_SMOTE_dfs(df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframes) dataframe with the 1 row per participant format created by the functionmerge_time_by_rows\n",
    "    output: list with first a dataframe of the merged sessions with X rows per participant per X sessions, followed by the unique sessions\n",
    "    datasets.\n",
    "    DESCRIPTION: Splits the input dataframe into unique sessions, remove the session specific suffix from the column names, return a \n",
    "    list of the unique sessions dataset and a version with the merged sessions.\n",
    "    \"\"\"\n",
    "    # Reset index to use it as 'PRESGENE_ID' substitute\n",
    "    df_idx = df.reset_index(drop = False)\n",
    "\n",
    "    # Get the column names for the time splits    \n",
    "    tp1_cols = ['index', 'Sex', 'MS']\n",
    "    tp2_cols = ['index', 'Sex', 'MS']\n",
    "    tp3_cols = ['index', 'Sex', 'MS']\n",
    "\n",
    "    \n",
    "    for col in df_idx.columns:\n",
    "        if 'tp3' in col and col not in ['index_tp3','Sex_tp3', 'MS_tp3']:\n",
    "            tp3_cols.append(col)\n",
    "        elif 'tp2' in col and col not in ['index_tp2','Sex_tp2', 'MS_tp2']:\n",
    "            tp2_cols.append(col)\n",
    "        elif 'tp1' in col:\n",
    "            tp1_cols.append(col)\n",
    "\n",
    "    # Get the n timepoints and make n new subset dfs\n",
    "    df_tp1 = df_idx[tp1_cols]\n",
    "    df_tp2 = df_idx[tp2_cols]\n",
    "    df_tp3 = None\n",
    "    \n",
    "    if len(tp3_cols) > 3:\n",
    "        df_tp3 = df_idx[tp3_cols]\n",
    "\n",
    "    # Rename the columns\n",
    "    df_tp1.columns = df_tp1.columns.str.replace('_tp1', '', regex=False)\n",
    "    df_tp2.columns = df_tp2.columns.str.replace('_tp2', '', regex=False)\n",
    "    comb_ls = [df_tp1, df_tp2]\n",
    "    if df_tp3 is not None:\n",
    "        df_tp3.columns = df_tp3.columns.str.replace('_tp3', '', regex=False)\n",
    "        comb_ls.append(df_tp3)\n",
    "\n",
    "    # Make the combined tp dataset\n",
    "    combined_df = pd.concat(comb_ls, ignore_index=True)\n",
    "\n",
    "    return [combined_df] + comb_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c145db-9e0f-4fcb-ada1-d4dda00d7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_rebalancing_SMOTE(ls_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_df : (nested list of dataframes) nested list containing the imputed datasets, the first list for the MS unique sessions datasets \n",
    "    and a second for ALL unique sessions datasets.\n",
    "    OUTPUT: lists of dataframes containing the combined sessions and the unique sessions datasets\n",
    "    DESCRIPTION: Uses SMOTE to fix class imbalance in the (HC +) MS phenotypes in the datasets\n",
    "    respectively. \n",
    "    \"\"\"\n",
    "    # Combine the unique time points row-wise\n",
    "    rowco_df = merge_time_by_rows(ls_df)\n",
    "    smote_ft = 'MStype_tp1'\n",
    "            \n",
    "    ### SMOTE\n",
    "    # Define X & y\n",
    "    X = rowco_df.drop([smote_ft, 'PRESGENE_ID'], axis=1)\n",
    "    y = rowco_df[smote_ft]\n",
    "\n",
    "    # Find frequency of the majority class in MS phenotypes\n",
    "    resample_size = y.value_counts().max() * 2\n",
    "\n",
    "    # Create a sampling strategy for classes with < 2 samples  \n",
    "    small_classes = y.value_counts()[y.value_counts() < 2].index\n",
    "    sampling_strategy = {label: resample_size for label in y.unique() if label not in small_classes}\n",
    "    \n",
    "    # Dynamic number of neighbors (3 or smallest class, at least 1)\n",
    "    min_samples = y.value_counts().min()\n",
    "    min_neighbors = max(1, min(3, min_samples - 1))\n",
    "\n",
    "    smote_model = SMOTE(sampling_strategy = sampling_strategy, k_neighbors = min_neighbors, random_state=42) \n",
    "    X_SMOTE, y_SMOTE = smote_model.fit_resample(X, y)\n",
    "\n",
    "    output_df = pd.DataFrame(X_SMOTE, columns = X.columns)\n",
    "    output_df = pd.concat([output_df, pd.Series(y_SMOTE, name=smote_ft)], axis=1)\n",
    "    \n",
    "    # Reformat the extrapolated EDSS values\n",
    "    edss_cols = [col for col in output_df.columns if col.startswith('EDSS')]\n",
    "    for col in edss_cols:\n",
    "        output_df[col] = output_df[col].apply(lambda x: 999 if x >= 10.5 else \n",
    "                                              (0 if x < 0.5 else \n",
    "                                               (1 if x < 1.5 else \n",
    "                                                round(x * 2) / 2 if x != 999 else 999)))\n",
    "\n",
    "    # Ensure the MS and MStype columns remain integers\n",
    "    label_cols = [col for col in output_df.columns if col.startswith('MS')]\n",
    "    for col in label_cols:\n",
    "        output_df[col] = output_df[col].round().astype(int)\n",
    "        \n",
    "    # Reformat the dataset from 1 row per subject to n row per n time points per subject.\n",
    "    output_ls = reformat_SMOTE_dfs(output_df)\n",
    "\n",
    "    return output_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910ed98-52b1-4d19-8694-7eff5d9550d4",
   "metadata": {},
   "source": [
    "## Functions for dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321f554-12c6-4bad-9d80-7f5670bfe996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_unique_UMAP(train_df, test_df, perp, dist, metric):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    train_df : (Dataframe) Containing the training data for fitting and embedding.\n",
    "    test_df : (Dataframe) Containing the test data for embedding.\n",
    "    perp : (Integer) the perplexity value to use during UMAP embedding.\n",
    "    dist : (Integer) the minimum distance value to use during UMAP embedding.\n",
    "    metric : (String) name of the metric to use during UMAP embedding.\n",
    "    OUTPUT: 2 dataframes with UMAP embedded features (first is the training df and then the testing df)\n",
    "    DESCRIPTION: Removes columns not belonging to clinical, demographic, sMRI or dMRI before embedding. fits the UMAP embeddings to the\n",
    "    train dataset and embeds the test set with the fitted UMAP. converts the array to data frames.\n",
    "    is also saved with the given df name as a .csv file. \n",
    "    \"\"\"\n",
    "    # Get name, perplexity, learning rate and label column name for the df\n",
    "    label_col = [col for col in train_df.columns if col.startswith('MS')]\n",
    "    index_col = train_df.columns[0]\n",
    "\n",
    "    # Remove target variables and normalise the train df\n",
    "    norm_train = train_df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', 'HC_CI_CP', index_col] + label_col, axis = 1)\n",
    "    norm_train = StandardScaler().fit_transform(norm_train) \n",
    "    # Remove target variables and normalise the test df\n",
    "    norm_test = test_df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', 'HC_CI_CP', index_col] + label_col, axis = 1)\n",
    "    norm_test = StandardScaler().fit_transform(norm_test) \n",
    "\n",
    "    # Run the UMAP model\n",
    "    umap_model = UMAP(n_components = 2, n_neighbors = perp, min_dist = dist, \n",
    "                      metric = metric, random_state = 42)\n",
    "    train_array = umap_model.fit_transform(norm_train)\n",
    "    test_array = umap_model.transform(norm_test)\n",
    "\n",
    "    # Create the output dfs\n",
    "    train_output = pd.DataFrame(train_array)\n",
    "    train_idx_df = train_df.loc[:, [index_col, 'Time']]\n",
    "    train_idx_df = train_idx_df.reset_index(drop=True)\n",
    "    train_output = pd.concat([train_idx_df, train_output],axis = 1)\n",
    "\n",
    "    test_output = pd.DataFrame(test_array)\n",
    "    test_idx_df = test_df.loc[:, [index_col, 'Time']]\n",
    "    test_idx_df = test_idx_df.reset_index(drop=True)\n",
    "    test_output = pd.concat([test_idx_df, test_output],axis = 1)\n",
    "\n",
    "    return train_output, test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157488-14a3-4281-babe-e285b80396a7",
   "metadata": {},
   "source": [
    "## Functions for Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2dc70b-5df2-4b5f-bbd5-740d36c68325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dependent(transformed_df, df, task_ft):\n",
    "    \"\"\"\n",
    "    INPUTS :\n",
    "    transformed_df : (dataframe) to which he engineered feature should be added to\n",
    "    df : (data frame) smote oversampled df with combined sessions from which the features will be engineered\n",
    "    task_ft : (string) the name of the dependent variable.\n",
    "    OUTPUT: 1 dataframe\n",
    "    DESCRIPTION: Use the existing clinical columns: EDSS, BL_Avg_cognition, MS, MStype to make the new dependent variables: bi_EDSS, \n",
    "    tri_EDSS, worsening_EDSS, and RR_to_SP. The feature corresponding to the task_ft is added to the transformed_df as the target variable. \n",
    "    \"\"\"\n",
    "    #Isolate the wanted columns\n",
    "    index_col = transformed_df.columns[0]\n",
    "    dep_df = df[[index_col, 'Time']]\n",
    "    \n",
    "    if task_ft == 'BL_Avg_cognition':\n",
    "        # Round the average cognition score to 2 decimal points (as in the original datasets)\n",
    "        dep_df.loc[:, 'BL_Avg_cognition'] = df.loc[:,'BL_Avg_cognition'].round(2) \n",
    "\n",
    "    elif task_ft == 'bi_EDSS':\n",
    "        # Make the bi_EDSS (EDSS ≤ 5.5 = 1 lower bound for worsening cutoff. EDSS > 5.5 upper bound for worsening EDSS cutoff)\n",
    "        dep_df.loc[:, 'bi_EDSS'] = df.loc[:, 'EDSS'].apply(lambda x: 0 if x > 5.5 else 1)\n",
    "\n",
    "    elif task_ft == 'tri_EDSS':\n",
    "        # Make the tri_EDSS (EDSS ≤ 3.5 = 1 (mild), 3.5 < EDSS ≤ 6.0 = 2 (moderate), EDSS ≥ 6.5 = 3 (severe)) feature, 4 = HC\n",
    "        dep_df.loc[:, 'tri_EDSS'] = df.loc[:, 'EDSS'].apply(lambda x: 1 if x <= 3.5 else (2 if x > 3.5 and x <= 6.0 else (3 if x > 6.0 and x <= 10.0 else 4)))\n",
    "\n",
    "    elif task_ft == 'worsening_EDSS':\n",
    "        # Make the 'EDSS_worsening' feature (if : change ≥ 1.0 if baseline EDSS ≤ 5.5 or change ≥ 0.5 if baseline EDSS > 5.5)\n",
    "        max_tp = df['Time'].unique().tolist()[-1]\n",
    "        dep_df.loc[:, 'worsening_EDSS'] = df.groupby(by = index_col)['EDSS'].transform(lambda x: (\n",
    "                                                                                        1 if (\n",
    "                                                                                            (x[df['Time'] == max_tp].values[0] - x[df['Time'] == 1].values[0] >= 0.5 and x[dep_df['Time'] == 1].values[0] > 5.5) or \n",
    "                                                                                            (x[df['Time'] == max_tp].values[0] - x[df['Time'] == 1].values[0] >= 1.0 and x[dep_df['Time'] == 1].values[0] <= 5.5)\n",
    "                                                                                        ) \n",
    "                                                                                        else 0))\n",
    "\n",
    "\n",
    "    elif task_ft == 'RR_to_SP':\n",
    "        max_tp = df['Time'].max()\n",
    "        \n",
    "        # Calculate RR_to_SP for each group\n",
    "        rr_to_sp = (\n",
    "            df.groupby(index_col)[['Time', 'MStype']]\n",
    "            .apply(\n",
    "                lambda group: int(\n",
    "                    not group[group['Time'] == 1].empty and \n",
    "                    not group[group['Time'] == max_tp].empty and\n",
    "                    group.loc[group['Time'] == 1, 'MStype'].values[0] == 3 and\n",
    "                    group.loc[group['Time'] == max_tp, 'MStype'].values[0] == 2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Map RR_to_SP back to all rows in the original DataFrame\n",
    "        dep_df.loc[:, 'RR_to_SP'] = df[index_col].map(rr_to_sp)\n",
    "        \n",
    "        # Fill NaN values with 0\n",
    "        dep_df.loc[:,'RR_to_SP'] = dep_df['RR_to_SP'].fillna(0).astype(int)\n",
    "    \n",
    "    elif task_ft == 'MS':\n",
    "        # Conver MS to proper encoding: 0 if HC, 1 if MS\n",
    "        dep_df.loc[:, 'MS'] = df['MS']\n",
    "        dep_df.loc[dep_df['MS'] == 2, 'MS'] = 0\n",
    "\n",
    "    elif task_ft == 'EDSS':\n",
    "        # Reformat EDSS if task_ft\n",
    "        dep_df.loc[:, 'EDSS'] = df['EDSS']\n",
    "        dep_df.loc[:, 'EDSS'] = dep_df['EDSS'].apply(lambda x: 0 if x < 0.5 else (1 if x < 1.5 else round(x * 2) / 2 if x != 999 else 999))\n",
    "\n",
    "    else:\n",
    "        dep_df.loc[:, task_ft] = df[task_ft]\n",
    "\n",
    "    # Prep the transformed df for merging with the dependent feature df\n",
    "    dep_var_cols = [col for col in dep_df.columns if col not in [index_col, 'Time']]\n",
    "    working_df = transformed_df[[col for col in transformed_df.columns if col not in dep_var_cols]]\n",
    "\n",
    "    # Combine the dependent feature with the transformed or raw df \n",
    "    output_df = pd.merge(dep_df, working_df, on=[index_col, 'Time'], how='inner')\n",
    "\n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a1f01-228f-41b1-b910-fb042b893a54",
   "metadata": {},
   "source": [
    "# Step 3: Set up the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b61f-a375-442c-9ea6-723a2a85f221",
   "metadata": {},
   "source": [
    "## General MLA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37650f-5d20-42bb-9c1b-0222171b21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDSSLabelEncoder:\n",
    "    # Create a class for EDSS value encoding\n",
    "    def __init__(self):\n",
    "        # Define the mapping from float to integer\n",
    "        self.mapping = {\n",
    "            0.0: 0,\n",
    "            1.0: 1,\n",
    "            1.5: 2,\n",
    "            2.0: 3,\n",
    "            2.5: 4,\n",
    "            3.0: 5,\n",
    "            3.5: 6,\n",
    "            4.0: 7,\n",
    "            4.5: 8,\n",
    "            5.0: 9,\n",
    "            5.5: 10,\n",
    "            6.0: 11,\n",
    "            6.5: 12,\n",
    "            7.0: 13,\n",
    "            7.5: 14,\n",
    "            8.0: 15,\n",
    "            8.5: 16,\n",
    "            9.0: 17,\n",
    "            9.5: 18,\n",
    "            10.0: 19,\n",
    "            999.0: 20,\n",
    "        }\n",
    "        # Create the inverse mapping\n",
    "        self.inverse_mapping = {v: k for k, v in self.mapping.items()}\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return [self.mapping[val] for val in y]\n",
    "\n",
    "    def transform(self, y):\n",
    "        return [self.mapping[val] for val in y]\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        return [self.inverse_mapping[val] for val in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea2f6a-30f7-4d6c-98ad-e962c1125d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdm_oversampling(df, dep_ft):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df : (Dataframe) Containing embedded data and dependent feature of choice.\n",
    "    dep_ft : feature used for oversampling (target variable)\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Use random sampling to rebalance the classes in a df based on the input feature of choice. \n",
    "    \"\"\"\n",
    "    # Isolate the target variable\n",
    "    X = df.drop(dep_ft, axis = 1)\n",
    "    y = df[dep_ft]\n",
    "\n",
    "    # Initialise and fit the random over sampler (ROS)\n",
    "    ROS = RandomOverSampler(random_state=42)\n",
    "\n",
    "    # Apply ROS\n",
    "    X_ROS, y_ROS = ROS.fit_resample(X,y)\n",
    "\n",
    "    # Format the output df to match the column order of the input dataframe\n",
    "    col_idx = df.columns.get_loc(dep_ft)\n",
    "    output_df = pd.concat([X_ROS.iloc[:, :col_idx], y_ROS, X_ROS.iloc[:, col_idx:]], axis=1)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbbe17-f545-49f9-b375-b0a609b4e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score(old_score, eval_scores_dict, old_best, new_best):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    old_score: (float) value corresponding to previous eval score value\n",
    "    eval_scores_dict: (dictionary) containing scores obtained in current MLA evaluation\n",
    "    old_best: (dictionary or MLA model) contains the hps of the previous best model or the previous best model\n",
    "    new_best: (dictionary or MLA model) contains the hps of the newest model or the previous best model\n",
    "    OUTPUTS: evaluation score (float) and dictionary of hyperparameters\n",
    "    DESCRIPTION: General function for comparing the output scores of MLAs. \n",
    "    \"\"\"    \n",
    "    # For Classification tasks\n",
    "    if 'f1score' in eval_scores_dict:\n",
    "        new_f1 = eval_scores_dict['f1score']\n",
    "        if new_f1 > old_score: \n",
    "            score = new_f1\n",
    "            best = new_best\n",
    "            print(f'~~ The old F1 score was: {old_score} & the new F1 score is: {score}')\n",
    "        \n",
    "        else:\n",
    "            score = old_score\n",
    "            best = old_best\n",
    "            print('The f1score was not greater, the evaluation score remains unchanged')\n",
    "\n",
    "    # For Regression tasks\n",
    "    elif 'mse' in eval_scores_dict:\n",
    "        new_mse = eval_scores_dict['mse']\n",
    "        if new_mse < old_score:\n",
    "            score = new_mse\n",
    "            best = new_best\n",
    "            print(f'~~ The old MSE score was: {old_score}, the new MSE score is: {score}')\n",
    "\n",
    "        else:\n",
    "            score = old_score\n",
    "            best = old_best\n",
    "            print('The mse was not smaller, the evaluation score remains unchanged')\n",
    "\n",
    "    return score, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d81a08-b67f-4559-8256-2eaee8a65180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUCROC(model_fitted, X_test, y_test, task_type, model_type):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    model_fitted : (fitted model) A model fitted to the training data\n",
    "    X_test : (DataFrame) data of test subjects\n",
    "    y_test : (DataFrame) column of test DataFrame with the test labels \n",
    "    task_type : (str) nature of the (classifaication) task, 'binary' or 'multiclass'\n",
    "    model_type : (str) MLA abbreviated name ('SVM', 'LogR', 'RF', or 'XGB)'\n",
    "    OUTPUT: AUC score for the ROC and its corresponding plot\n",
    "    DESCRIPTION: Obtain and plot the ROC, and compute its AUC score for binary or multiclass tasks.\n",
    "    \"\"\"\n",
    "    # Initialise the outputs\n",
    "    roc_auc, pred_scores, output_plot, classes = None, None, None, None \n",
    "\n",
    "    if len(np.unique(y_test)) > 1:  #Ensures that AUC ROC is only calculated when more than 1 class is present\n",
    "\n",
    "        # Get the prediction scores depending on the MLA\n",
    "        if hasattr(model_fitted, \"decision_function\") or model_type == 'SVM': # For SVM\n",
    "            pred_scores = model_fitted.decision_function(X_test)\n",
    "\n",
    "            if task_type != 'binary':\n",
    "                classes = list(model_fitted.classes_)\n",
    "        \n",
    "        elif model_type == 'LSTM':\n",
    "            pred_scores = model_fitted.model.predict(X_test)\n",
    "            \n",
    "            if task_type == 'binary':\n",
    "                pred_scores = pred_scores[:, 1]\n",
    "                y_test = y_test[:, 1]\n",
    "\n",
    "            else:\n",
    "                y_test = np.argmax(y_test, axis=1)\n",
    "                classes = list(range(pred_scores.shape[1]))\n",
    "                print(classes)\n",
    "                \n",
    "        else:\n",
    "            if task_type == 'binary':\n",
    "                pred_scores = model_fitted.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                pred_scores = model_fitted.predict_proba(X_test)\n",
    "                classes = list(model_fitted.classes_)\n",
    "        \n",
    "        if task_type == 'binary': # Binary ROC\n",
    "            # Predict probabilities & find TPR, FPR at varying thresholds\n",
    "            fpr, tpr, threshold = roc_curve(y_test, pred_scores, pos_label=1)\n",
    "    \n",
    "            # auc score\n",
    "            roc_auc = roc_auc_score(y_test, pred_scores)\n",
    "            \n",
    "            # plot the ROC curve\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.plot(fpr, tpr, linestyle = '-',color = '#1E8449', label=f'{model_type} (AUC = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], linestyle = '--', color = '#17202A', label = 'Random (AUC = 0.50)')\n",
    "            plt.title(f'Binary ROC Curve for {model_type}')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive rate')\n",
    "            plt.legend(loc='lower right', prop = {'size':10})\n",
    "            \n",
    "            output_plot = fig\n",
    "    \n",
    "        else: # Multiclass ROC \n",
    "            fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "            fig, ax = plt.subplots()\n",
    "            common_classes = [cls for cls in classes if cls in np.unique(y_test)]\n",
    "\n",
    "            # Calculate ROC curve and AUC for each class\n",
    "            if model_type == 'LSTM':\n",
    "                for cls in common_classes:\n",
    "                    fpr[cls], tpr[cls], _ = roc_curve(y_test == cls, pred_scores[:, cls])\n",
    "                    roc_auc[cls] = roc_auc_score(y_test == cls, pred_scores[:, cls])\n",
    "\n",
    "            else:\n",
    "                for cls in common_classes:\n",
    "                    fpr[cls], tpr[cls], _ = roc_curve(y_test == cls, pred_scores[:, classes.index(cls)])\n",
    "                    roc_auc[cls] = roc_auc_score(y_test == cls, pred_scores[:, classes.index(cls)])\n",
    "        \n",
    "            # Plot ROC curves\n",
    "            colors = [ 'pink', 'orange', 'purple', '#C0392B', '#2980B9', '#3498DB', '#1ABC9C', '#16A085', '#27AE60', '#F1C40F', '#E67E22', \n",
    "                     '#D35400', '#F5B7B1', '#D7BDE2', '#AED6F1', '#A3E4D7', '#A9DFBF', '#F9E79F', '#FAD7A0', '#EDBB99', '#641E16',\n",
    "                     '#4A235A', '#154360', '#117864', '#196F3D']\n",
    "            for cls, color in zip(common_classes, colors):\n",
    "                plt.plot(fpr[cls], tpr[cls], color=color, lw=2, label=f'Class {cls} (AUC = {roc_auc[cls]:.2f})')\n",
    "                \n",
    "            plt.plot([0, 1], [0, 1], color='#17202A', lw=2, linestyle='--', label='Random (AUC = 0.50)')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.title(f'Multiclass ROC Curve for {model_type}')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.legend(loc='lower right', prop = {'size':7})\n",
    "    \n",
    "            output_plot = fig\n",
    "\n",
    "        return roc_auc, output_plot\n",
    "\n",
    "    else:\n",
    "        print('Only 1 class was present in the test set, no aucroc will be returned.')\n",
    "        \n",
    "        return {}, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a5d2c-510f-47ad-bfb8-2f211f8cfcfd",
   "metadata": {},
   "source": [
    "## Functions for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025bbd1-c75e-435d-8053-60af9013b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_LSTM_data(df, task_ft):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df : (dataframe)\n",
    "    task_ft : (String) name of the column to use as target variable.\n",
    "    OUTPUT: dataset\n",
    "    DESCRIPTION: Removes irrelevant columns and restructures the dataset to 1 row per subject (structure needed for LSTM input).\n",
    "    \"\"\"\n",
    "    ## Split the dataset per time point\n",
    "    index_col = df.columns[0]\n",
    "    unique_tps = df['Time'].unique().tolist()\n",
    "    max_tp = max(unique_tps)\n",
    "    dep_var_ls = ['MS', 'MStype', 'EDSS', 'BL_Avg_cognition', 'HC_CI_CP', 'bi_EDSS',\n",
    "              'tri_EDSS', 'worsening_EDSS', 'RR_to_SP']\n",
    "    dep_var_ls.remove(task_ft)\n",
    "\n",
    "    filtered_df = df[[col for col in df.columns if col not in dep_var_ls]]\n",
    "\n",
    "    #Make tp 1 base datset\n",
    "    working_df = filtered_df[filtered_df['Time'] == 1]\n",
    "    working_df = working_df.drop([task_ft], axis = 1)\n",
    "    \n",
    "    ## Combine the time points such that each subject only has 1 row\n",
    "    for tp in unique_tps:\n",
    "        if tp == 2:\n",
    "            ### Combine the time points for the datasets\n",
    "            working_df = working_df.merge(filtered_df[filtered_df['Time'] == tp], on=index_col, how='outer',\n",
    "                                     suffixes=('_tp1', '_tp2'))\n",
    "\n",
    "            if max_tp > 2:\n",
    "                    working_df = working_df.drop([task_ft], axis = 1)\n",
    "\n",
    "        elif tp > 2:  \n",
    "            ### Add the appropriate suffix based on the corresponding time point for time points greater than 2\n",
    "            add_working_df = filtered_df[filtered_df['Time'] == tp].rename(columns={col: str(col) + '_tp' + str(tp) for col in filtered_df.columns if col not in [index_col, task_ft]})\n",
    "\n",
    "            ### Combine the time points for the datasets\n",
    "            working_df = working_df.merge(add_working_df, on=index_col, how='outer')\n",
    "\n",
    "            if max_tp > tp:\n",
    "                working_df = working_df.drop([task_ft], axis = 1)\n",
    "    \n",
    "    ## Remove the time columns as they will not be needed further\n",
    "    working_df = working_df.drop([col for col in working_df.columns if col.startswith('Time') or col == index_col], axis = 1)\n",
    "\n",
    "    return working_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7942b8-cf2a-44fa-80c3-e7b6d866fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm(train_df, test_df, task_ft, transformation, num_tps, outer_run, inner_run):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    train_df : (dataframe) dataframe of embedded training values\n",
    "    test_df : (dataframe) dataframe of embedded testing values\n",
    "    task_ft : (String) name of the column to use as label column.\n",
    "    transformation: (string) The mthod of transformation to apply to the dataset (None/UMAP)\n",
    "    num_tps: (integer) number of the highest session to include (0 = Y00, 1 = Y05, 3 = Y10)\n",
    "    outer_run: (integer) current outer run\n",
    "    inner_run: (integer) current inner run\n",
    "    OUTPUT:  float of the evaluation metrics and the corresponding hp\n",
    "    DESCRIPTION: Tunes the LSTM for the inner runs of the nCV\n",
    "    \"\"\"\n",
    "    index_col = train_df.columns[0]\n",
    "    num_tps = len(train_df['Time'].unique().tolist())\n",
    "    output_score = {}\n",
    "    dep_cols = ['PRESGENE_ID', 'Time', 'MS', 'MStype', 'BL_Avg_cognition', 'HC_CI_CP','bi_EDSS', 'tri_EDSS', 'worsening_EDSS', 'RR_to_SP', 'EDSS']\n",
    "\n",
    "    if len(train_df['Time'].unique()) > 1: # Multiple tps need 1 row per subject rearrangement\n",
    "        train_df = prep_LSTM_data(train_df, task_ft)\n",
    "        test_df = prep_LSTM_data(test_df, task_ft)\n",
    "        print('Datasets now have 1 row per subject')\n",
    "    \n",
    "    # Determine task type (Regression or Classification)\n",
    "    if task_ft in ['BL_Avg_cognition']:\n",
    "        print('The task for this run is: Regression')\n",
    "        # Bootstrapping\n",
    "        n_samples = train_df.shape[0] * 2\n",
    "        bstrp_df = resample(train_df, replace=True, n_samples = n_samples, random_state=42)\n",
    "        #print(f\" the new number of rows of the df after bootstrapping is {bstrp_df.shape[0]} compared to the initial {train_df.shape[0]} rows\")\n",
    "\n",
    "        # Define variables\n",
    "        temp_X_train = bstrp_df[[col for col in bstrp_df.columns if col not in dep_cols]].values\n",
    "        X_train = temp_X_train.reshape((temp_X_train.shape[0], num_tps, int(temp_X_train.shape[1]/num_tps)))\n",
    "        y_train = bstrp_df[[task_ft]].values\n",
    "        \n",
    "        temp_X_test = test_df[[col for col in bstrp_df.columns if col not in dep_cols]].values\n",
    "        X_test = temp_X_test.reshape((temp_X_test.shape[0], num_tps, int(temp_X_test.shape[1]/num_tps)))\n",
    "        y_test = test_df[[task_ft]].values\n",
    "\n",
    "        # define LSTM creator for the random search\n",
    "        def lstm_regressor_inner(hp):\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(hp.Int('input_unit', min_value=10, max_value=400, step=20), return_sequences=True, \n",
    "                           input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        \n",
    "            # Varying number of layers\n",
    "            for i in range(hp.Int('n_layers', 1, 6)):\n",
    "                model.add(LSTM(hp.Int(f'lstm_{i}_units', min_value=10, max_value=400, step=20), return_sequences=True))\n",
    "                \n",
    "                if random.choice([True, False]): # Randomly include dropout statement or not\n",
    "                    model.add(Dropout(hp.Float(f'Dropout_rate_for_unit_{i}', min_value=0, max_value=0.5, step=0.1)))\n",
    "        \n",
    "            model.add(LSTM(hp.Int('layer_final_neurons', min_value=10, max_value=400, step=20)))\n",
    "            \n",
    "            if random.choice([True, False]):\n",
    "                model.add(Dropout(hp.Float('Dropout_rate_final', min_value=0, max_value=0.5, step=0.1)))\n",
    "                \n",
    "            model.add(Dense(1, activation=hp.Choice('dense_activation', values=['relu', 'sigmoid', 'linear'], default='linear'))) \n",
    "            model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "        \n",
    "            return model\n",
    "    \n",
    "        # Random search\n",
    "        regression_tuner = RandomSearch(\n",
    "            lstm_regressor_inner, \n",
    "            objective='mse', \n",
    "            max_trials=10, \n",
    "            executions_per_trial=1, \n",
    "            directory=f'updated_data/nested_CV_final/LSTM/{transformation}/models', \n",
    "            project_name=f'model_{transformation}{num_tps}{outer_run}{inner_run}{task_ft}')\n",
    "\n",
    "        regression_tuner.search(x=X_train, y=y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "        # Get best MSE & hps\n",
    "        best_model = regression_tuner.get_best_models(num_models=1)[0]\n",
    "        _, output_score['mse'] = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "        best_hps = regression_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    else: # Classification Task\n",
    "        print(f'The task for this run is: Classification, the task feature is: {task_ft}')\n",
    "        rs_df_train = train_df.copy() \n",
    "        rs_df_train.columns = rs_df_train.columns.astype(str)\n",
    "        rs_df_test = test_df.copy()\n",
    "        rs_df_test.columns = rs_df_test.columns.astype(str)\n",
    "        label_encoder = None\n",
    "        \n",
    "        # Class Rebalancing with random over sampling\n",
    "        if task_ft == 'EDSS':\n",
    "            label_encoder = EDSSLabelEncoder()\n",
    "            rs_df_train['EDSS'] = label_encoder.fit_transform(rs_df_train['EDSS'])\n",
    "            rs_df_test['EDSS'] = label_encoder.transform(rs_df_test['EDSS'])\n",
    "\n",
    "            rs_df_train = rdm_oversampling(rs_df_train, task_ft)\n",
    "\n",
    "        elif len(set(Counter(train_df[task_ft].values))) != 1:\n",
    "            rs_df_train = rdm_oversampling(rs_df_train, task_ft)\n",
    "            print(f'the rbalanced classes are: {rs_df_train[task_ft].value_counts()}')\n",
    "\n",
    "        # Define variables\n",
    "        temp_X_train = rs_df_train[[col for col in rs_df_train.columns if col not in dep_cols]].values\n",
    "        X_train = temp_X_train.reshape((temp_X_train.shape[0], num_tps, int(temp_X_train.shape[1]/num_tps)))\n",
    "\n",
    "        temp_X_test = rs_df_test[[col for col in rs_df_test.columns if col not in dep_cols]].values\n",
    "        X_test = temp_X_test.reshape((temp_X_test.shape[0], num_tps, int(temp_X_test.shape[1]/num_tps)))\n",
    "\n",
    "        ## Get the one hot encoded y_train & y_test\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "        y_train = ohe.fit_transform(rs_df_train[[task_ft]].values)\n",
    "        y_test = ohe.transform(rs_df_test[[task_ft]].values)\n",
    "        \n",
    "        dense_output = y_train.shape[1]\n",
    "\n",
    "        # define LSTM creator for the random search\n",
    "        def lstm_classifier_inner(hp):\n",
    "            model = Sequential()\n",
    "        \n",
    "            model.add(LSTM(hp.Int('input_unit',min_value=10,max_value=400,step=20),\n",
    "                           return_sequences=True, \n",
    "                           input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "           \n",
    "            for i in range(hp.Int('n_layers', 1, 6)):\n",
    "                model.add(LSTM(hp.Int(f'lstm_{i+1}_units',min_value=10,max_value=400,step=20),return_sequences=True))\n",
    "        \n",
    "                if random.choice([True, False]): # Randomly include dropout statement or not\n",
    "                    model.add(Dropout(hp.Float(f'Dropout_rate_for_unit_{i}', min_value=0, max_value=0.5, step=0.1)))\n",
    "        \n",
    "            model.add(LSTM(hp.Int('layer_final_neurons', min_value=10, max_value=400, step=20),return_sequences=False))\n",
    "            \n",
    "            if random.choice([True, False]):\n",
    "                model.add(Dropout(hp.Float('Dropout_rate_final', min_value=0, max_value=0.5, step=0.1)))\n",
    "            \n",
    "            model.add(Dense(dense_output, activation='softmax'))\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "            \n",
    "            return model\n",
    "    \n",
    "        # Random search\n",
    "        classifier_tuner = RandomSearch(\n",
    "            lstm_classifier_inner, \n",
    "            objective= 'val_accuracy',\n",
    "            max_trials=10, \n",
    "            executions_per_trial=1, \n",
    "            directory=f'updated_data/nested_CV_final/LSTM/{transformation}/models', \n",
    "            project_name=f'model_{transformation}{num_tps}{outer_run}{inner_run}{task_ft}')\n",
    "\n",
    "        classifier_tuner.search(x=X_train, y=y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "        # Get best MSE & hps\n",
    "        best_model = classifier_tuner.get_best_models(num_models=1)[0]\n",
    "        output_score['mse'], acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "        best_hps = classifier_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print('val loss:', output_score['mse'])\n",
    "\n",
    "    return output_score, best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e48e4-4852-4c44-8ab6-a9a01a2cf02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_regressor_outer(best_hps, X_train):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    best_hps : (dictionary) contains the hps to build the LSTM with\n",
    "    X_train:  (dataframe) train dataset without target variable\n",
    "    OUTPUT: LSTM Regressor\n",
    "    DESCRIPTION: Create the LSTM regressor based on the given hps and the shape of the input train dataset for the outer runs of the nCV\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_hps['input_unit'], return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    # Adding layers based on the hyperparameters\n",
    "    for i in range(best_hps['n_layers']):\n",
    "        model.add(LSTM(best_hps[f'lstm_{i}_units'], return_sequences=True))\n",
    "        \n",
    "        if f'Dropout_rate_for_unit_{i}' in best_hps:\n",
    "            model.add(Dropout(best_hps[f'Dropout_rate_for_unit_{i}']))\n",
    "    \n",
    "    model.add(LSTM(best_hps['layer_final_neurons']))\n",
    "    \n",
    "    if 'Dropout_rate_final' in best_hps:\n",
    "        model.add(Dropout(best_hps['Dropout_rate_final']))\n",
    "    \n",
    "    model.add(Dense(1, activation=best_hps['dense_activation']))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c8256-7a57-422c-87e6-0b129907d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_classifier_outer(best_hps, X_train, dense_output):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    best_hps : (dictionary) contains the hps to build the LSTM with\n",
    "    X_train:  (dataframe) train dataset without target variable\n",
    "    dense_output: (integer) number of classes in y_train (the target variable)\n",
    "    OUTPUT: LSTM Classifier\n",
    "    DESCRIPTION: Create the LSTM classifier based on the given hps and the shape of the input train dataset for the outer runs of the nCV\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_hps['input_unit'], return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "   \n",
    "    for i in range(best_hps['n_layers']):\n",
    "        model.add(LSTM(best_hps[f'lstm_{i+1}_units'], return_sequences=True))\n",
    "        \n",
    "        if f'Dropout_rate_for_unit_{i}' in best_hps:\n",
    "            model.add(Dropout(best_hps[f'Dropout_rate_for_unit_{i}']))\n",
    "\n",
    "    model.add(LSTM(best_hps['layer_final_neurons']))\n",
    "    \n",
    "    if 'Dropout_rate_final' in best_hps:\n",
    "        model.add(Dropout(best_hps['Dropout_rate_final']))\n",
    "    \n",
    "    model.add(Dense(dense_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c9ec3-de64-4cc0-9641-7187a8d946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(train_df, test_df, task_ft, model_params, transformation, num_tps, outer_run):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    train_df : (dataframe) dataframe of embedded training values\n",
    "    test_df : (dataframe) dataframe of embedded testing values\n",
    "    task_ft : (String) name of the column to use as label column.\n",
    "    transformation: (string) The mthod of transformation to apply to the dataset (None/UMAP)\n",
    "    num_tps: (integer) number of the highest session to include (0 = Y00, 1 = Y05, 3 = Y10)\n",
    "    outer_run: (integer) current outer run\n",
    "    inner_run: (integer) current inner run\n",
    "    OUTPUT:  float of the evaluation metrics and the corresponding hps\n",
    "    DESCRIPTION: Builds and evaluates the LSTM for the outer runs of the nCV\n",
    "    \"\"\"\n",
    "    index_col = train_df.columns[0]\n",
    "    max_tp = len(train_df['Time'].unique().tolist())\n",
    "    output_dict = {}\n",
    "    auc_plot = None\n",
    "    dep_cols = ['PRESGENE_ID', 'Time', 'MS', 'MStype', 'BL_Avg_cognition', 'HC_CI_CP','bi_EDSS', 'tri_EDSS', 'worsening_EDSS', 'RR_to_SP', 'EDSS']\n",
    "    \n",
    "    if len(train_df['Time'].unique()) > 1: # Multiple tps need 1 row per subject rearrangement\n",
    "        train_df = prep_LSTM_data(train_df, task_ft)\n",
    "        test_df = prep_LSTM_data(test_df, task_ft)\n",
    "        print('Datasets now have 1 row per subject')\n",
    "    \n",
    "    # Determine task type (Regression or Classification)\n",
    "    if task_ft in ['BL_Avg_cognition']:\n",
    "        print('The task for this run is: Regression')\n",
    "        # Bootstrapping\n",
    "        n_samples = train_df.shape[0] * 2\n",
    "        bstrp_df = resample(train_df, replace=True, n_samples = n_samples, random_state=42)\n",
    "        #print(f\" the new number of rows of the df after bootstrapping is {bstrp_df.shape[0]} compared to the initial {train_df.shape[0]} rows\")\n",
    "\n",
    "        # Define variables\n",
    "        temp_X_train = bstrp_df[[col for col in bstrp_df.columns if col not in dep_cols]].values\n",
    "        X_train = temp_X_train.reshape((temp_X_train.shape[0], max_tp, int(temp_X_train.shape[1]/max_tp)))\n",
    "        y_train = bstrp_df[[task_ft]].values\n",
    "        \n",
    "        temp_X_test = test_df[[col for col in bstrp_df.columns if col not in dep_cols]].values\n",
    "        X_test = temp_X_test.reshape((temp_X_test.shape[0], max_tp, int(temp_X_test.shape[1]/max_tp)))\n",
    "        y_test = test_df[[task_ft]].values\n",
    "\n",
    "        # Build the LSTM\n",
    "        lstm = lstm_regressor_outer(model_params, X_train)\n",
    "        LSTM_fitted = lstm.fit(X_train, y_train, epochs= 10)\n",
    "    \n",
    "        # Model evaluation\n",
    "        y_pred = LSTM_fitted.model.predict(X_test)\n",
    "        output_dict = {\n",
    "            \"mse\": mean_squared_error(y_test, y_pred),\n",
    "            \"r2\": r2_score(y_test, y_pred),\n",
    "            \"explained_variance\": explained_variance_score(y_test, y_pred)}\n",
    "        \n",
    "    else: # Classification Task\n",
    "        print('The task for this run is: Classification')\n",
    "        rs_df_train = train_df.copy() \n",
    "        rs_df_train.columns = rs_df_train.columns.astype(str)\n",
    "        rs_df_test = test_df.copy()\n",
    "        rs_df_test.columns = rs_df_test.columns.astype(str)\n",
    "        label_encoder = None\n",
    "        \n",
    "        # Class Rebalancing with random oversampling\n",
    "        if task_ft == 'EDSS':\n",
    "            label_encoder = EDSSLabelEncoder()\n",
    "            rs_df_train['EDSS'] = label_encoder.fit_transform(rs_df_train['EDSS'])\n",
    "            rs_df_test['EDSS'] = label_encoder.transform(rs_df_test['EDSS'])\n",
    "\n",
    "            rs_df_train = rdm_oversampling(rs_df_train, task_ft)\n",
    "\n",
    "        elif len(set(Counter(train_df[task_ft].values))) != 1:\n",
    "            rs_df_train = rdm_oversampling(rs_df_train, task_ft)\n",
    "\n",
    "        # Define variables\n",
    "        temp_X_train = rs_df_train[[col for col in rs_df_train.columns if col not in dep_cols]].values\n",
    "        X_train = temp_X_train.reshape((temp_X_train.shape[0], max_tp, int(temp_X_train.shape[1]/max_tp)))\n",
    "\n",
    "        temp_X_test = rs_df_test[[col for col in rs_df_train.columns if col not in dep_cols]].values\n",
    "        X_test = temp_X_test.reshape((temp_X_test.shape[0], max_tp, int(temp_X_test.shape[1]/max_tp)))\n",
    "\n",
    "        ## Get the one hot encoded y_train & y_test\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "        y_train = ohe.fit_transform(rs_df_train[[task_ft]].values)\n",
    "        y_test = rs_df_test[[task_ft]].values\n",
    "        y_test_ohe = ohe.transform(y_test)\n",
    "\n",
    "        dense_output = y_train.shape[1]\n",
    "\n",
    "        # Build the LSTM\n",
    "        lstm = lstm_classifier_outer(model_params, X_train, dense_output)\n",
    "        LSTM_fitted = lstm.fit(X_train, y_train, epochs= 10)\n",
    "\n",
    "        # Make predictions & evaluate       \n",
    "        y_pred_probs = LSTM_fitted.model.predict(X_test)\n",
    "        y_pred_max = np.argmax(y_pred_probs, axis=1)\n",
    "        y_pred = ohe.inverse_transform(np.eye(ohe.categories_[0].shape[0])[y_pred_max])\n",
    "\n",
    "        task_type = 'binary' if train_df[task_ft].nunique() <= 2 else 'weighted'\n",
    "        auc_score, auc_plot = AUCROC(LSTM_fitted, X_test, y_test_ohe, task_type, 'LSTM')\n",
    "\n",
    "        print('auc plot is', isinstance(auc_plot, plt.Figure))\n",
    "        output_dict = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average=task_type),\n",
    "            \"recall\": recall_score(y_test, y_pred, average=task_type),\n",
    "            \"f1score\": f1_score(y_test, y_pred, average=task_type),\n",
    "            \"auc\": auc_score}\n",
    "\n",
    "        print('output_dict', output_dict, 'LSTM_fitted', LSTM_fitted, 'auc_plot', auc_plot)\n",
    "    \n",
    "    return output_dict, LSTM_fitted, auc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a744fa0-2a50-42be-8dc8-e301108504a0",
   "metadata": {},
   "source": [
    "# Step 4: Combine the Functions in the nCV Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1344adc-3aba-4444-8b4c-54dbb3c87228",
   "metadata": {},
   "source": [
    "## General Functions for nCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eac1e9-0bf6-448a-a58e-4da3b9cbe0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_subs(basedf, task_ft):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    basedf : (dataframe)\n",
    "    task_ft:  (string) name of the target variable for the current task\n",
    "    OUTPUT: A dataframe with 1 column containing the unique subjects qualified for Kfold splitting for the current target variable\n",
    "    with the string containing the updated task feature \n",
    "    DESCRIPTION: Obtains the participants suitable for stratifiedKfold for the current target variable and the name of the feature\n",
    "    to stratify the subjects based on the original task feature\n",
    "    \"\"\"\n",
    "    unique_subs = None\n",
    "    sub_split_ft = ''\n",
    "\n",
    "    if task_ft == 'EDSS' or 'EDSS' in task_ft:\n",
    "        # Update the task_ft for subject splitting\n",
    "        sub_split_ft = 'EDSS'\n",
    "        \n",
    "        # Isolate the PRESGENE_ID and subject splitting feature column\n",
    "        unique_subs = basedf[['PRESGENE_ID', sub_split_ft]].copy()\n",
    "        \n",
    "        # Fill missing value with mode class temporarily for subject splitting\n",
    "        mode_class = unique_subs[sub_split_ft].mode()[0]\n",
    "        unique_subs[sub_split_ft] = unique_subs[sub_split_ft].fillna(mode_class)\n",
    "    \n",
    "        # Encode the EDSS values \n",
    "        unique_subs[sub_split_ft] = EDSSLabelEncoder().fit_transform(unique_subs['EDSS'])\n",
    "\n",
    "        # Remove classes with a frequency of less than 3:\n",
    "        class_counts = unique_subs['EDSS'].value_counts()\n",
    "        unique_subs = unique_subs[unique_subs['EDSS'].isin(class_counts[class_counts > 2].index)]\n",
    "    \n",
    "        # Keep unique subject IDs\n",
    "        unique_subs = unique_subs.drop_duplicates(subset = ['PRESGENE_ID'], keep = 'first')\n",
    "\n",
    "    elif task_ft == 'BL_Avg_cognition': # Continuous features\n",
    "        # Update the task_ft for subject splitting\n",
    "        sub_split_ft = 'binned_continuous'\n",
    "\n",
    "        # Isolate the PRESGENE_ID column\n",
    "        unique_subs = basedf[['PRESGENE_ID']].copy()\n",
    "\n",
    "        # Fill in missing values with mean temporarily for subject splitting\n",
    "        basedf[task_ft] = basedf[task_ft].fillna(basedf[task_ft].mean())\n",
    "\n",
    "        # Convert continuous variable into categorical for subject splitting (binning)\n",
    "        bins = pd.qcut(basedf[task_ft], q=3, labels=False)\n",
    "\n",
    "        # Add new bins to subject splitting dataframe\n",
    "        unique_subs['binned_continuous'] = bins\n",
    "\n",
    "        # Keep unique subject IDs\n",
    "        unique_subs = unique_subs.drop_duplicates(subset = ['PRESGENE_ID'], keep = 'last')\n",
    "\n",
    "    elif task_ft == 'RR_to_SP':\n",
    "        # Update the task_ft for subject splitting\n",
    "        sub_split_ft = 'MStype'\n",
    "        \n",
    "        # Isolate the PRESGENE_ID and subject splitting/ task feature column\n",
    "        unique_subs = basedf[['PRESGENE_ID', sub_split_ft]].copy()\n",
    "        \n",
    "        # Fill missing value with mode class temporarily for subject splitting\n",
    "        mode_class = basedf[sub_split_ft].mode()[0]\n",
    "        unique_subs[sub_split_ft] = unique_subs[sub_split_ft].fillna(mode_class)\n",
    "\n",
    "        # Remove classes with a frequency of less than 3:\n",
    "        class_counts = basedf[sub_split_ft].value_counts()\n",
    "        unique_subs = unique_subs[unique_subs[sub_split_ft].isin(class_counts[class_counts > 2].index)]\n",
    "\n",
    "        # Convert the task feature column to integer (format needed for stratifiedKFold)\n",
    "        unique_subs[sub_split_ft] = unique_subs[sub_split_ft].astype('Int64')\n",
    "\n",
    "        # Keep unique subject IDs\n",
    "        unique_subs = unique_subs.drop_duplicates(subset = ['PRESGENE_ID'], keep = 'first')\n",
    "\n",
    "        \n",
    "    else: # other categorical features\n",
    "        # Update the task_ft for subject splitting\n",
    "        sub_split_ft = task_ft\n",
    "        \n",
    "        # Isolate the PRESGENE_ID and subject splitting/ task feature column\n",
    "        unique_subs = basedf[['PRESGENE_ID', task_ft]].copy()\n",
    "        \n",
    "        # Fill missing value with mode class temporarily for subject splitting\n",
    "        mode_class = basedf[task_ft].mode()[0]\n",
    "        unique_subs[task_ft] = unique_subs[task_ft].fillna(mode_class)\n",
    "\n",
    "        # Remove classes with a frequency of less than 3:\n",
    "        class_counts = basedf[task_ft].value_counts()\n",
    "        unique_subs = unique_subs[unique_subs[task_ft].isin(class_counts[class_counts > 2].index)]\n",
    "\n",
    "        # Convert the task feature column to integer (format needed for stratifiedKFold)\n",
    "        unique_subs[task_ft] = unique_subs[task_ft].astype('Int64')\n",
    "\n",
    "        # Keep unique subject IDs\n",
    "        unique_subs = unique_subs.drop_duplicates(subset = ['PRESGENE_ID'], keep = 'first')\n",
    "        \n",
    "    return unique_subs, sub_split_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ac568-2703-4ca1-8ece-0431970839c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_imputed(ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_df : (list of dataframes) list of dataframes with unique sessions values\n",
    "    OUTPUT: a list with the conbined sesions dataframe and the unique sessions dataframe\n",
    "    DESCRIPTION: Merges the unique sessions dataset to obtain 1 dataset with all sessions in it.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialisation of output list    \n",
    "    output_df = None\n",
    "    max_tp = len(ls_df)\n",
    "\n",
    "    # Identify common columns between the first two dataframes\n",
    "    common_columns = ls_df[0].columns.intersection(ls_df[1].columns)\n",
    "    temp_df1 = ls_df[0][common_columns]\n",
    "    temp_df2 = ls_df[1][common_columns]\n",
    "    \n",
    "    # Check if there is a third time point\n",
    "    if max_tp > 2:\n",
    "        concat_extras = []\n",
    "        for i in range(2, max_tp):\n",
    "            # Find common columns between the existing common columns and the nth dataframe\n",
    "            common_columns = common_columns.intersection(ls_df[i].columns)\n",
    "            temp_df = ls_df[i][common_columns]\n",
    "            concat_extras.append(temp_df)\n",
    "        # Combine all subsets\n",
    "        output_df = pd.concat([temp_df1, temp_df2]+concat_extras, axis=0)\n",
    "    \n",
    "    else:\n",
    "        # Combine only the first two subsets\n",
    "        output_df = pd.concat([temp_df1, temp_df2], axis=0)\n",
    "        \n",
    "    return [output_df] + ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31283d4-b5dd-48dc-b9cc-14397c698c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform_labeled(smote_inner_train_ls, smote_inner_test_ls, transformation, task_ft):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    smote_inner_train_ls : (list of dataframes) list containing the training datasets\n",
    "    smote_inner_test_ls:  (list of dataframes) list containing the testing datasets\n",
    "    transformation: (string) method of transformation None/UMAP\n",
    "    task_ft: (string) name of the target variable\n",
    "    OUTPUT: 2 lists of dataframes the first for the train datasets and the second for the test datasets like the input tarin and test lists.\n",
    "    DESCRIPTION: Applies UMAP or no transformation to the input datasets based on the requested transformation. Applies it to both the train\n",
    "    and test datasets fo the input lists. Then passes the datasets to the feature engineering function where the target variable is either\n",
    "    created or reformatted if it already existed in the base dataframe (e.g. MS, MStype...)\n",
    "    \"\"\"\n",
    "    # Initialise the output lists\n",
    "    dep_dr_inner_train_ls, dep_dr_inner_test_ls = [], []\n",
    "\n",
    "    if transformation == 'UMAP': # UMAP transformation\n",
    "        for i, smote_inner_train_df in enumerate(smote_inner_train_ls): # Iterate over the train and test input lists\n",
    "            perp = 20 if task_ft in ['MS', 'HC_CI_CP', 'BL_Avg_cognition'] else 5\n",
    "            dist = 0 if task_ft in ['MS', 'HC_CI_CP', 'BL_Avg_cognition'] else 0.5\n",
    "            train_df, test_df = apply_unique_UMAP(smote_inner_train_df, smote_inner_test_ls[i], perp, dist, 'euclidean')\n",
    "            \n",
    "            # Add engineered features if relevant for the train dataset\n",
    "            labeled_train_df = make_dependent(train_df, smote_inner_train_ls[0], task_ft)\n",
    "            dep_dr_inner_train_ls.append(labeled_train_df)\n",
    "            \n",
    "            # Add engineered features if relevant for the test dataset\n",
    "            labeled_test_df = make_dependent(test_df, smote_inner_test_ls[0], task_ft)\n",
    "            dep_dr_inner_test_ls.append(labeled_test_df)\n",
    "            \n",
    "\n",
    "    elif transformation == 'RAW':\n",
    "        for i, smote_inner_train_df in enumerate(smote_inner_train_ls):\n",
    "            # Add engineered features if relevant for the train dataset\n",
    "            labeled_train_df = make_dependent(smote_inner_train_df, smote_inner_train_ls[0], task_ft)\n",
    "            dep_dr_inner_train_ls.append(labeled_train_df)\n",
    "\n",
    "            # Add engineered features if relevant for the test dataset\n",
    "            labeled_test_df = make_dependent(smote_inner_test_ls[i], smote_inner_test_ls[0], task_ft)\n",
    "            dep_dr_inner_test_ls.append(labeled_test_df)\n",
    "\n",
    "    else :\n",
    "        print('The chosen method is not an appropriate transformation technique, choose from RAW or UMAP')\n",
    "\n",
    "    return dep_dr_inner_train_ls, dep_dr_inner_test_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7e794-74b6-47e3-b87f-bd04dbe3eca3",
   "metadata": {},
   "source": [
    "## Main function for nCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c314a9-5000-4a98-b200-de9d9c32cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_pipeline(dMRI, sMRI_vol, sMRI_ls, num_tps, test_tp, num_outer_folds, num_inner_fold, transformation, task_ft):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    dMRI:(dataframe) dataset with dMRI data\n",
    "    sMRI_vol: (dataframe) dataset with demographic, clinical and structural MRI dataset.\n",
    "    sMRI_ls: (nested list of dfs) Each session has its own sublist containing the left and right hemisphere sMRI thickness \n",
    "    data within the main list.\n",
    "    num_tps: (integer) number of the highest session to include (0 = Y00, 1 = Y05, 3 = Y10)\n",
    "    test_tp: (integer) the number of the session to use as the test set (0 = Y00, 1 = Y05, 3 = Y10, 4 = Combined sessions)\n",
    "    num_outer_folds: (integer) the number of outer folds to split the data into\n",
    "    num_inner_fold: (integer) the number of inner folds to split each outer fold into\n",
    "    transformation: (string) The mthod of transformation to apply to the dataset (None/UMAP)\n",
    "    task_ft: (string) target variable name\n",
    "    OUTPUTs: dictionary of the scores and hps obtained during the outer runs, dictionary of the scores and \n",
    "    hps obtained during the inner runs and the best model.\n",
    "    DESCRIPTION: Groups all of the functions needed to run the nCV pipeline for LSTM.\n",
    "    \"\"\"\n",
    "    # Make the base data frame from the individual input data frames\n",
    "    tps = 2 if num_tps == 2 else 3\n",
    "    base_df = make_base_df(dMRI, sMRI_vol, sMRI_ls, tps, task_ft)\n",
    "\n",
    "    unique_subs, sub_split_ft  = get_unique_subs(base_df, task_ft)\n",
    "    print(f'~~ The unique subjects have been obtained. The subject splitting feature is {sub_split_ft}')\n",
    "    \n",
    "    # Initialize the nested cross-validation with stratification\n",
    "    outer_cv = StratifiedKFold(n_splits=num_outer_folds, shuffle=True, random_state=42)\n",
    "    inner_cv = StratifiedKFold(n_splits=num_inner_fold, shuffle=True, random_state=42)\n",
    "\n",
    "    outer_runs_dict, inner_runs_dict = {}, {}\n",
    "    outer_eval = float('inf') if task_ft in ['BL_Avg_cognition'] else float('-inf')\n",
    "    best_model = None\n",
    "\n",
    "    for outer_run, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(unique_subs['PRESGENE_ID'], unique_subs[sub_split_ft])):\n",
    "        print(f'~~ The current OUTER run is: {outer_run + 1} / {num_outer_folds} for task {task_ft}')\n",
    "        # Get train and test subject IDs (outer loop)\n",
    "        outer_train_subs = unique_subs.iloc[train_outer_idx]\n",
    "        outer_test_subs = unique_subs.iloc[test_outer_idx]\n",
    "\n",
    "        print('~~ The OUTER Subject split has been obtained.')\n",
    "        \n",
    "        inner_eval = float('inf') if task_ft in ['BL_Avg_cognition'] else float('-inf')\n",
    "        inner_hps = None\n",
    "  \n",
    "        for inner_run, (train_inner_idx, test_inner_idx) in enumerate(inner_cv.split(outer_train_subs['PRESGENE_ID'], outer_train_subs[sub_split_ft])):\n",
    "            \n",
    "            print(f'~~ The current INNER run is: {inner_run + 1} / {num_inner_fold} for outer run: {outer_run + 1} / {num_outer_folds} for task {task_ft}')\n",
    "            # Get train and test subject IDs (inner loop)\n",
    "            inner_train_subs = outer_train_subs.iloc[train_inner_idx]\n",
    "            inner_test_subs = outer_train_subs.iloc[test_inner_idx]\n",
    "            \n",
    "            # Get all occurrences of the selected subjects\n",
    "            inner_train_mask = base_df['PRESGENE_ID'].isin(inner_train_subs['PRESGENE_ID'])\n",
    "            inner_test_mask = base_df['PRESGENE_ID'].isin(inner_test_subs['PRESGENE_ID'])\n",
    "    \n",
    "            # Get the training and test data\n",
    "            train_inner, test_inner = base_df[inner_train_mask], base_df[inner_test_mask]\n",
    "        \n",
    "            print('~~ The INNER Subject split has been obtained.')\n",
    "            \n",
    "            ### Data Preprocessing\n",
    "            # Impute missing values\n",
    "            imp_inner_train_ls = impute_df(train_inner)\n",
    "            imp_inner_test_ls = impute_df(test_inner)\n",
    "            print('~~ The INNER imputation has been done.')                \n",
    "    \n",
    "            # Balance out minority classes with SMOTE for categorical variables only\n",
    "            if task_ft == 'BL_Avg_cognition':\n",
    "                smote_inner_train_ls = merge_time_imputed(imp_inner_train_ls)\n",
    "                smote_inner_test_ls = merge_time_imputed(imp_inner_test_ls)\n",
    "    \n",
    "            else:\n",
    "                smote_inner_train_ls = class_rebalancing_SMOTE(imp_inner_train_ls)\n",
    "                smote_inner_test_ls = class_rebalancing_SMOTE(imp_inner_test_ls)\n",
    "            print('~~ The INNER SMOTE Class rebalancing is done.')\n",
    "    \n",
    "            \n",
    "            # Apply dimensionality reduction if wanted and add dependent variables\n",
    "            dep_dr_inner_train_ls, dep_dr_inner_test_ls = apply_transform_labeled(smote_inner_train_ls, smote_inner_test_ls, \n",
    "                                                                                  transformation, task_ft)\n",
    "            print(f'the NAs after dim reduction {dep_dr_inner_train_ls[0].isna().any().sum()}, test {dep_dr_inner_test_ls[0].isna().any().sum()}')\n",
    "    \n",
    "            print('~~ INNER dimensionality reduction & dependent features have been applied/added.')\n",
    "    \n",
    "            working_train_inner, working_test_inner = None, None\n",
    "    \n",
    "            #### LSTM & Evaluation\n",
    "            working_train_inner = dep_dr_inner_train_ls[0] if num_tps == 3 or num_tps == 2 else dep_dr_inner_train_ls[1]\n",
    "            working_test_inner = dep_dr_inner_test_ls[0] if num_tps == 3 or num_tps == 2 else dep_dr_inner_test_ls[test_tp]\n",
    "            new_score, new_hps = tune_lstm(working_train_inner, working_test_inner, task_ft, transformation, num_tps, outer_run, inner_run)\n",
    "            inner_eval, inner_hps = evaluate_score(inner_eval, new_score, inner_hps, new_hps)\n",
    "            \n",
    "            inner_runs_dict[f'{outer_run + 1}{inner_run + 1}'] = {'eval_scores': inner_eval, 'best_hps': inner_hps}\n",
    "                    \n",
    "            print(f'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ INNER RUN {inner_run + 1} of outer run {outer_run + 1} is completed , best inner eval score: {inner_eval} , with best hps: {inner_hps} for task {task_ft} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            ### Outer loop: Use the best inner hyperparameters to train and evaluate the model\n",
    "            \n",
    "        # Get the outer loop datasets\n",
    "        outer_train_mask = base_df['PRESGENE_ID'].isin(outer_train_subs['PRESGENE_ID'])\n",
    "        outer_test_mask = base_df['PRESGENE_ID'].isin(outer_test_subs['PRESGENE_ID'])\n",
    "        train_outer, test_outer = base_df[outer_train_mask], base_df[outer_test_mask]\n",
    "        \n",
    "        print('~~ The OUTER dataset split has been obtained.')\n",
    "         \n",
    "        ### Data Preprocessing\n",
    "        # Impute missing values\n",
    "        imp_outer_train_ls = impute_df(train_outer)\n",
    "        imp_outer_test_ls = impute_df(test_outer)\n",
    "        print('~~ The OUTER imputation has been done.')\n",
    "        \n",
    "        # Balance out minority classes with SMOTE\n",
    "        if task_ft == 'BL_Avg_cognition':\n",
    "            smote_outer_train_ls = merge_time_imputed(imp_outer_train_ls)\n",
    "            smote_outer_test_ls = merge_time_imputed(imp_outer_test_ls)\n",
    "        else:\n",
    "            smote_outer_train_ls = class_rebalancing_SMOTE(imp_outer_train_ls)\n",
    "            smote_outer_test_ls = class_rebalancing_SMOTE(imp_outer_test_ls)\n",
    "        print('~~ The OUTER SMOTE Class rebalancing is done.')\n",
    "        \n",
    "        # Apply dimensionality reduction if wanted and add dependent variables\n",
    "        dep_dr_outer_train_ls, dep_dr_outer_test_ls = apply_transform_labeled(smote_outer_train_ls, smote_outer_test_ls,\n",
    "                                                                              transformation, task_ft)\n",
    "        \n",
    "        print('~~ OUTER Missing values have been imputed, Class rebalancing has been performed and dimensionality reduction & dependent features have been applied/added.')\n",
    "        \n",
    "        #### LSTM & Evaluation\n",
    "        working_train_outer, working_test_outer, outer_eval_scores_dict = None, None, None\n",
    "        \n",
    "        #### LSTM & Evaluation\n",
    "        working_train_outer = dep_dr_outer_train_ls[0] if num_tps == 3 or num_tps == 2 else dep_dr_outer_train_ls[1]\n",
    "        working_test_outer = dep_dr_outer_test_ls[0] if num_tps == 3 or num_tps == 2 else dep_dr_outer_test_ls[test_tp]\n",
    "        outer_eval_scores_dict, LSTM_fitted, auc_plot = run_lstm(working_train_outer, working_test_outer, task_ft, inner_hps, transformation, num_tps, outer_run)\n",
    "        outer_eval, best_model = evaluate_score(outer_eval, outer_eval_scores_dict, best_model, LSTM_fitted)\n",
    "\n",
    "        # Store the evaluation scores and hyperparameter\n",
    "        outer_runs_dict[outer_run + 1] = {'eval_scores': outer_eval_scores_dict, 'best_hps': inner_hps}\n",
    "        \n",
    "        # Save the AUCROC plot (only for categorical variables) & close it once saved\n",
    "        if isinstance(auc_plot, plt.Figure):\n",
    "            auc_plot.savefig(f'updated_data/nested_CV_final/LSTM/{transformation}/AUCROC_nestCV_run{outer_run + 1}_{transformation}_{task_ft}{num_tps}.png')\n",
    "            plt.close(auc_plot)\n",
    "            \n",
    "        print(f'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ OUTER RUN {outer_run + 1} is completed, best outer eval score: {outer_eval} & best model {best_model} for task {task_ft} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    print('Exited all nCV loops')\n",
    "    return outer_runs_dict, inner_runs_dict, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becab652-c2bc-4b7d-9c65-f12c8d21e857",
   "metadata": {},
   "source": [
    "# Step 5: Run the nCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f5fca-7662-4717-b478-40973ebfb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the nCV run\n",
    "transformation = '' # Options: RAW/UMAP\n",
    "dep_vars_ls = ['EDSS', 'MS', 'MStype', 'BL_Avg_cognition', 'HC_CI_CP','bi_EDSS', 'tri_EDSS', 'worsening_EDSS', 'RR_to_SP']\n",
    "n_ses = 3 # Number of sessions to include in the training data (1, 2, or 3)\n",
    "n_test = 3 # Session to use as test set (1, 2, or 3)\n",
    "n_outer = 3 # Number of outer nCV runs (> 1)\n",
    "n_inner = 3 #Number of inner nCV runs (> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12a8de-d6fd-4d7f-a078-62e05fa6f585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the nCV with the given parameters\n",
    "filepath = f'updated_data/nested_CV_final/LSTM/{transformation}'\n",
    "master_path = f'{filepath}/All3_10_master_dict_LSTM_final.pkl'\n",
    "\n",
    "for task in dep_vars_ls:\n",
    "    # Run nested CV for the task\n",
    "    outer_runs_dict, inner_runs_dict, best_model = CV_pipeline(None, volume_df, sMRI_ls, n_ses,\n",
    "                                                               n_test, n_outer, n_inner, transformation, task, 'LSTM')\n",
    "    \n",
    "    # Save the outer runs dictionary\n",
    "    with open(f'{filepath}/All3_10_outer_dict_{task}_LSTM_final.pkl', 'wb') as file:\n",
    "        pickle.dump(outer_runs_dict, file)\n",
    "    # Save the inner runs dictionary\n",
    "    with open(f'{filepath}/All3_10_inner_dict_{task}_LSTM_final.pkl', 'wb') as file:\n",
    "        pickle.dump(inner_runs_dict, file)\n",
    "    # Save the best model\n",
    "    with open(f'{filepath}/All3_10_best_outer_model_{task}_LSTM_final.pkl', 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(master_path):\n",
    "        # Load the existing dictionary\n",
    "        with open(master_path, 'rb') as file:\n",
    "            master_dict = pickle.load(file)\n",
    "\n",
    "    else: # Otherwise create the dictionary\n",
    "        master_dict = {}\n",
    "\n",
    "    # Add entries\n",
    "    master_dict[task] = [outer_runs_dict, inner_runs_dict, best_model]\n",
    "\n",
    "    # Save the updated dictionary\n",
    "    with open(master_path, 'wb') as file:\n",
    "            pickle.dump(master_dict, file)\n",
    "\n",
    "    print(f'Run for task {task} is completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
