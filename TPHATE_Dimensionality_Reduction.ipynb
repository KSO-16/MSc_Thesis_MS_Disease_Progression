{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a80b4c-b2e2-4405-bf45-10d82a59d3e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains all of the scripts used to tune and apply temporal potential of heat-diffusion for affinity-based trajectory embedding (TPHATE) to the datasets cleaned and merged in the data_preprocessing_DR.ipynb notebook. The following 4 steps are taken in this notebook:\n",
    "\n",
    "Step 1: Import the necessary libraries and datasets for the dimensionality reduction. The datasets involve a combination of MS/ALL, imputation methods, and unique/combined sessions. There is a total of 18 training datasets. Prior to loading all of the datasets, the 'tphate' library will need to be installed with pip, conda, or any other library installer.\n",
    "\n",
    "Step 2: Tune the hyperparameters (hps) of the TPHATE algorithm (numbe of principal components (PCs) and diffusion time step (t)) using K_fold in the make_K_folds function and the TPHATE_gridsearch function. The dataset is split into 3 folds. For unique sessions datasets the subjects are split into train and test subjects which are then used to obtain the train and test dataset. For the combined sessions, the train subjects at Y00 and Y05 are used for training and the test subjects at Y05 are used for testing. The current train fold is used to fit the TPHATE and Kmeans algorithm, the fitted TPHATE is then used to get the test TPHATE embeddings which are then used for making predictions with Kmeans. Adjusted rand index (ARI) is then used to evaluate the predictions against the true test labels. This sequence is repeated for every fold combination, after which the average ARI (AARI) is obtained for the given hps combination. These steps are then repeated for the other hyperparameter (hp) values. A dataframe of the optimal hp value per dataset is then created with the make_gridsearch_table function.\n",
    "\n",
    "Step 3: The best hp value per dataset found during step 2 is used to make the training TPHATE embeddings using the apply_TPHATE function. A 2 dimensional plot (and therefore 2 PCs) of the embedded data is produced in the function group_plot_TPHATE. The apply_TPHATE function provides the embedded arrays for the unique sessions and combined session dataset. \n",
    "\n",
    "Step 4: The embedded TPHATE arrays are saved and reserved for later use.\n",
    "\n",
    "These datasets were used to assess the performance of TPHATE in comparison with PCA, tSNE, and UMAP. The statistical outcomes of part 1 of the project can be found in the 'Dimensionality Reduction' subsection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806e9ea",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tphate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733f70a-2457-4ba5-8098-ea1b8d140ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the combined sessions datasets\n",
    "MS_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia.xlsx')\n",
    "ALL_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia.xlsx')\n",
    "MS_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it.xlsx')\n",
    "ALL_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it.xlsx')\n",
    "MS_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in.xlsx')\n",
    "ALL_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time imputation method)\n",
    "MS_t1_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia00.xlsx')\n",
    "MS_t2_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia05.xlsx')\n",
    "ALL_t1_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia00.xlsx')\n",
    "ALL_t2_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Type imputation method)\n",
    "MS_t1_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it00.xlsx')\n",
    "MS_t2_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it05.xlsx')\n",
    "ALL_t1_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it00.xlsx')\n",
    "ALL_t2_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Neighbor imputation method)\n",
    "MS_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in00.xlsx')\n",
    "MS_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in05.xlsx')\n",
    "ALL_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in00.xlsx')\n",
    "ALL_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in05.xlsx')\n",
    "\n",
    "# Group the datasets by imputation methods and then by unique session \n",
    "time_set_all = [MS_t1_imp_all, MS_t2_imp_all, ALL_t1_imp_all, ALL_t2_imp_all]\n",
    "time_set_type = [MS_t1_imp_type, MS_t2_imp_type, ALL_t1_imp_type, ALL_t2_imp_type]\n",
    "time_set_nb = [MS_t1_imp_nb,  MS_t2_imp_nb, ALL_t1_imp_nb, ALL_t2_imp_nb]\n",
    "time_set_ls = [time_set_all, time_set_type, time_set_nb]\n",
    "\n",
    "# Group the datasets by imputation methods and then by combined sessions\n",
    "complete_set_ls = [[MS_imp_all, ALL_imp_all], [MS_imp_type, ALL_imp_type], [MS_imp_nb, ALL_imp_nb]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da4402",
   "metadata": {},
   "source": [
    "# Step 2: Hyperparameter tuning for TPHATE (number of PCs and t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e45c5-f347-4c38-8b1e-bcd80c0b77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_folds(df):\n",
    "    \"\"\"\n",
    "    INPUT: Dataframe\n",
    "    OUPUT: Nested list of dataframes containing the train and test datasets\n",
    "    DESCRIPTION: Use k folds to split the input data into 3 splits, and return the x_train, x_test, y_train, and y_test for each of the\n",
    "    folds as unique lists.\n",
    "    \"\"\"\n",
    "    # Get the subject IDs & number of unique time points\n",
    "    subjects = df['index'].unique()\n",
    "    num_ses = len(df['Time'].unique())\n",
    "    \n",
    "    # Get the true labels\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "    labels = df[label_col[0]]\n",
    "    \n",
    "    # Normalize the dataframe\n",
    "    norm_df = df.drop(columns=['EDSS', 'BL_Avg_cognition', 'Time', 'index', 'HC_CI_CP'] + label_col, axis=1)\n",
    "    norm_df = StandardScaler().fit_transform(norm_df)\n",
    "    \n",
    "    # Initialize KFold for subjects\n",
    "    kfold = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Initialize output list\n",
    "    x_train_list, x_test_list, y_test_list = [], [], []\n",
    "    \n",
    "    if num_ses == 1:  # For unique session datasets\n",
    "        for train_indices, test_indices in kfold.split(norm_df):\n",
    "            # Get the training and test data for fold\n",
    "            x_train, x_test = norm_df[train_indices], norm_df[test_indices]\n",
    "            y_test = labels[test_indices]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "        \n",
    "    else: # For combined sessions datasets\n",
    "        for train_idx, test_idx in kfold.split(df[df['Time'] == 1]):\n",
    "            # Get the subject IDs for the training and test sets\n",
    "            fold_subjects = subjects[test_idx]\n",
    "\n",
    "            # Get the indices for test set\n",
    "            test_idx = df[(df['index'].isin(fold_subjects)) & (df['Time'] == 2)].index\n",
    "\n",
    "            # Get the training and test data\n",
    "            x_train, x_test = norm_df[train_idx], norm_df[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "    \n",
    "    return [x_train_list, x_test_list, y_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7743e6-3cf4-4938-be97-12395bb5606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPHATE_gridsearch(ls_ls_df, n_runs, param_grid):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_df : (Nested list of dataframes)\n",
    "    n_runs : (integer) number of runs per kfold\n",
    "    OUTPUT: List of floats (AARI), list of dictionaries (hps) list of arrays (with ARIs per hps).\n",
    "    DESCRIPTION: Find the optimal PC numbers & diffusion time steps (t) to include for each of the df based on \n",
    "    their obtained average adjusted rand index\n",
    "    \"\"\"\n",
    "    # Initialise output lists for score and parameters\n",
    "    output_scores, output_params, output_rand = [], [], []\n",
    "\n",
    "    for ls_df in ls_ls_df:\n",
    "        # Initialise sub-output lists for score and parameters\n",
    "        sub_scores, sub_params, sub_rand = [], [], []\n",
    "        \n",
    "        for df_num, df in enumerate(ls_df):\n",
    "            best_score = -np.inf\n",
    "            best_params = None\n",
    "            output_rand_ls = []\n",
    "        \n",
    "            # Get the number of clusters needed for the df\n",
    "            n_clusters = 2 if 'MS' in df.columns else 4\n",
    "    \n",
    "            # Get k-fold splits of the data\n",
    "            k_fold_ls = make_K_folds(df)\n",
    "    \n",
    "            # Initializes the rand indices array (per df) \n",
    "            rand_indices = np.zeros((len(param_grid['n_components']), len(param_grid['t'])))\n",
    "            \n",
    "            # Perform grid search\n",
    "            for i, n_components in enumerate(param_grid['n_components']): # Iterate over number of PCs\n",
    "                for j, t in enumerate(param_grid['t']): # Iterate over diffusion time steps\n",
    "                    temp_ARIs = []\n",
    "                    for k in range(0, len(k_fold_ls[0])): # Iterate over the K folds\n",
    "                        x_train = k_fold_ls[0][k]\n",
    "                        x_test = k_fold_ls[1][k]\n",
    "                        y_test = k_fold_ls[2][k]\n",
    "                        \n",
    "                        for run in range(n_runs): # Repeat for the number of runs per fold\n",
    "                            tphate_model = tphate.TPHATE(verbose = 0, n_jobs = -1, n_landmark = x_train.shape[0],\n",
    "                                                          n_components = n_components, t = t)\n",
    "        \n",
    "                            train_transformed = tphate_model.fit_transform(x_train)\n",
    "                            test_transformed = tphate_model.transform(x_test)\n",
    "        \n",
    "                            # Evaluate the performance of the hps at the given run of Kfold with Kmeans & ARI\n",
    "                            kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
    "                            kmeans.fit(train_transformed)\n",
    "                            y_pred = kmeans.predict(test_transformed)\n",
    "                            temp_ARIs.append(adjusted_rand_score(y_test, y_pred))\n",
    "        \n",
    "                    # Get average ARI score\n",
    "                    AARI = mean(temp_ARIs)\n",
    "                    rand_indices[i,j] = AARI\n",
    "        \n",
    "                    if AARI > best_score:\n",
    "                        best_score = AARI\n",
    "                        best_params = {'n_components' : n_components, 't': t}\n",
    "\n",
    "            sub_scores.append(best_score)\n",
    "            sub_params.append(best_params)\n",
    "            sub_rand.append(rand_indices)\n",
    "        \n",
    "        # Update the ouput lists        \n",
    "        output_scores.append(sub_scores)\n",
    "        output_params.append(sub_params)\n",
    "        output_rand.append(sub_rand)\n",
    "        \n",
    "    return output_scores, output_params, output_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f23da9-e982-4dec-8007-421c2b10faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_row_names():\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    OUTPUT: List of strings\n",
    "    DESCRIPTION: Creates the row names corresponding to the different datasets included in the make_gridsearch_tbl \n",
    "    function\n",
    "    \"\"\"\n",
    "    ls_row_names = []\n",
    "    ls_types = ['imp_type']\n",
    "    ls_subjects = ['MS_', 'ALL_']\n",
    "    \n",
    "    for str3 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            for str2 in ['t1_', 't2_', 't3_']:\n",
    "                name = str1 + str2 + str3\n",
    "                ls_row_names.append(name)\n",
    "    for str2 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            name = str1 + str2\n",
    "            ls_row_names.append(name)   \n",
    "    \n",
    "    return ls_row_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b58c7-7291-4fc5-bb6b-bec6a93a17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gridsearch_tbl(time_best_param, time_best_score, comp_best_param, comp_best_score):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (nested lists of arrays) arrays AARI float for time split datasets\n",
    "    comp_RI : (nested lists of arrays) arrays AARI float for time combined datasets\n",
    "    OUTPUT: Dataframe\n",
    "    DESCRIPTION: Create a table (df) with dataset names in column 1, best number of PCs values in column 2, best \n",
    "    diffusion time steps in column 3, and gridsearch AARI scores in column 4. \n",
    "    \"\"\"\n",
    "    # Make row names\n",
    "    dataset_names = make_row_names()\n",
    "    \n",
    "    # Initialize lists to store maximum values, row indices, and column indices\n",
    "    max_ari, max_components, max_t = [], [], []\n",
    "\n",
    "    # Iterate over time list (time_RI)\n",
    "    for i, sublist in enumerate(time_best_param):\n",
    "        for j, dict in enumerate(sublist):\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_ari.append(time_best_score[i][j])\n",
    "            max_components.append(time_best_param[i][j]['n_components'])\n",
    "            max_t.append(time_best_param[i][j]['t'])\n",
    "    \n",
    "    # Iterate over combined time list (comp_RI)\n",
    "    for i, sublist in enumerate(comp_best_params):\n",
    "        for j, dict in enumerate(sublist):\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_ari.append(comp_best_score[i][j])\n",
    "            max_components.append(comp_best_param[i][j]['n_components'])\n",
    "            max_t.append(comp_best_param[i][j]['t'])\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'Dataset': dataset_names,\n",
    "        'Best_n_components': max_components,\n",
    "        'Best_diffusion_time_step': max_t,\n",
    "        'Best_ARI_Score': max_ari})\n",
    "     \n",
    "    output_df.to_excel('updated_data/TPHATE/best_gridsearch_per_dataset_tbl.xlsx', index = False)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bcace-3c16-4ba5-9de8-6319493192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_TPHATE_gridsearch(time_gridsearch, comp_gridsearch):\n",
    "    \"\"\"\n",
    "    INPUT: 2 lists of nested lists of 2D numpy arrays,\n",
    "    OUTPUT: 3 figures (with 3x2 subplots)\n",
    "    DESCRIPTION: Plot the AARI scores for each TPHATE gridsearch run. Figure 1-3 correspond to different \n",
    "    imputation types, and plots 1 and 2 represent the MS only vs.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the perplexity, learning rate ranges, and the number of runs\n",
    "    n_components = list(range(1,16))\n",
    "    n_t = [1, 3, 5, 10, 20, 50]\n",
    "    imp_types = ['All Imputation', 'Type Imputation', 'Neighbor Imputation']\n",
    "    time_point = ['1', '1', '2', '2', 'Combined']\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    df_ind_range = [0, 2, 1, 3]\n",
    "    MS_col_ls = ['#EBDEF0','#D2B4DE', '#AF7AC5', '#8E44AD','#76448A', '#4A235A']\n",
    "    ALL_col_ls = ['#D4EFDF', '#A9DFBF', '#7DCEA0', '#27AE60', '#1E8449', '#145A32']  \n",
    "\n",
    "\n",
    "    # Make 3 main figures (imputation types)\n",
    "    for fig_num in range(1, 4):\n",
    "        plt.figure(figsize=(18, 24))\n",
    "\n",
    "        # Make 6 main plots (MS & ALL)\n",
    "        for plot_num in range(1, 7):\n",
    "            plt.subplot(3, 2, plot_num)\n",
    "\n",
    "            if plot_num % 2 != 0 and plot_num != 5: #Odd & not 5\n",
    "                df_ind = df_ind_range[plot_num - 1]\n",
    "                plot_title = ('MS Patients Only', time_point[plot_num - 1])\n",
    "                for i, t in enumerate(n_t): \n",
    "                    plt.plot(n_components, time_gridsearch[fig_num - 1][df_ind][:,i], label = f't = {t}', color = MS_col_ls[i])\n",
    "\n",
    "            elif plot_num % 2 == 0 and plot_num != 6: #Even & not 6\n",
    "                df_ind = df_ind_range[plot_num - 1]\n",
    "                plot_title = ('All Patients', time_point[plot_num - 1])\n",
    "                for i, t in enumerate(n_t):\n",
    "                    plt.plot(n_components, time_gridsearch[fig_num - 1][df_ind][:,i], label = f't = {t}', color = ALL_col_ls[i])\n",
    "                \n",
    "            elif plot_num == 5:\n",
    "                plot_title = ('MS Patients Only', time_point[4])\n",
    "                for i, t in enumerate(n_t):\n",
    "                    plt.plot(n_components, comp_gridsearch[fig_num - 1][plot_num - 5][:,i], label = f't = {t}', color = MS_col_ls[i])\n",
    "                    \n",
    "            elif plot_num == 6:\n",
    "                plot_title = ('All Patients', time_point[4])\n",
    "                for i, t in enumerate(n_t):\n",
    "                    plt.plot(n_components, comp_gridsearch[fig_num - 1][plot_num - 5][:,i], label = f't = {t}', color = ALL_col_ls[i])\n",
    "                \n",
    "            plt.xlabel('Number of PCs for affinity matrix')\n",
    "            plt.ylabel('Average Adjusted Rand Index')\n",
    "            plt.title(f'{imp_types[fig_num - 1]} for {plot_title[0]} Dataset at Time Point {plot_title[1]}')\n",
    "            plt.legend(title = 'Diffusion time step (t)')\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/TPHATE/{file_name[fig_num - 1]}/Gridsearch_plots_figure_{file_name[fig_num - 1]}_{plot_title[1]}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6b8bc-a867-4731-a781-0f5e37508e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_components': list(range(1,16)),\n",
    "    't': [1, 3, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "# Run the TPHATE search\n",
    "time_best_scores, time_best_params, time_gridsearch = TPHATE_gridsearch(time_set_ls, 2, param_dict):\n",
    "comp_best_scores, comp_best_params, comp_gridsearch = TPHATE_gridsearch(complete_set_ls, 2, param_dict)\n",
    "\n",
    "# Make the table with best hps and corresponding ARI score\n",
    "best_gridsearch_df = make_gridsearch_tbl(time_best_params, time_best_scores, comp_best_params, comp_best_scores)\n",
    "\n",
    "# Plot the gridsearch runs\n",
    "plot_TPHATE_gridsearch(time_gridsearch, comp_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea9985-e72f-4556-b42d-5181e7f46758",
   "metadata": {},
   "source": [
    "# Step 3: Apply TPHATE with Gridsearch Results (+ plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa1a45-6002-40b5-9ef6-a38b22191950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_plot_TPHATE(time_arrays, time_ls, comp_arrays, comp_ls, best_GS_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_arrays : (nested lists of arrays) nested lists with arrays of the TPHATE embeddings for the time seperated datasets\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_arrays : (nested lists of arrays) nested lists with arrays of the TPHATE embeddings for the time combined datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    OUTPUT: 3 figures of 2 by 3 subplots\n",
    "    DESCRIPTION: creates 2 dimensional plots of the TPHATE embedded dataframes\n",
    "    \"\"\"\n",
    "    # Make list of file names for saving, and list to order the plots within the figure\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    ordered_ls = [0,2,1,3,0,1]\n",
    "    GS_ints = [0,2,1,3,12,13]\n",
    "    \n",
    "    # Make n main figures (imputation types)\n",
    "    for fig_num in range(1, len(time_ls) + 1):\n",
    "        plt.figure(figsize=(18, 24))\n",
    "\n",
    "        # Make m main plots\n",
    "        num_plots = len(time_arrays[0]) + len(comp_arrays[0])\n",
    "        for plot_num, df_ind in enumerate(ordered_ls):\n",
    "            plt.subplot(int(num_plots/2), 2, plot_num + 1)\n",
    "            \n",
    "            # Assign a df and array to plot\n",
    "            label_df = time_ls[fig_num - 1][df_ind] if plot_num + 1 <= len(time_arrays[0]) else comp_ls[fig_num - 1][df_ind]\n",
    "            plotting_array = time_arrays[fig_num - 1][df_ind] if plot_num + 1 <= len(time_arrays[0]) else comp_arrays[fig_num - 1][df_ind] \n",
    "                \n",
    "            # Define the plot colours & label colum\n",
    "            label_col = [col for col in label_df.columns if col.startswith('MS')]\n",
    "            color_map = {0: 'pink', 1: 'orange', 2: 'purple'} if label_col[0] == 'MStype' else {0: 'green', 1: 'purple'}\n",
    "            legend_labels = {0: 'PPMS', 1: 'SPMS', 2: 'RRMS'} if label_col[0] == 'MStype' else {0: 'HC', 1: 'MS'}\n",
    "            mapped_colors = label_df['MStype'].map(color_map) if label_col[0] == 'MStype' else label_df['MS'].map(color_map)  \n",
    "            \n",
    "            # Make the plots\n",
    "            for category, color in color_map.items():\n",
    "                indices = label_df[label_col[0]] == category\n",
    "                plt.scatter(plotting_array[indices, 0], plotting_array[indices, 1], c = color, label = legend_labels[category], alpha=0.7)\n",
    "            \n",
    "            # Make plot labels\n",
    "            df_name = best_GS_df.iloc[GS_ints[plot_num], 0]\n",
    "            n_components = 2 if best_GS_df.iloc[GS_ints[plot_num], 1] < 2 else best_GS_df.iloc[GS_ints[plot_num], 1]\n",
    "            n_t = best_GS_df.iloc[GS_ints[plot_num], 2]\n",
    "            plt.xlabel('TPHATE Component 1')\n",
    "            plt.ylabel('TPHATE Component 2')\n",
    "            plt.title(f'2D TPHATE for {df_name} Dataset (Components={n_components}, diffusion time step={n_t})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)  \n",
    "        \n",
    "        GS_ints = [x + 4 if i < 4 else x + 2 for i, x in enumerate(GS_ints)]\n",
    "\n",
    "        # Make figures and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/TPHATE/{file_name[fig_num - 1]}/best_param_TPHATE_plots_{file_name[fig_num - 1]}_multicolor.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de84cc-c457-47b8-954a-0010c2883c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_TPHATE(time_ls, comp_ls, best_GS_df, plot_param):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    plot_param : (Boolean) True/False make a 2-dimensional plot of the TPHATE embeddings\n",
    "    OUTPUT: 2 lists of nested dfs, 6 lists of nested floats\n",
    "    DESCRIPTION: Applies TPHATE fitting to each of the dataframes in the given list of nested dataframes, based on\n",
    "    its optimal perplexity and learning rate values. Plots the ouput arrays of the TPHATE fittings if plot_param is\n",
    "    True.\n",
    "    \"\"\"\n",
    "    # Initialise output lists\n",
    "    output_time_ls, output_comp_ls = [], []\n",
    "    \n",
    "    # Needed to iterate through best_GS_df \n",
    "    counter = 0\n",
    "\n",
    "    for sublist in time_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, hps, label, and index column name for the current df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            n_components = 2 if best_GS_df.iloc[GS_ints[plot_num], 1] < 2 else best_GS_df.iloc[GS_ints[plot_num], 1]\n",
    "            n_t = best_GS_df.iloc[counter, 2]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "            idx_col = df.columns[0]\n",
    "    \n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', idx_col] + label_col, axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "    \n",
    "            # Run the TPHATE model\n",
    "            TPHATE_array = tphate.TPHATE(verbose = 0, n_jobs = -1, n_landmark = norm_df.shape[0], \n",
    "                              n_components = n_components, t = n_t).fit_transform(norm_df)\n",
    "            \n",
    "            type_list.append(TPHATE_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "    \n",
    "        output_time_ls.append(type_list)\n",
    "        \n",
    "    for sublist in comp_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            \n",
    "            # Get name, hps, label, and index column name for the current df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            n_components = 2 if best_GS_df.iloc[GS_ints[plot_num], 1] < 2 else best_GS_df.iloc[GS_ints[plot_num], 1]\n",
    "            n_t = best_GS_df.iloc[counter, 2]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "            idx_col = df.columns[0]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', idx_col] + label_col, axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "\n",
    "            # Run the components model\n",
    "            TPHATE_array = tphate.TPHATE(verbose = 0, n_jobs = -1, n_landmark = norm_df.shape[0], \n",
    "                                          n_components = n_components, t = n_t).fit_transform(norm_df)\n",
    "    \n",
    "            type_list.append(TPHATE_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        output_comp_ls.append(type_list)\n",
    "\n",
    "    # Plotting condition (if true, plots are generated)\n",
    "    if plot_param:\n",
    "        group_plot_TPHATE(output_time_ls, time_ls, output_comp_ls, comp_ls, best_GS_df)\n",
    "     \n",
    "    return output_time_ls, output_comp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c693855-eccc-4f2f-a56c-86796532cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run step 3 (apply_TPHATE)\n",
    "time_TPHATE_arrays, comp_TPHATE_arrays = apply_TPHATE(time_set_ls, complete_set_ls, best_gridsearch_df, True)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b810c4-6d4c-4564-894e-dc00f03d1df6",
   "metadata": {},
   "source": [
    "# Step 4: Save the TPHATE embedded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528f174-2388-4384-815f-c94273d95c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_TPHATE_embedings(ls_ls_tphate_array, ls_ls_matching_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_tphate_array : (nested list of np.array) nested list of array of the TPHATE embedding\n",
    "    ls_ls_matching_df : (nested list of pd.dataframe) nested list of original pre embedding dataframe\n",
    "    OUTPUT:\n",
    "    DESCRIPTION: Exports the tSNE embeddings as dataframes with the same index/ subjects ID as their original dataset. \n",
    "    \"\"\"\n",
    "    imp_file = ['all', 'type', 'neighbor']\n",
    "    imp_df = ['ia', 'it', 'in']\n",
    "\n",
    "    for imp_idx, imp_ls in enumerate(ls_ls_tphate_array):\n",
    "        for emb_idx, tphate_emb in enumerate(imp_ls):\n",
    "            # Make a df from the array\n",
    "            output_df = pd.DataFrame(tphate_emb, columns = [f'PC{i+1}' for i in range(tphate_emb.shape[1])])\n",
    "\n",
    "            # Reintroduce the patients ID (index)\n",
    "            output_df['index'] = ls_ls_matching_df[imp_idx][emb_idx]['index'].reset_index(drop = True)\n",
    "            columns = ['index'] + [col for col in output_df.columns if col != 'index']\n",
    "            \n",
    "            # Reorder the columns such that index is first\n",
    "            output_df = output_df[columns]\n",
    "\n",
    "            # Check for time split list or not\n",
    "            if len(ls_ls_matching_df[0]) > 3:\n",
    "                sub_type = 'MStrain_' if emb_idx < 2 else 'ALLtrain_'\n",
    "                year = '00' if emb_idx % 2 == 0 else '05'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/TPHATE/{imp_file[imp_idx]}/TPHATE_{sub_type}{imp_df[imp_idx]}{year}.xlsx', index=False)\n",
    "            \n",
    "            else:\n",
    "                sub_type = 'MStrain_' if emb_idx == 0 else 'ALLtrain_'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/TPHATE/{imp_file[imp_idx]}/TPHATE_{sub_type}{imp_df[imp_idx]}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888315e-962a-4ddf-86aa-7a269984ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export_TPHATE_embedings function for the unique sessions and the combined sessions.\n",
    "export_TPHATE_embedings(time_TPHATE_arrays, time_set_ls)\n",
    "export_TPHATE_embedings(comp_TPHATE_arrays, complete_set_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
