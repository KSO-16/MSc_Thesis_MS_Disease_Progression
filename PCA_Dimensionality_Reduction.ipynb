{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe552c3-9912-4bd1-9098-1559a9fab38e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains all of the scripts used to tune and apply principal component analysis (PCA) to the datasets cleaned and merged in the data_preprocessing_DR.ipynb notebook. The following 5 steps are taken in this notebook:\n",
    "\n",
    "Step 1: Import the necessary libraries and datasets for the dimensionality reduction. The datasets involve a combination of MS/ALL, imputation methods, and unique/combined sessions. There is a total of 18 training datasets.\n",
    "\n",
    "Step 2: Tune the hyperparameters (hps) of the PCA algorithm (number of principal components (PCs)) using K_fold in the make_K_folds function and the PCA_gridsearch function. The dataset is split into 3 folds. For unique sessions datasets the subjects are split into train and test subjects which are then used to obtain the train and test dataset. For the combined sessions, the train subjects at Y00 and Y05 are used for training and the test subjects at Y05 are used for testing. The current train fold is used to fit the PCA and Kmeans algorithm, the fitted PCA is then used to get the test pca embeddings which are then used for making predictions with Kmeans. Adjusted rand index (ARI) is then used to evaluate the predictions against the true test labels. This sequence is repeated for every fold combination, after which the average ARI (AARI) is obtained for the given number of principal components. These steps are then repeated for the other hyperparameter (hp) values. A dataframe of the optimal hp value per dataset is then created with the make_PC_ARI_table function.\n",
    "\n",
    "Step 3: The best hp value per dataset found during step 2 is used to make the training PCA embeddings using the apply_PCA function. A 2 dimensional (and therefore 2 PCs) plot of the embedding data is produced. The apply_PCA function provides the embedded arrays for the unique sessions and combined session dataset, the variance and total ratios for each dataset, and the feature loadings for each dataset. \n",
    "\n",
    "Step 4: The Variance explained and feature loadings obtained in step 3 are then added to the existing gridsearch dataframe, which is updated and saved.\n",
    "\n",
    "Step 5: The embedded PCA arrays are saved and reserved for later use.\n",
    "\n",
    "These datasets were used to assess the performance of PCA in comparison with tSNE, UMAP, and TPHATE. The statistical outcomes of part 1 of the project can be found in the 'Dimensionality Reduction' subsection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feceba",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230959b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the combined sessions datasets\n",
    "MS_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia.xlsx')\n",
    "ALL_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia.xlsx')\n",
    "MS_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it.xlsx')\n",
    "ALL_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it.xlsx')\n",
    "MS_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in.xlsx')\n",
    "ALL_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time imputation method)\n",
    "MS_t1_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia00.xlsx')\n",
    "MS_t2_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia05.xlsx')\n",
    "ALL_t1_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia00.xlsx')\n",
    "ALL_t2_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Type imputation method)\n",
    "MS_t1_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it00.xlsx')\n",
    "MS_t2_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it05.xlsx')\n",
    "ALL_t1_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it00.xlsx')\n",
    "ALL_t2_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Neighbor imputation method)\n",
    "MS_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in00.xlsx')\n",
    "MS_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in05.xlsx')\n",
    "ALL_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in00.xlsx')\n",
    "ALL_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in05.xlsx')\n",
    "\n",
    "# Group the datasets by imputation methods and then by unique session \n",
    "time_set_all = [MS_t1_imp_all, MS_t2_imp_all, ALL_t1_imp_all, ALL_t2_imp_all]\n",
    "time_set_type = [MS_t1_imp_type, MS_t2_imp_type, ALL_t1_imp_type, ALL_t2_imp_type]\n",
    "time_set_nb = [MS_t1_imp_nb,  MS_t2_imp_nb, ALL_t1_imp_nb, ALL_t2_imp_nb]\n",
    "time_set_ls = [time_set_all, time_set_type, time_set_nb]\n",
    "\n",
    "# Group the datasets by imputation methods and then by combined sessions\n",
    "complete_set_ls = [[MS_imp_all, ALL_imp_all], [MS_imp_type, ALL_imp_type], [MS_imp_nb, ALL_imp_nb]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e9ff",
   "metadata": {},
   "source": [
    "# Step 2: Hyperparameter tuning for PCA (n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4efd1-a7b1-4722-8b7b-e151568534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_folds(df):\n",
    "    \"\"\"\n",
    "    INPUT: Dataframe\n",
    "    OUPUT: Nested list of dataframes containing the train and test datasets\n",
    "    DESCRIPTION: Use k folds to split the input data into 3 splits, and return the x_train, x_test, y_train, and y_test for each of the\n",
    "    folds as unique lists.\n",
    "    \"\"\"\n",
    "    # Get the subject IDs & number of unique time points\n",
    "    subjects = df['index'].unique()\n",
    "    num_ses = len(df['Time'].unique())\n",
    "    \n",
    "    # Get the true labels\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "    labels = df[label_col[0]]\n",
    "    \n",
    "    # Normalize the dataframe\n",
    "    norm_df = df.drop(columns=['EDSS', 'BL_Avg_cognition', 'Time', 'index'] + label_col, axis=1)\n",
    "    norm_df = StandardScaler().fit_transform(norm_df)\n",
    "    \n",
    "    # Initialize KFold for subjects\n",
    "    kfold = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Initialize output list\n",
    "    x_train_list, x_test_list, y_test_list = [], [], []\n",
    "    \n",
    "    if num_ses == 1: # For unique session datasets\n",
    "\n",
    "        for train_indices, test_indices in kfold.split(norm_df):\n",
    "            # Get the training and test data for fold\n",
    "            x_train, x_test = norm_df[train_indices], norm_df[test_indices]\n",
    "            y_test = labels[test_indices]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "        \n",
    "    else: # For combined sessions datasets\n",
    "        for train_idx, test_idx in kfold.split(df[df['Time'] == 1]):\n",
    "            # Get the subject IDs for the training and test sets\n",
    "            fold_subjects = subjects[test_idx]\n",
    "\n",
    "            # Get the indices for test set\n",
    "            test_idx = df[(df['index'].isin(fold_subjects)) & (df['Time'] == 2)].index\n",
    "\n",
    "            # Get the training and test data\n",
    "            x_train, x_test = norm_df[train_idx], norm_df[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_test)\n",
    "    \n",
    "    return [x_train_list, x_test_list, y_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493676f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_gridsearch(lst, hp_dict):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    lst : (Nested lists of dataframes)\n",
    "    OUTPUT: Nested lists of integers (Average adjusted rand index scores)\n",
    "    DESCRIPTION: Find the optimal number of PCs to include for each of the df based on their obtained AARI\n",
    "    \"\"\"\n",
    "    # Define the range of principal components (PCs) to test\n",
    "    output_rand_lists = []\n",
    "    best_rands = []\n",
    "\n",
    "    # Iterate over the imputation methods lists\n",
    "    for sublst in lst:\n",
    "        ls_rand_indices = []\n",
    "        best_rands_sublist = []\n",
    "        for i, df in enumerate(sublst):\n",
    "            # Get labels and initialize rand list\n",
    "            n_clusters = 2 if 'MS' in df.columns else 4\n",
    "            \n",
    "            # Get k-fold splits of the data\n",
    "            k_fold_ls = make_K_folds(df)\n",
    "            \n",
    "            rand_indices = []\n",
    "                \n",
    "            # For each number of PCs\n",
    "            for n_comp in hp_dict['n_pc']:\n",
    "                temp_ARIs = []\n",
    "                for k in range(0, len(k_fold_ls[0])):\n",
    "                    x_train = k_fold_ls[0][k]\n",
    "                    x_test = k_fold_ls[1][k]\n",
    "                    y_test = k_fold_ls[2][k]\n",
    "\n",
    "                    # Initialise PCA and fit/transform the x data\n",
    "                    pca = PCA(n_components = n_comp)\n",
    "                    x_train_pca = pca.fit_transform(x_train)\n",
    "                    x_test_pca = pca.transform(x_test)\n",
    "\n",
    "                    # Fit & apply K-means clustering\n",
    "                    kmeans = KMeans(n_clusters = n_clusters, random_state = 42, n_init = 'auto')\n",
    "                    kmeans.fit(x_train_pca)\n",
    "                    y_pred = kmeans.predict(x_test_pca)\n",
    "                    temp_ARIs.append(adjusted_rand_score(y_test, y_pred))\n",
    "                                \n",
    "                # Get average ARI score\n",
    "                rand_indices.append(np.mean(temp_ARIs))\n",
    "            \n",
    "            # Add AARI per PC number (for df) to sublist (for imputation type)\n",
    "            ls_rand_indices.append(rand_indices)\n",
    "            \n",
    "            # Find best PC number and corresponding ARI\n",
    "            best_rand_value = max(rand_indices)\n",
    "            best_pc_number = rand_indices.index(best_rand_value) + 1 \n",
    "            best_rands_sublist.append((best_pc_number, best_rand_value))\n",
    "        \n",
    "        output_rand_lists.append(ls_rand_indices)\n",
    "        best_rands.append(best_rands_sublist)\n",
    "    \n",
    "    return output_rand_lists, best_rands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc096c-017a-49ad-9ce2-9b93c1fd0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_row_names():\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    OUTPUT: List of strings\n",
    "    DESCRIPTION: Creates the row names corresponding to the different datasets included in the make_PC_ARI_table \n",
    "    function\n",
    "    \"\"\"\n",
    "    ls_row_names = []\n",
    "    ls_types = ['imp_all', 'imp_type', 'imp_neighbor']\n",
    "    ls_subjects = ['MS_', 'ALL_']\n",
    "    \n",
    "    for str3 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            for str2 in ['t1_', 't2_']:\n",
    "                name = str1 + str2 + str3\n",
    "                ls_row_names.append(name)\n",
    "    for str2 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            name = str1 + str2\n",
    "            ls_row_names.append(name)   \n",
    "    \n",
    "    return ls_row_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571160eb-13f4-4d36-ba65-69789fbd1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_PC_ARI_table(ls_best_ARI_time, ls_best_ARI_comp):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_best_ARI_time : (nested lists of tuples) tuples with integer and float (number of PCs, AARI score) for \n",
    "    unique sessions datasets\n",
    "    ls_best_ARI_comp : (nested lists of tuples) tuples with integer and float (number of PCs, AARI score) for \n",
    "    combined sessions datasets\n",
    "    OUTPUT: Dataframe\n",
    "    DESCRIPTION: Create a dataframe with dataset names in column 1, best number of PCs in column 2, and gridsearch\n",
    "    AARI scores in column 3. \n",
    "    \"\"\"\n",
    "    # Initialize table\n",
    "    table_data = []\n",
    "    \n",
    "    # Flatten the list\n",
    "    flattened_best_indices = [item for sublist in ls_best_ARI_time + ls_best_ARI_comp for item in sublist]\n",
    "    \n",
    "    # Make row names\n",
    "    dataset_names = make_row_names()\n",
    "    \n",
    "    # Fill the table data\n",
    "    for dataset, (PC, ARI) in zip(dataset_names, flattened_best_indices):\n",
    "        table_data.append([dataset, PC, ARI])\n",
    "        \n",
    "    # Make DataFrame\n",
    "    PC_ARI_df = pd.DataFrame(table_data, columns=[\"Dataset\", \"Best_PC_Number\", \"Best_ARI_Score\"])\n",
    "    \n",
    "    # Save the output df\n",
    "    PC_ARI_df.to_excel('updated_data/DR/PCA/bestPC_per_dataset_tbl.xlsx', index = False)\n",
    "    \n",
    "    return PC_ARI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40594904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA_ARI(time_RI, comp_RI, hp_dict):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time split datasets\n",
    "    comp_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time combined datasets\n",
    "    OUTPUT: 3 figures\n",
    "    DESCRIPTION: Plot the AARI scores for each PCA gridsearch run. Figure 1-3 correspond to different \n",
    "    imputation types, and plots 1 and 2 represent the MS only vs.\n",
    "    \"\"\"\n",
    "    # Define the number of principal components tested (1 to 11)\n",
    "    pc_range = hp_dict['n_pc']\n",
    "    imp_types = ['All Imputation', 'Type Imputation', 'Neighbor Imputation']\n",
    "    data_types = ['MS Patients Only', 'All Patients']\n",
    "\n",
    "\n",
    "    # Make 3 main figures (imputation types)\n",
    "    for fig_num in range(1, 4):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Make 2 main plots (MS & ALL)\n",
    "        for plot_num in range(1, 3):\n",
    "            plt.subplot(1, 2, plot_num)\n",
    "\n",
    "            if plot_num == 1:\n",
    "                plt.plot(pc_range, time_RI[fig_num - 1][0], label='Time point 1', color = '#D7BDE2')\n",
    "                plt.plot(pc_range, time_RI[fig_num - 1][1], label='Time point 2', color = '#9B59B6')\n",
    "                plt.plot(pc_range, comp_RI[fig_num - 1][0], linestyle='--', label='Time point 1 & 2', color = '#5B2C6F')\n",
    "            elif plot_num == 2:\n",
    "                plt.plot(pc_range, time_RI[fig_num - 1][2], label='Time point 1', color = '#A9DFBF')\n",
    "                plt.plot(pc_range, time_RI[fig_num - 1][3], label='Time point 2', color = '#27AE60')\n",
    "                plt.plot(pc_range, comp_RI[fig_num - 1][1], linestyle='--', label='Time point 1 & 2', color = '#196F3D')\n",
    "\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('ARI Score')\n",
    "            plt.title(f'{imp_types[fig_num - 1]} for {data_types[plot_num - 1]} dataset')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/PCA/PC_v_ARI_plots_figure_{fig_num}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbb5d9-7e71-4571-abb4-007009362402",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'n_pc': range(1, 12)\n",
    "}\n",
    "# Run the PCA gridsearch\n",
    "time_rand_indices, time_best_indices = PCA_gridsearch(time_set_ls, param_dict)\n",
    "comp_rand_indices, comp_best_indices = PCA_gridsearch(complete_set_ls, param_dict)\n",
    "\n",
    "# Make table with best PC number and corresponding ARI score\n",
    "PC_ARI_tbl = make_PC_ARI_table(time_best_indices, comp_best_indices)\n",
    "\n",
    "# Plot the gridsearch runs\n",
    "plot_PCA_ARI(time_rand_indices, comp_rand_indices, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b716f",
   "metadata": {},
   "source": [
    "# Step 3: Apply PCA with Gridsearch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c977647-7876-44d3-be3d-7fae0dcc5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_loadings(df, n_comp, pca_model):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    df : (dataframe)\n",
    "    n_comp : (integer) number of PCs\n",
    "    pca_model : PCA model for the input df\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Create a dataframe of the feature loadings of the given dataframe for the specified number of PCs.\n",
    "    \"\"\"\n",
    "    # Create feature loading dataframe\n",
    "    feat_load = pd.DataFrame(pca_model.components_[:n_comp], columns = df.columns)\n",
    "    \n",
    "    # Structure the dataframe\n",
    "    feat_load = feat_load.transpose()                                     # Get PCs as columns\n",
    "    feat_load.columns = [f'PC {n}' for n in range(1, n_comp + 1)]         # Rename the columns\n",
    "    feat_load.index.name = 'Feature'                                   # Get new index column (not features)\n",
    "    feat_load.reset_index(inplace=True) \n",
    "    \n",
    "    return feat_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_plot_PCA(time_arrays, time_ls, comp_arrays, comp_ls, best_GS_df, exp_var_ls):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_arrays : (nested lists of arrays) nested lists with arrays of the pca embeddings for the time seperated datasets\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_arrays : (nested lists of arrays) nested lists with arrays of the pca embeddings for the time combined datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    exp_var_ls : (nested lists of floats) nested lists of explained variance for the dataframe with their optimal number of PCs\n",
    "    OUTPUT: 3 figures of 2 by 3 subplots\n",
    "    DESCRIPTION: creates 2 dimensional plots of the PCA embedded dataframes\n",
    "    \"\"\"\n",
    "    # Make list of file names for saving, and list to order the plots within the figure\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    ordered_ls = [0,2,1,3,0,1]\n",
    "    GS_ints = [0,2,1,3,12,13]\n",
    "    \n",
    "    # Make n main figures (imputation types)\n",
    "    for fig_num in range(1, len(time_ls) + 1):\n",
    "        plt.figure(figsize=(18, 24))\n",
    "\n",
    "        # Make m main plots\n",
    "        num_plots = len(time_arrays[0]) + len(comp_arrays[0])\n",
    "        for plot_num, df_ind in enumerate(ordered_ls):\n",
    "            plt.subplot(int(num_plots/2), 2, plot_num + 1)\n",
    "            \n",
    "            # Assign a df and array to plot\n",
    "            label_df = time_ls[fig_num - 1][df_ind] if plot_num < len(time_arrays[0]) else comp_ls[fig_num - 1][df_ind]\n",
    "            plotting_array = time_arrays[fig_num - 1][df_ind] if plot_num < len(time_arrays[0]) else comp_arrays[fig_num - 1][df_ind]\n",
    "\n",
    "            # Define the plot colours & label colum\n",
    "            label_col = [col for col in label_df.columns if col.startswith('MS')]\n",
    "            color_map = {0: 'pink', 1: 'orange', 2: 'purple'} if label_col[0] == 'MStype' else {0: 'green', 1: 'purple'}\n",
    "            legend_labels = {0: 'PPMS', 1: 'SPMS', 2: 'RRMS'} if label_col[0] == 'MStype' else {0: 'HC', 1: 'MS'}\n",
    "            mapped_colors = label_df['MStype'].map(color_map) if label_col[0] == 'MStype' else label_df['MS'].map(color_map)            \n",
    "            \n",
    "            # Make plot\n",
    "            for category, color in color_map.items():\n",
    "                indices = label_df[label_col[0]] == category\n",
    "                plt.scatter(plotting_array[indices, 0], plotting_array[indices, 1], \n",
    "                            c = color, label = legend_labels[category], alpha=0.7)\n",
    "            \n",
    "            # Make plot labels            \n",
    "            df_name = best_GS_df.iloc[GS_ints[plot_num], 0]\n",
    "            plt.xlabel('Principle Component 1')\n",
    "            plt.ylabel('Principle Component 2')\n",
    "            plt.title(f'2D PCA for {df_name} Dataset (2PC Variance Explained = {round(exp_var_ls[GS_ints[plot_num]] * 100, 2)}%)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "        GS_ints = [x + 4 if i < 4 else x + 2 for i, x in enumerate(GS_ints)] \n",
    "\n",
    "        # Make figures and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/PCA/{file_name[fig_num - 1]}/2PCA_plots_{file_name[fig_num - 1]}_multicolor.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c08174-c279-4515-bcd2-f9c115bde7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_PCA(time_ls, comp_ls, best_GS_df, plot_param):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the unique sessions datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the combined sessions datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    plot_param : (Boolean) True/False make a 2-dimensional plot of the PCA embeddings\n",
    "    OUTPUT: 2 lists of nested dfs, 6 lists of nested floats\n",
    "    DESCRIPTION: Applies PCA fitting to each of the dataframes in the given list of nested dataframes, based on\n",
    "    its optimal perplexity and learning rate values. Plots the ouput arrays of the PCA fittings if plot_param is True.\n",
    "    \"\"\"\n",
    "    # Initialise output lists\n",
    "    output_time_ls, output_comp_ls = [], []\n",
    "    plot_time_ls, plot_comp_ls = [], []\n",
    "    exp_var_plot_ls = []\n",
    "    time_var_ratios, comp_var_ratios = [], []\n",
    "    time_feature_loadings, comp_feature_loadings = [], []\n",
    "    time_total_var_ratios, comp_total_var_ratios = [], []\n",
    "    \n",
    "    # Needed to iterate through best_GS_df \n",
    "    counter = 0\n",
    "\n",
    "    # Iterate through the unique sessions dataset\n",
    "    for sublist in time_ls:\n",
    "        type_list = []\n",
    "        plot_list = []\n",
    "        sub_var_ratios = []\n",
    "        subtotal_var_ratios = []\n",
    "        sub_feat_loads = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, label column name, and number of pc for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "            n_comp = best_GS_df.iloc[counter, 1]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            dropped_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(dropped_df) \n",
    "\n",
    "            # Run PCA models (Table & plot)\n",
    "            temp_pca_model = PCA(n_components = n_comp)\n",
    "            pca_embedding = temp_pca_model.fit_transform(norm_df)\n",
    "            \n",
    "            plot_pca_model = PCA(n_components = 2)\n",
    "            plot_pca_embedding = plot_pca_model.fit_transform(norm_df)\n",
    "\n",
    "            # Store pca information (table & plot)\n",
    "            type_list.append((n_comp, pca_embedding))\n",
    "            sub_var_ratios.append(temp_pca_model.explained_variance_ratio_)\n",
    "            subtotal_var_ratios.append(sum(temp_pca_model.explained_variance_ratio_))\n",
    "            sub_feat_loads.append(get_feature_loadings(dropped_df, n_comp, temp_pca_model))\n",
    "            \n",
    "            exp_var_plot_ls.append(np.sum(plot_pca_model.explained_variance_ratio_[:2]))\n",
    "            plot_list.append(plot_pca_embedding)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        # Add the new information to the ouput lists\n",
    "        output_time_ls.append(type_list)\n",
    "        time_var_ratios.append(sub_var_ratios)\n",
    "        time_total_var_ratios.append(subtotal_var_ratios)\n",
    "        time_feature_loadings.append(sub_feat_loads)\n",
    "        plot_time_ls.append(plot_list)\n",
    "\n",
    "    # Iterate through the combined sessions dataset\n",
    "    for sublist in comp_ls:\n",
    "        type_list = []\n",
    "        plot_list = []\n",
    "        sub_var_ratios = []\n",
    "        subtotal_var_ratios = []\n",
    "        sub_feat_loads = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, perplexity, learning rate and label column name for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            n_comp = best_GS_df.iloc[counter, 1]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            dropped_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'Time', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(dropped_df) \n",
    "\n",
    "            # Run PCA models (Table & plot)\n",
    "            temp_pca_model = PCA(n_components = n_comp)\n",
    "            pca_embedding = temp_pca_model.fit_transform(norm_df)\n",
    "            \n",
    "            plot_pca_model = PCA(n_components = 2)\n",
    "            plot_pca_embedding = plot_pca_model.fit_transform(norm_df)\n",
    "\n",
    "            # Store pca information (table & plot)\n",
    "            type_list.append((n_comp, pca_embedding))\n",
    "            sub_var_ratios.append(temp_pca_model.explained_variance_ratio_)\n",
    "            subtotal_var_ratios.append(sum(temp_pca_model.explained_variance_ratio_))\n",
    "            sub_feat_loads.append(get_feature_loadings(dropped_df, n_comp, temp_pca_model))\n",
    "            \n",
    "            exp_var_plot_ls.append(np.sum(plot_pca_model.explained_variance_ratio_[:2]))\n",
    "            plot_list.append(plot_pca_embedding)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        # Add the new information to the ouput lists\n",
    "        output_comp_ls.append(type_list)\n",
    "        comp_var_ratios.append(sub_var_ratios)\n",
    "        comp_total_var_ratios.append(subtotal_var_ratios)\n",
    "        comp_feature_loadings.append(sub_feat_loads)\n",
    "        plot_comp_ls.append(plot_list)\n",
    "                       \n",
    "    # Plotting condition (if true, plots are generated)\n",
    "    if plot_param:\n",
    "        group_plot_PCA(plot_time_ls, time_ls, plot_comp_ls, comp_ls, best_GS_df, exp_var_plot_ls)\n",
    "     \n",
    "    return output_time_ls, output_comp_ls, time_var_ratios, comp_var_ratios, time_total_var_ratios, comp_total_var_ratios, time_feature_loadings, comp_feature_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1992f9e-dbb0-43c5-9382-45e77724476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run step 3 (apply_PCA)\n",
    "time_pca_embed, comp_pca_embed, time_var_ratios, comp_var_ratios, time_total_var_ratios, comp_total_var_ratios, time_feat_load, comp_feat_load  = apply_PCA(time_set_ls, complete_set_ls, PC_ARI_tbl, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459777fc-9951-4610-823a-ecff81fd8c1e",
   "metadata": {},
   "source": [
    "# Step 4: Variance Explained & Feature loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_exp_var(PC_ari_df, tot_var_ratios_time, tot_var_ratios_comp):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    PC_ari_df : (dataframe) gridsearch dataframe\n",
    "    tot_var_ratios_time : (nested lists of floats) nested list of total variance ratios for the time split datasets\n",
    "    tot_var_ratios_comp : (nested lists of floats) nested list of total variance ratios for the time combined datasets\n",
    "    OUTPUT: dataframe\n",
    "    DESCRIPTION: Adds the 'total_variance_exp' column with corresponding values to the input dataframe.\n",
    "    \"\"\"\n",
    "    # Initialise the column name and get the data\n",
    "    col_name = 'total_variance_exp'\n",
    "    flattened_time = [num for sublist in tot_var_ratios_time for num in sublist]\n",
    "    flattened_comp = [num for sublist in tot_var_ratios_comp for num in sublist]\n",
    "    \n",
    "    col_values = flattened_time + flattened_comp\n",
    "\n",
    "    # Add the column to the gridsearch dataset\n",
    "    PC_ari_df[col_name] = col_values\n",
    "    \n",
    "    return PC_ari_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_feat_loadings(ls_ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_ls_df : (Nested lists of dataframes) nested lists of feature loading dataframes\n",
    "    OUTPUT: Nested lists of dataframes\n",
    "    DESCRIPTION: Create a dataframe of the feature loadings of the given dataframe for the specified number of PCs.\n",
    "    \"\"\"\n",
    "    # Initialise output list\n",
    "    output_df_ls = []\n",
    "\n",
    "    # Iterate through the imputation lists\n",
    "    for sublst in ls_ls_df:\n",
    "        ft_load_sublist = []\n",
    "        for df in sublst:\n",
    "    \n",
    "            # Initialize the list of best scoring indexes\n",
    "            top_indexes = []\n",
    "            \n",
    "            # Get the absolute values of feature loadings\n",
    "            abs_df = df.iloc[:,1:].abs()\n",
    "\n",
    "            # Iterate over the columns of abs_df\n",
    "            for col in abs_df.columns:\n",
    "                # Get the top 10 indexes\n",
    "                top_10_idx = abs_df[col].nlargest(10).index\n",
    "                top_indexes.extend(top_10_idx)\n",
    "\n",
    "            # Remove duplicates\n",
    "            unique_indexes = list(set(top_indexes))\n",
    "\n",
    "            # Make output df with selected rows\n",
    "            output_df = df.iloc[unique_indexes]\n",
    "            \n",
    "            ft_load_sublist.append(output_df)\n",
    "\n",
    "        output_df_ls.append(ft_load_sublist)\n",
    "    \n",
    "    return output_df_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34322794-d581-42c4-ad0c-76b95ab89a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_significant_feature_loadings(ls_ls_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_ls_df : (Nested list of dataframes) nested lists of feature loading dataframes\n",
    "    OUTPUT: N/A\n",
    "    DESCRIPTION: Saves the dataframes with the significant feature loadings\n",
    "    \"\"\"\n",
    "    # Initialize naming lists and print statement variable\n",
    "    imp_types = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    data_types = ['MS', 'MS', 'ALL', 'ALL']\n",
    "    time_types = ['t1', 't2', 't1', 't2']\n",
    "    list_type_name = ''\n",
    "    \n",
    "    # For time split data\n",
    "    if len(ls_ls_df[0]) == 4:\n",
    "        list_type_name = 'time split'\n",
    "        for i, sublst in enumerate(ls_ls_df):\n",
    "            for j, df in enumerate(sublst):\n",
    "                df.to_excel(f'output/PCA/{imp_types[i]}/top_feature_loading_{data_types[j]}_{imp_types[i]}_{time_types[j]}.xlsx', index = False)\n",
    "   \n",
    "    # For complete dataset\n",
    "    else:\n",
    "        list_type_name = 'complete'\n",
    "        for i, sublst in enumerate(ls_ls_df):\n",
    "            for j, df in enumerate(sublst):\n",
    "                df.to_excel(f'output/PCA/{imp_types[i]}/top_feature_loading_{data_types[j + 1]}_{imp_types[i]}.xlsx', index = False)\n",
    "        \n",
    "    \n",
    "    print(f'All data frames in the {list_type_name} input list have been successfully saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84901fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dataframe with feature loadings\n",
    "PC_ARI_TVR_tbl = add_total_exp_var(PC_ARI_tbl, time_total_var_ratios, comp_total_var_ratios)\n",
    "\n",
    "# Update the bestPC table\n",
    "PC_ARI_TVR_tbl.to_excel('output/PCA/bestPC_per_dataset_tbl.xlsx', index = False)\n",
    "\n",
    "# Get the feature loadings for the unique sessions and combined session\n",
    "top_time_feat_load = get_significant_feat_loadings(time_feat_load)\n",
    "top_comp_feat_load = get_significant_feat_loadings(comp_feat_load)\n",
    "\n",
    "# Save the feature loadings for the unique sessions and combined session\n",
    "save_significant_feature_loadings(top_time_feat_load)\n",
    "save_significant_feature_loadings(top_comp_feat_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa4a25-e655-41ba-94ee-93d10b339fcf",
   "metadata": {},
   "source": [
    "# Step 5: Save PCA embedded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973850d5-a4fe-4baa-895f-2753d6728366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_pca_embedings(ls_ls_pca_embedding, ls_ls_matching_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_pca_embedding : (nested list of np.array) nested list of array of the PCA embedding\n",
    "    ls_ls_matching_df : (nested list of pd.dataframe) nested list of original pre emebedding dataframe\n",
    "    OUTPUT:\n",
    "    DESCRIPTION: Exports the PCA embeddings as dataframes with the same index/ subjects ID as their original dataset. \n",
    "    \"\"\"\n",
    "    imp_file = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    imp_df = ['ia', 'it', 'in']\n",
    "\n",
    "    for imp_idx, imp_ls in enumerate(ls_ls_pca_embedding):\n",
    "        for emb_idx, pca_emb in enumerate(imp_ls):\n",
    "            # Make a dataframe from the array\n",
    "            output_df = pd.DataFrame(pca_emb[1], columns = [f'PC{i+1}' for i in range(pca_emb[1].shape[1])])\n",
    "\n",
    "            # Reintroduce the participant ID (index)\n",
    "            output_df['index'] = ls_ls_matching_df[imp_idx][emb_idx]['index'].reset_index(drop = True)\n",
    "            columns = ['index'] + [col for col in output_df.columns if col != 'index']\n",
    "            # Reorder the columns such that index is first\n",
    "            output_df = output_df[columns]\n",
    "\n",
    "            sub_type = ''\n",
    "            year = ''\n",
    "            # Check for time split list or not\n",
    "            if len(ls_ls_pca_embedding[0]) > 3:\n",
    "                print('entered time list statement')\n",
    "                if emb_idx < 2:\n",
    "                    sub_type = 'MStrain_'\n",
    "                else: \n",
    "                    sub_type = 'ALLtrain_'\n",
    "                \n",
    "                if emb_idx % 2 == 0:\n",
    "                    year = '00'\n",
    "                else:\n",
    "                    year = '05'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/PCA/{imp_file[imp_idx]}/PCA_{sub_type}{imp_df[imp_idx]}{year}.xlsx', index=False)\n",
    "            \n",
    "            else:\n",
    "                print('entered no split list statement')\n",
    "                if emb_idx == 0:\n",
    "                    sub_type = 'MStrain_'\n",
    "                else: \n",
    "                    sub_type = 'ALLtrain_'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/PCA/{imp_file[imp_idx]}/PCA_{sub_type}{imp_df[imp_idx]}.xlsx', index=False)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71f974-db40-46bc-8693-d5cc21df2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export_pca_embedings function for the unique sessions and the combined sessions.\n",
    "export_pca_embedings(time_pca_embed, time_set_ls)\n",
    "export_pca_embedings(comp_pca_embed, complete_set_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
