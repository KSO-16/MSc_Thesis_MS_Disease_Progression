{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dcc62f-8efe-498d-addd-edfc4414a6ff",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains all of the scripts used to tune and apply t-distributed Stochastic Neighbor Embedding (tSNE) to the datasets cleaned and merged in the data_preprocessing_DR.ipynb notebook. The following 4 steps are taken in this notebook:\n",
    "\n",
    "Step 1: Import the necessary libraries and datasets for the dimensionality reduction. The datasets involve a combination of MS/ALL, imputation methods, and unique/combined sessions. There is a total of 18 training datasets.\n",
    "\n",
    "Step 2: Tune the hyperparameters (hps) of the tSNE algorithm (perplexity and learning rate) using K_fold in the make_K_folds function and the tSNE_gridsearch function. The dataset is split into 3 folds. For unique sessions datasets the subjects are split into train and test subjects which are then used to obtain the train and test dataset. For the combined sessions, the train subjects at Y00 and Y05 are used for training and the test subjects at Y05 are used for testing. The current train fold is used to fit the tSNE and Kmeans algorithm, the fitted tSNE is then used to get the test tSNE embeddings which are then used for making predictions with Kmeans. Adjusted rand index (ARI) is then used to evaluate the predictions against the true test labels. This sequence is repeated for every fold combination, after which the average ARI (AARI) is obtained for the given hps combination. These steps are then repeated for the other hyperparameter (hp) values. A dataframe of the optimal hp value per dataset is then created with the make_gridsearch_table function.\n",
    "\n",
    "Step 3: The best hp value per dataset found during step 2 is used to make the training tSNE embeddings using the apply_tSNE function. A 2 dimensional plot of the embedded data is produced in the function group_plot_tSNE. The apply_tSNE function provides the embedded arrays for the unique sessions and combined session dataset. \n",
    "\n",
    "Step 4: The embedded tSNE arrays are saved and reserved for later use.\n",
    "\n",
    "These datasets were used to assess the performance of tSNE in comparison with PCA, UMAP, and TPHATE. The statistical outcomes of part 1 of the project can be found in the 'Dimensionality Reduction' subsection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cbc6d",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b56650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a420815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the combined sessions datasets\n",
    "MS_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia.xlsx')\n",
    "ALL_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia.xlsx')\n",
    "MS_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it.xlsx')\n",
    "ALL_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it.xlsx')\n",
    "MS_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in.xlsx')\n",
    "ALL_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time imputation method)\n",
    "MS_t1_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia00.xlsx')\n",
    "MS_t2_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia05.xlsx')\n",
    "ALL_t1_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia00.xlsx')\n",
    "ALL_t2_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Type imputation method)\n",
    "MS_t1_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it00.xlsx')\n",
    "MS_t2_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it05.xlsx')\n",
    "ALL_t1_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it00.xlsx')\n",
    "ALL_t2_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Neighbor imputation method)\n",
    "MS_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in00.xlsx')\n",
    "MS_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in05.xlsx')\n",
    "ALL_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in00.xlsx')\n",
    "ALL_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in05.xlsx')\n",
    "\n",
    "# Group the datasets by imputation methods and then by unique session \n",
    "time_set_all = [MS_t1_imp_all, MS_t2_imp_all, ALL_t1_imp_all, ALL_t2_imp_all]\n",
    "time_set_type = [MS_t1_imp_type, MS_t2_imp_type, ALL_t1_imp_type, ALL_t2_imp_type]\n",
    "time_set_nb = [MS_t1_imp_nb,  MS_t2_imp_nb, ALL_t1_imp_nb, ALL_t2_imp_nb]\n",
    "time_set_ls = [time_set_all, time_set_type, time_set_nb]\n",
    "\n",
    "# Group the datasets by imputation methods and then by combined sessions\n",
    "complete_set_ls = [[MS_imp_all, ALL_imp_all], [MS_imp_type, ALL_imp_type], [MS_imp_nb, ALL_imp_nb]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c5452",
   "metadata": {},
   "source": [
    "# Step 2: Hyperparameter tuning for tSNE (perplexity & learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b4256-4f0f-4c6b-aed6-de966ea92135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_folds(df):\n",
    "    \"\"\"\n",
    "    INPUT: Dataframe\n",
    "    OUPUT: Nested list of dataframes containing the train and test datasets\n",
    "    DESCRIPTION: Use k folds to split the input data into 3 splits, and return the x_train, x_test, y_train, and y_test for each of the\n",
    "    folds as unique lists.\n",
    "    \"\"\"\n",
    "    # Get the subject IDs & number of unique time points\n",
    "    subjects = df['index'].unique()\n",
    "    num_ses = len(df['Time'].unique())\n",
    "    \n",
    "    # Get the true labels\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "    labels = df[label_col[0]]\n",
    "    \n",
    "    # Normalize the dataframe\n",
    "    norm_df = df.drop(columns=['EDSS', 'BL_Avg_cognition', 'Time', 'index'] + label_col, axis=1)\n",
    "    norm_df = StandardScaler().fit_transform(norm_df)\n",
    "    \n",
    "    # Initialize KFold for subjects\n",
    "    kfold = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Initialize output list\n",
    "    x_train_list, x_test_list, y_test_list = [], [], []\n",
    "    \n",
    "    if num_ses == 1: # For unique session datasets\n",
    "\n",
    "        for train_indices, test_indices in kfold.split(norm_df):\n",
    "            # Get the training and test data for fold\n",
    "            x_train, x_test = norm_df[train_indices], norm_df[test_indices]\n",
    "            y_test = labels[test_indices]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "        \n",
    "    else: # For combined sessions datasets\n",
    "        for train_idx, test_idx in kfold.split(df[df['Time'] == 1]):\n",
    "            # Get the subject IDs for the training and test sets\n",
    "            fold_subjects = subjects[test_idx]\n",
    "\n",
    "            # Get the indices for test set\n",
    "            test_idx = df[(df['index'].isin(fold_subjects)) & (df['Time'] == 2)].index\n",
    "\n",
    "            # Get the training and test data\n",
    "            x_train, x_test = norm_df[train_idx], norm_df[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_test)\n",
    "    \n",
    "    return [x_train_list, x_test_list, y_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tSNE_gridsearch(ls_ls_df, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_ls_df : (Nested lists of dataframes)\n",
    "    OUTPUT: Nested lists of integers (Average adjusted rand index scores)\n",
    "    DESCRIPTION: Find the optimal perplexity value & learning rate to include for each of the df based on their \n",
    "    obtained average adjusted rand index\n",
    "    \"\"\"\n",
    "    # Define the perplexity, learning rate ranges and the number of runs\n",
    "    perplexities = hp_dic['perplexities']\n",
    "    learning_rates = hp_dic['learning_rates']\n",
    "    n_runs = 2\n",
    "    \n",
    "    # Initialize the output list to maintain same structure as input list\n",
    "    output_rand_indices = []\n",
    "\n",
    "    # Iterate over the imputation methods lists\n",
    "    for m, sublist in enumerate(ls_ls_df):\n",
    "        sublist_rand_indices = []\n",
    "        for n, df in enumerate(sublist):\n",
    "            # Get the number of clusters needed for the df\n",
    "            n_clusters = 2 if 'MS' in df.columns else 4\n",
    "            \n",
    "            # Get k-fold splits of the data\n",
    "            k_fold_ls = make_K_folds(df)\n",
    "            \n",
    "            # Initializes the rand indices array (per df)           \n",
    "            rand_indices = np.zeros((len(perplexities), len(learning_rates)))  \n",
    "\n",
    "            # Perform grid search\n",
    "            for i, perplexity in enumerate(perplexities):\n",
    "                for j, learning_rate in enumerate(learning_rates):\n",
    "                    temp_ARIs = []\n",
    "                    for k in range(0, len(k_fold_ls[0])):\n",
    "                        x_train = k_fold_ls[0][k]\n",
    "                        x_test = k_fold_ls[1][k]\n",
    "                        y_test = k_fold_ls[2][k]\n",
    "                        \n",
    "                        for run in range(n_runs):\n",
    "                            # Initialise and fit tSNE model\n",
    "                            tsne_model = TSNE(n_components = 2, perplexity = perplexity, \n",
    "                                              learning_rate = learning_rate, random_state = k)\n",
    "                            x_train_tsne = tsne_model.fit_transform(x_train)\n",
    "                            x_test_tsne = tsne_model.transform(x_test)\n",
    "                            \n",
    "                            # Fit & apply K-means clustering\n",
    "                            kmeans = KMeans(n_clusters = n_clusters, random_state = 42, n_init = 'auto')\n",
    "                            kmeans.fit(x_train_tsne)\n",
    "                            y_pred = kmeans.predict(x_test_tsne)\n",
    "                            temp_ARIs.append(adjusted_rand_score(y_test, y_pred))\n",
    "\n",
    "                    # Get average ARI score\n",
    "                    rand_indices[i, j] = np.mean(temp_ARIs)\n",
    "            \n",
    "            # Add AARI score array per df to sublist (for imputation type)\n",
    "            sublist_rand_indices.append(rand_indices)\n",
    "            print(f'Gridsearch for dataset {n} of type list {m} is completed')\n",
    "        \n",
    "        # Add (imputation type) sublist to output list\n",
    "        output_rand_indices.append(sublist_rand_indices)\n",
    "        \n",
    "    return output_rand_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325e6e1-ebc3-45c6-b478-c5697fa4772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_row_names():\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    OUTPUT: List of strings\n",
    "    DESCRIPTION: Creates the row names corresponding to the different datasets included in the make_gridsearch_tbl \n",
    "    function\n",
    "    \"\"\"\n",
    "    ls_row_names = []\n",
    "    ls_types = ['imp_type']\n",
    "    ls_subjects = ['MS_', 'ALL_']\n",
    "    \n",
    "    for str3 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            for str2 in ['t1_', 't2_', 't3_']:\n",
    "                name = str1 + str2 + str3\n",
    "                ls_row_names.append(name)\n",
    "    for str2 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            name = str1 + str2\n",
    "            ls_row_names.append(name)   \n",
    "    \n",
    "    return ls_row_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854f711-3306-4c47-86e6-c94408487b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gridsearch_tbl(time_RI, comp_RI, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (nested lists of arrays) arrays AARI float for time split datasets\n",
    "    comp_RI : (nested lists of arrays) arrays AARI float for time combined datasets\n",
    "    OUTPUT: Dataframe\n",
    "    DESCRIPTION: Create a table (df) with dataset names in column 1, best perplexity values in column 2, best \n",
    "    learning rate value in column 3, and gridsearch AARI scores in column 4. \n",
    "    \"\"\"\n",
    "    # Make row names\n",
    "    dataset_names = make_row_names()\n",
    "    \n",
    "    perplexities = hp_dic['perplexities']\n",
    "    learning_rates = hp_dic['learning_rates']\n",
    "\n",
    "    # Initialize lists to store maximum values, row indices, and column indices\n",
    "    max_values = []\n",
    "    max_perp_indices = [] # rows\n",
    "    max_LR_indices = [] #cols\n",
    "\n",
    "    # Iterate over time list (time_RI)\n",
    "    for sublist in time_RI:\n",
    "        for array in sublist:\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_values.append(array.max())\n",
    "            max_perp_indices.append(np.argwhere(array == array.max())[0][0]) # first occurance | row index\n",
    "            max_LR_indices.append(np.argwhere(array == array.max())[0][1]) # first occurance | col index\n",
    "    \n",
    "    # Iterate over combined time list (comp_RI)\n",
    "    for sublist in comp_RI:\n",
    "        for array in sublist:\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_values.append(array.max())\n",
    "            max_perp_indices.append(np.argwhere(array == array.max())[0][0]) # first occurance | row index\n",
    "            max_LR_indices.append(np.argwhere(array == array.max())[0][1]) # first occurance | col index\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'Dataset': dataset_names,\n",
    "        'Best_Perplexity': [perplexities[i] for i in max_perp_indices],\n",
    "        'Best_Learning_Rate': [learning_rates[j] for j in max_LR_indices],\n",
    "        'Best_ARI_Score': max_values})\n",
    "     \n",
    "    output_df.to_excel('output/tSNE/best_gridsearch_per_dataset_tbl.xlsx', index = False)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a00e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE_gridsearch(time_RI, comp_RI, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time split datasets\n",
    "    comp_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time combined datasets\n",
    "    OUTPUT: 3 figures\n",
    "    DESCRIPTION: Plot the AARI scores for each tSNE gridsearch run. Figures 1-3 correspond to different \n",
    "    imputation methods and plots 1 and 2 represent the pwMS vs HC + pwMS datasets.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameters\n",
    "    perplexities = hp_dic['perplexities']\n",
    "    learning_rates = hp_dic['learning_rates']\n",
    "\n",
    "    # Define the lists to iterate over for plotting & naming. \n",
    "    df_ind_range = [0, 2, 1, 3]\n",
    "    imp_types = ['All Imputation', 'Type Imputation', 'Neighbor Imputation']\n",
    "    time_point = ['1', '1', '2', '2', 'Combined']\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    MS_col_ls = ['#D7BDE2', '#A569BD', '#7D3C98', '#4A235A']\n",
    "    ALL_col_ls = ['#A9DFBF', '#27AE60', '#1E8449', '#145A32']\n",
    "\n",
    "    # Make 3 main figures (imputation types)\n",
    "    for fig_num in range(1, 4):\n",
    "        plt.figure(figsize=(18, 24))\n",
    "\n",
    "        # Make 6 main plots (MS & ALL)\n",
    "        for plot_num in range(1, 7):\n",
    "            plt.subplot(3, 2, plot_num)\n",
    "\n",
    "            if plot_num % 2 != 0 and plot_num != 5:\n",
    "                df_ind = df_ind_range[plot_num - 1]\n",
    "                plot_title = ('MS Patients Only', time_point[plot_num - 1])\n",
    "                for i, lr in enumerate(learning_rates):\n",
    "                    plt.plot(perplexities, time_RI[fig_num - 1][df_ind][:,i], label = f'learning rate={learning_rates[i]}', color = MS_col_ls[i])\n",
    "\n",
    "            elif plot_num % 2 == 0 and plot_num != 6:\n",
    "                df_ind = df_ind_range[plot_num - 1]\n",
    "                plot_title = ('All Patients', time_point[plot_num - 1])\n",
    "                for i, lr in enumerate(learning_rates):\n",
    "                    plt.plot(perplexities, time_RI[fig_num - 1][df_ind][:,i], label = f'learning rate={learning_rates[i]}', color = ALL_col_ls[i])\n",
    "                \n",
    "            elif plot_num == 5:\n",
    "                plot_title = ('MS Patients Only', time_point[4])\n",
    "                for i, lr in enumerate(learning_rates):\n",
    "                    plt.plot(perplexities, comp_RI[fig_num - 1][plot_num - 5][:,i], label = f'learning rate={learning_rates[i]}', color = MS_col_ls[i])\n",
    "\n",
    "            elif plot_num == 6:\n",
    "                plot_title = ('All Patients', time_point[4])\n",
    "                for i, lr in enumerate(learning_rates):\n",
    "                    plt.plot(perplexities, comp_RI[fig_num - 1][plot_num - 5][:,i], label = f'learning rate={learning_rates[i]}', color = ALL_col_ls[i])\n",
    "\n",
    "            plt.xlabel('Perplexity')\n",
    "            plt.ylabel('Average Adjusted Rand Index')\n",
    "            plt.title(f'{imp_types[fig_num - 1]} for {plot_title[0]} Dataset at Time Point {plot_title[1]}')\n",
    "            plt.legend(title = 'Learning Rates')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/tSNE/{file_name[fig_num - 1]}/Gridsearch_plots_figure_{file_name[fig_num - 1]}_{plot_title[1]}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335148e3-5208-4b44-8040-6a674f330173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hps to use in gridsearch\n",
    "param_dict = {\n",
    "    'perplexities': [2, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
    "    'learning_rates': [0.1, 0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Run the tSNE gridsearch\n",
    "time_gridsearch = tSNE_gridsearch(time_set_ls)\n",
    "comp_gridsearch = tSNE_gridsearch(complete_set_ls)\n",
    "\n",
    "# Make table with best PC number and corresponding ARI score\n",
    "best_gridsearch_df = make_gridsearch_tbl(time_gridsearch, comp_gridsearch)\n",
    "\n",
    "# Save the gridsearch table as an excel file\n",
    "best_gridsearch_df.to_excel('output/tSNE/best_gridsearch_per_impdata.xlsx')\n",
    "\n",
    "# Plot the gridsearch runs\n",
    "plot_tSNE_gridsearch(time_gridsearch, comp_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f7ba1",
   "metadata": {},
   "source": [
    "# Step 3: Apply tSNE with Gridsearch Results (+ plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24839826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_plot_tSNE(time_arrays, time_ls, comp_arrays, comp_ls, best_GS_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_arrays : (nested lists of arrays) nested lists with arrays of the tSNE embeddings for the time seperated datasets\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_arrays : (nested lists of arrays) nested lists with arrays of the tSNE embeddings for the time combined datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    OUTPUT: 3 figures of 2 by 3 subplots\n",
    "    DESCRIPTION: creates 2 dimensional plots of the tSNE embedded dataframes\n",
    "    \"\"\"\n",
    "    # Make list of file names for saving, and list to order the plots within the figure\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    ordered_ls = [0,2,1,3,0,1]\n",
    "    GS_ints = [0,2,1,3,12,13]\n",
    "    \n",
    "    # Make n main figures (imputation types)\n",
    "    for fig_num in range(1, len(time_ls) + 1):\n",
    "        plt.figure(figsize=(18, 24))\n",
    "\n",
    "        # Make m main plots\n",
    "        num_plots = len(time_arrays[0]) + len(comp_arrays[0])\n",
    "        for plot_num, df_ind in enumerate(ordered_ls):\n",
    "            plt.subplot(int(num_plots/2), 2, plot_num + 1)\n",
    "            \n",
    "            # Assign a df and array to plot\n",
    "            label_df = time_ls[fig_num - 1][df_ind] if plot_num < len(time_arrays[0]) else comp_ls[fig_num - 1][df_ind]\n",
    "            plotting_array = time_arrays[fig_num - 1][df_ind] if plot_num < len(time_arrays[0]) else comp_arrays[fig_num - 1][df_ind]    \n",
    "            \n",
    "            # Define the plot colours & label colum\n",
    "            label_col = [col for col in label_df.columns if col.startswith('MS')]\n",
    "            color_map = {0: 'pink', 1: 'orange', 2: 'purple'} if label_col[0] == 'MStype' else {0: 'green', 1: 'purple'}\n",
    "            legend_labels = {0: 'PPMS', 1: 'SPMS', 2: 'RRMS'} if label_col[0] == 'MStype' else {0: 'HC', 1: 'MS'}\n",
    "            mapped_colors = label_df['MStype'].map(color_map) if label_col[0] == 'MStype' else label_df['MS'].map(color_map)  \n",
    "\n",
    "            # Make plot\n",
    "            for category, color in color_map.items():\n",
    "                indices = label_df[label_col[0]] == category\n",
    "                plt.scatter(plotting_array[indices, 0], plotting_array[indices, 1], \n",
    "                            c = color, label = legend_labels[category], alpha=0.7)\n",
    "            \n",
    "            # Make plot labels\n",
    "            df_name = best_GS_df.iloc[GS_ints[plot_num], 0]\n",
    "            perplexity = best_GS_df.iloc[GS_ints[plot_num], 1]\n",
    "            learning_rate = best_GS_df.iloc[GS_ints[plot_num], 2]\n",
    "            plt.xlabel('t-SNE Component 1')\n",
    "            plt.ylabel('t-SNE Component 2')\n",
    "            plt.title(f'2D t-SNE for {df_name} Dataset (Perplexity={perplexity}, Learning Rate={learning_rate})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)  \n",
    "        \n",
    "        GS_ints = [x + 4 if i < 4 else x + 2 for i, x in enumerate(GS_ints)]\n",
    "\n",
    "        # Make figures and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/tSNE/{file_name[fig_num - 1]}/best_param_tsne_plots_{file_name[fig_num - 1]}_multicolor.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d45a43-895c-415d-8579-77471acc6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tSNE(time_ls, comp_ls, best_GS_df, plot_param):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    plot_param : (Boolean) True/False make a 2-dimensional plot of the tSNE embeddings\n",
    "    OUTPUT: 2 lists of nested dfs, 6 lists of nested floats\n",
    "    DESCRIPTION: Applies tSNE fitting to each of the dataframes in the given list of nested dataframes, based on\n",
    "    its optimal perplexity and learning rate values. Plots the ouput arrays of the tSNE fittings if plot_param is\n",
    "    True.\n",
    "    \"\"\"\n",
    "    # Initialise output lists\n",
    "    output_time_ls, output_comp_ls = [], []\n",
    "    \n",
    "    # Needed to iterate through best_GS_df \n",
    "    counter = 0\n",
    "    \n",
    "    # Iterate through the unique sessions dataset\n",
    "    for sublist in time_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, perplexity, learning rate and label column name for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            perplexity = best_GS_df.iloc[counter, 1]\n",
    "            learning_rate = best_GS_df.iloc[counter, 2]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "\n",
    "            # Run the tSNE model\n",
    "            tsne_model = TSNE(n_components = 2, perplexity = perplexity, learning_rate = learning_rate, random_state = 42)\n",
    "            tsne_array = tsne_model.fit_transform(norm_df)\n",
    "\n",
    "            type_list.append(tsne_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        output_time_ls.append(type_list)\n",
    "        \n",
    "\n",
    "    for sublist in comp_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, perplexity, learning rate and label column name for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            perplexity = best_GS_df.iloc[counter, 1]\n",
    "            learning_rate = best_GS_df.iloc[counter, 2]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "\n",
    "            # Run the tSNE model\n",
    "            tsne_model = TSNE(n_components = 2, perplexity = perplexity, learning_rate = learning_rate, random_state = 42)\n",
    "            tsne_array = tsne_model.fit_transform(norm_df)\n",
    "\n",
    "            type_list.append(tsne_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        output_comp_ls.append(type_list)\n",
    "                       \n",
    "    # Plotting condition (if true, plots are generated)\n",
    "    if plot_param:\n",
    "        group_plot_tSNE(output_time_ls, time_ls, output_comp_ls, comp_ls, best_GS_df)\n",
    "     \n",
    "    return output_time_ls, output_comp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a422d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run step 3 (apply_tSNE)\n",
    "time_tsne_arrays, comp_tsne_arrays = apply_tSNE(time_set_ls, complete_set_ls, best_gridsearch_df, True)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0135a-246f-4b49-b032-49ae79bd57eb",
   "metadata": {},
   "source": [
    "# Step 4: Save the tSNE embedded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afeabe-3fcf-4262-bf01-90e25cf97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tSNE_embedings(ls_ls_tsne_array, ls_ls_matching_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_tsne_array : (nested list of np.array) nested list of array of the tSNE embedding\n",
    "    ls_ls_matching_df : (nested list of pd.dataframe) nested list of original pre embedding dataframe\n",
    "    OUTPUT:\n",
    "    DESCRIPTION: Exports the tSNE embeddings as dataframes with the same index/ subjects ID as their original dataset. \n",
    "    \"\"\"\n",
    "    imp_file = ['all', 'type', 'neighbor']\n",
    "    imp_df = ['ia', 'it', 'in']\n",
    "\n",
    "    for imp_idx, imp_ls in enumerate(ls_ls_tsne_array):\n",
    "        for emb_idx, tsne_emb in enumerate(imp_ls):\n",
    "            # Make a dataframe from the array\n",
    "            output_df = pd.DataFrame(tsne_emb, columns = [f'tSNE{i+1}' for i in range(tsne_emb.shape[1])])\n",
    "\n",
    "            # Reintroduce the patients ID (index)\n",
    "            output_df['index'] = ls_ls_matching_df[imp_idx][emb_idx]['index'].reset_index(drop = True)\n",
    "            columns = ['index'] + [col for col in output_df.columns if col != 'index']\n",
    "            \n",
    "            # Reorder the columns such that index is first\n",
    "            output_df = output_df[columns]\n",
    "\n",
    "            sub_type = ''\n",
    "            year = ''\n",
    "            # Check for time split list or not\n",
    "            if len(ls_ls_matching_df[0]) > 3:\n",
    "                sub_type = 'MStrain_' if emb_idx < 2 else 'ALLtrain_'\n",
    "                year = '00' if emb_idx % 2 == 0 else '05'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/tSNE/{imp_file[imp_idx]}/tSNE_{sub_type}{imp_df[imp_idx]}{year}.xlsx', index=False)\n",
    "            \n",
    "            else:\n",
    "                sub_type = 'MStrain_' if emb_idx == 0 else 'ALLtrain_'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/tSNE/{imp_file[imp_idx]}/tSNE_{sub_type}{imp_df[imp_idx]}.xlsx', index=False)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ff01d-7277-4d55-978a-ca438cff882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export_tSNE_embedings function for the unique sessions and the combined sessions.\n",
    "export_tSNE_embedings(time_tsne_arrays, time_set_ls)\n",
    "export_tSNE_embedings(comp_tsne_arrays, complete_set_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
