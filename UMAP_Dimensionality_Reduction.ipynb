{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565a29e-e474-45af-a611-344c4bfd8950",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains all of the scripts used to tune and apply Uniform Manifold Approximation and Projection (UMAP) to the datasets cleaned and merged in the data_preprocessing_DR.ipynb notebook. The following 4 steps are taken in this notebook:\n",
    "\n",
    "Step 1: Import the necessary libraries and datasets for the dimensionality reduction. The datasets involve a combination of MS/ALL, imputation methods, and unique/combined sessions. There is a total of 18 training datasets. Prior to loading all of the datasets, the 'umap' library will need to be installed with pip, conda or any other library installer.\n",
    "\n",
    "Step 2: Tune the hyperparameters (hps) of the UMAP algorithm (perplexity, minimum distance, and metric) using K_fold in the make_K_folds function and the UMAP_gridsearch function. The dataset is split into 3 folds. For unique sessions datasets the subjects are split into train and test subjects which are then used to obtain the train and test dataset. For the combined sessions, the train subjects at Y00 and Y05 are used for training and the test subjects at Y05 are used for testing. The current train fold is used to fit the UMAP and Kmeans algorithm, the fitted UMAP is then used to get the test UMAP embeddings which are then used for making predictions with Kmeans. Adjusted rand index (ARI) is then used to evaluate the predictions against the true test labels. This sequence is repeated for every fold combination, after which the average ARI (AARI) is obtained for the given hps combination. These steps are then repeated for the other hyperparameter (hp) values. A dataframe of the optimal hp value per dataset is then created with the make_gridsearch_table function.\n",
    "\n",
    "Step 3: The best hp value per dataset found during step 2 is used to make the training UMAP embeddings using the apply_UMAP function. A 2 dimensional plot of the embedded data is produced in the function group_plot_UMAP. The apply_UMAP function provides the embedded arrays for the unique sessions and combined session dataset. \n",
    "\n",
    "Step 4: The embedded UMAP arrays are saved and reserved for later use.\n",
    "\n",
    "These datasets were used to assess the performance of UMAP in comparison with PCA, tSNE, and TPHATE. The statistical outcomes of part 1 of the project can be found in the 'Dimensionality Reduction' subsection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d8a9b",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5fec4-d1f6-41dc-8bb9-c466b4b04e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the combined sessions datasets\n",
    "MS_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia.xlsx')\n",
    "ALL_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia.xlsx')\n",
    "MS_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it.xlsx')\n",
    "ALL_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it.xlsx')\n",
    "MS_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in.xlsx')\n",
    "ALL_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time imputation method)\n",
    "MS_t1_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia00.xlsx')\n",
    "MS_t2_imp_all = pd.read_excel('prepro_data/all_imp/MStrain_ia05.xlsx')\n",
    "ALL_t1_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia00.xlsx')\n",
    "ALL_t2_imp_all = pd.read_excel('prepro_data/all_imp/ALLtrain_ia05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Type imputation method)\n",
    "MS_t1_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it00.xlsx')\n",
    "MS_t2_imp_type = pd.read_excel('prepro_data/type_imp/MStrain_it05.xlsx')\n",
    "ALL_t1_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it00.xlsx')\n",
    "ALL_t2_imp_type = pd.read_excel('prepro_data/type_imp/ALLtrain_it05.xlsx')\n",
    "\n",
    "# Import the unique sessions datasets (for the Time + Neighbor imputation method)\n",
    "MS_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in00.xlsx')\n",
    "MS_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/MStrain_in05.xlsx')\n",
    "ALL_t1_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in00.xlsx')\n",
    "ALL_t2_imp_nb = pd.read_excel('prepro_data/nb_imp/ALLtrain_in05.xlsx')\n",
    "\n",
    "# Group the datasets by imputation methods and then by unique session \n",
    "time_set_all = [MS_t1_imp_all, MS_t2_imp_all, ALL_t1_imp_all, ALL_t2_imp_all]\n",
    "time_set_type = [MS_t1_imp_type, MS_t2_imp_type, ALL_t1_imp_type, ALL_t2_imp_type]\n",
    "time_set_nb = [MS_t1_imp_nb,  MS_t2_imp_nb, ALL_t1_imp_nb, ALL_t2_imp_nb]\n",
    "time_set_ls = [time_set_all, time_set_type, time_set_nb]\n",
    "\n",
    "# Group the datasets by imputation methods and then by combined sessions\n",
    "complete_set_ls = [[MS_imp_all, ALL_imp_all], [MS_imp_type, ALL_imp_type], [MS_imp_nb, ALL_imp_nb]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39e5f6",
   "metadata": {},
   "source": [
    "# Step 2: Hyperparameter tuning for UMAP (perplexity, minimum distance, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9aed21-07a2-4388-92df-d97def61c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_folds(df):\n",
    "    \"\"\"\n",
    "    INPUT: Dataframe\n",
    "    OUPUT: Nested list of dataframes containing the train and test datasets\n",
    "    DESCRIPTION: Use k folds to split the input data into 3 splits, and return the x_train, x_test, y_train, and y_test for each of the\n",
    "    folds as unique lists.\n",
    "    \"\"\"\n",
    "    # Get the subject IDs & number of unique time points\n",
    "    subjects = df['index'].unique()\n",
    "    num_ses = len(df['Time'].unique())\n",
    "    \n",
    "    # Get the true labels\n",
    "    label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "    labels = df[label_col[0]]\n",
    "    \n",
    "    # Normalize the dataframe\n",
    "    norm_df = df.drop(columns=['EDSS', 'BL_Avg_cognition', 'Time', 'index', 'HC_CI_CP'] + label_col, axis=1)\n",
    "    norm_df = StandardScaler().fit_transform(norm_df)\n",
    "    \n",
    "    # Initialize KFold for subjects\n",
    "    kfold = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Initialize output list\n",
    "    x_train_list, x_test_list, y_test_list = [], [], []\n",
    "    \n",
    "    if num_ses == 1:  # For unique session datasets\n",
    "        for train_indices, test_indices in kfold.split(norm_df):\n",
    "            # Get the training and test data for fold\n",
    "            x_train, x_test = norm_df[train_indices], norm_df[test_indices]\n",
    "            y_test = labels[test_indices]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "        \n",
    "    else: # For combined sessions datasets\n",
    "        for train_idx, test_idx in kfold.split(df[df['Time'] == 1]):\n",
    "            # Get the subject IDs for the training and test sets\n",
    "            fold_subjects = subjects[test_idx]\n",
    "\n",
    "            # Get the indices for test set\n",
    "            test_idx = df[(df['index'].isin(fold_subjects)) & (df['Time'] == 2)].index\n",
    "\n",
    "            # Get the training and test data\n",
    "            x_train, x_test = norm_df[train_idx], norm_df[test_idx]\n",
    "            y_test = labels[test_idx]\n",
    "\n",
    "            # Add the fold's training and test data to the corresponding list\n",
    "            x_train_list.append(x_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_train)\n",
    "    \n",
    "    return [x_train_list, x_test_list, y_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afe2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UMAP_gridsearch(ls_ls_df, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    ls_ls_df : (Nested lists of dataframes)\n",
    "    OUTPUT: Nested lists of integers (Average adjusted rand index scores)\n",
    "    DESCRIPTION: Find the optimal perplexity value & minimum distance to include for each of the df based on their \n",
    "    obtained AARI.\n",
    "    \"\"\"\n",
    "    # Define the perplexity, learning rate ranges, and metrics and the number of runs\n",
    "    n_runs = 2\n",
    "    \n",
    "    # Initialize the output list to maintain the same structure as the input list\n",
    "    output_rand_lists = []\n",
    "\n",
    "    # Iterate over the imputation methods lists\n",
    "    for m, sublist in enumerate(ls_ls_df):\n",
    "        sublist_rand_indices = []\n",
    "        for n, df in enumerate(sublist):\n",
    "            # Get the number of clusters needed for the df\n",
    "            n_clusters = 2 if 'MS' in df.columns else 4\n",
    "            \n",
    "            # Get k-fold splits of the data\n",
    "            k_fold_ls = make_K_folds(df)\n",
    "            \n",
    "            # Initializes the rand indices array (per df)           \n",
    "            rand_indices = np.zeros((len(hp_dic['perplexities']), len(hp_dic['min_dist']), len(hp_dic['metrics'])))\n",
    "\n",
    "            # Perform grid search\n",
    "            for i, perplexity in enumerate(hp_dic['perplexities']):\n",
    "                for j, min_dist in enumerate(hp_dic['min_dist']):\n",
    "                    for k, metric in enumerate(hp_dic['metrics']):\n",
    "                        temp_ARIs = []\n",
    "                        for l in range(0, len(k_fold_ls[0])):\n",
    "                            x_train = k_fold_ls[0][l]\n",
    "                            x_test = k_fold_ls[1][l]\n",
    "                            y_test = k_fold_ls[2][l]\n",
    "\n",
    "                            for run in range(n_runs):\n",
    "                                # Initialise UMAP and fit/transform the x data\n",
    "                                umap_model = UMAP(n_components = 2, n_neighbors = perplexity, \n",
    "                                                  min_dist = min_dist, metric = metric, random_state = k)\n",
    "                                x_train_umap = umap_model.fit_transform(x_train)\n",
    "                                x_test_umap = umap_model.transform(x_test)\n",
    "\n",
    "                                # Fit & apply K-means clustering\n",
    "                                kmeans = KMeans(n_clusters = n_clusters, random_state = 42, n_init = 'auto')\n",
    "                                kmeans.fit(x_train_umap)\n",
    "                                y_pred = kmeans.predict(x_test_umap)\n",
    "                                temp_ARIs.append(adjusted_rand_score(y_test, y_pred))\n",
    "\n",
    "                        # Get average ARI score\n",
    "                        rand_indices[i, j, k] = np.mean(temp_ARIs)\n",
    "                        \n",
    "                print(f'{i + 1}/15 perplexity values completed')\n",
    "                \n",
    "            # Add AARI per hps (for df) to sublist (for imputation type)\n",
    "            sublist_rand_indices.append(rand_indices)\n",
    "            print(f'Gridsearch for dataset {n} of type list {m} is completed')\n",
    "        \n",
    "        # Add (imputation type) sublist to output list\n",
    "        output_rand_lists.append(sublist_rand_indices)\n",
    "        \n",
    "    return output_rand_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7a42b-8a02-4fa6-a3f1-1248bd28919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_row_names():\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    OUTPUT: List of strings\n",
    "    DESCRIPTION: Creates the row names corresponding to the different datasets included in the make_gridsearch_tbl \n",
    "    function\n",
    "    \"\"\"\n",
    "    ls_row_names = []\n",
    "    ls_types = ['imp_type']\n",
    "    ls_subjects = ['MS_', 'ALL_']\n",
    "    \n",
    "    for str3 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            for str2 in ['t1_', 't2_', 't3_']:\n",
    "                name = str1 + str2 + str3\n",
    "                ls_row_names.append(name)\n",
    "    for str2 in ls_types:\n",
    "        for str1 in ls_subjects:\n",
    "            name = str1 + str2\n",
    "            ls_row_names.append(name)   \n",
    "    \n",
    "    return ls_row_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b7bff-ba44-48da-96e0-db65bdbe9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gridsearch_tbl(time_RI, comp_RI, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (nested lists of arrays) arrays AARI float for time split datasets\n",
    "    comp_RI : (nested lists of arrays) arrays AARI float for time combined datasets\n",
    "    OUTPUT: Dataframe\n",
    "    DESCRIPTION: Create a table (df) with datset names in column 1, best perplexity values in column 2, best \n",
    "    minimum distance value in column 3, best metric value in column 4 and gridsearch AARI scores in column 5. \n",
    "    \"\"\"\n",
    "    # Make row names\n",
    "    dataset_names = make_row_names()\n",
    "    \n",
    "    perplexities = hp_dic['perplexities']\n",
    "    min_dist_values = hp_dic['min_dist']\n",
    "    metrics = hp_dic['metrics']\n",
    "\n",
    "    # Initialize lists to store maximum values, row indices, and column indices\n",
    "    max_values = []\n",
    "    max_perp_idx = [] # rows\n",
    "    max_MD_idx = [] #col1\n",
    "    max_met_idx = [] # col2\n",
    "\n",
    "    # Iterate over time list (time_RI)\n",
    "    for type_list in time_RI:\n",
    "        for array in type_list:\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_values.append(array.max())\n",
    "\n",
    "            max_idx = np.argwhere(array == array.max())\n",
    "            max_perp_idx.append(max_idx[0][0]) # first occurance | row index\n",
    "            max_MD_idx.append(max_idx[0][1]) # first occurance | col1 index\n",
    "            max_met_idx.append(max_idx[0][2]) # first occurance | col2 index)\n",
    "    \n",
    "    # Iterate over combined time list (comp_RI)\n",
    "    for type_list in comp_RI:\n",
    "        for array in type_list:\n",
    "            # Append the maximum value and its indices to the respective lists\n",
    "            max_values.append(array.max())\n",
    "            max_idx = np.argwhere(array == array.max())\n",
    "            max_perp_idx.append(max_idx[0][0]) # first occurance | row index\n",
    "            max_MD_idx.append(max_idx[0][1]) # first occurance | col1 index\n",
    "            max_met_idx.append(max_idx[0][2]) # first occurance | col2 index)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'Dataset': dataset_names,\n",
    "        'Best_Perplexity': [perplexities[i] for i in max_perp_idx],\n",
    "        'Best_Min_Distance': [min_dist_values[j] for j in max_MD_idx],\n",
    "        'Best_Metric': [metrics[k] for k in max_met_idx],\n",
    "        'Best_ARI_Score': max_values})\n",
    "     \n",
    "    output_df.to_excel('updated_data/UMAP/best_gridsearch_per_dataset_tbl.xlsx', index = False)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_UMAP_gridsearch(time_RI, comp_RI, hp_dic):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time split datasets\n",
    "    comp_RI : (Nested lists of floats) nested lists of gridsearch AARI scores for time combined datasets\n",
    "    OUTPUT: 6 figures\n",
    "    DESCRIPTION: Plot the AARI scores for each UMAP gridsearch run. For each figure each row corresponds to a \n",
    "    dataframe (t1, t2, combined) and each column a different metric. There are 2 figures per imputation styles,\n",
    "    corresponding to the pwMS datasets and all subjects dataset. \n",
    "    \"\"\"\n",
    "    # Define the hyperparameters\n",
    "    perplexities = hp_dic['perplexities']\n",
    "    min_dist_values = hp_dic['min_dist']\n",
    "    metrics = hp_dic['metrics']\n",
    "    \n",
    "    # Define the lists to iterate over for plotting & naming. \n",
    "    type_counter = 0\n",
    "    imp_types = ['All Imputation', 'Type Imputation', 'Neighbor Imputation']\n",
    "    file_name = ['all', 'type', 'neighbor']\n",
    "    MS_col_ls = [['#FFC3D7', '#FF90B5', '#EA608D', '#B72253'], \n",
    "                 ['#D7BDE2', '#A569BD', '#7D3C98', '#4A235A'],\n",
    "                 ['#D6EAF8', '#5DADE2', '#2E86C1', '#1B4F72']]\n",
    "    ALL_col_ls = [['#F9E79F', '#F1C40F', '#F8C471', '#F39C12'],\n",
    "                  ['#A9DFBF', '#27AE60', '#1E8449', '#145A32'],\n",
    "                  ['#D6EAF8', '#5DADE2', '#2E86C1', '#1B4F72']]\n",
    "\n",
    "    # Make 6 main figures (imputation types)\n",
    "    for fig_num in range(1, 7):\n",
    "        plt.figure(figsize=(24, 24))\n",
    "\n",
    "        # Define the colors, participants and index range for plotting\n",
    "        coloring = MS_col_ls if fig_num % 2 != 0 else ALL_col_ls \n",
    "        df_type = 'pwMS' if fig_num % 2 != 0 else 'HC + pwMS'\n",
    "        df_ind_range = [0, 0, 0, 1, 1, 1, 0, 0, 0] if fig_num % 2 != 0 else [2, 2, 2, 3, 3, 3, 1, 1, 1]      \n",
    "\n",
    "        # Make 9 main plots (MS & ALL)\n",
    "        for plot_num in range(1, 10):\n",
    "            plt.subplot(3, 3, plot_num)\n",
    "\n",
    "            if plot_num <= 3: #tp1\n",
    "                df_idx = df_ind_range[plot_num - 1]\n",
    "                m_idx = plot_num - 1\n",
    "                plot_title = [df_type, 1, metrics[m_idx]]\n",
    "\n",
    "                for i, md in enumerate(min_dist_values):\n",
    "                    plt.plot(perplexities, time_RI[type_counter][df_idx][:, i, m_idx], \n",
    "                             label=f'min distance={md}', color = coloring[m_idx][i])\n",
    "\n",
    "            elif plot_num > 3 and plot_num <= 6: # tp2\n",
    "                df_idx = df_ind_range[plot_num - 1]\n",
    "                m_idx = plot_num - 4\n",
    "                plot_title = (df_type, 2, metrics[m_idx])\n",
    "\n",
    "                for i, lr in enumerate(min_dist_values):\n",
    "                    plt.plot(perplexities, time_RI[type_counter][df_idx][:, i, m_idx], \n",
    "                         label=f'min distance={md}', color = coloring[m_idx][i])\n",
    "\n",
    "            elif plot_num > 6: # combined\n",
    "                df_idx = df_ind_range[plot_num - 1]\n",
    "                m_idx = plot_num - 7\n",
    "                plot_title = (df_type, 'Combined', metrics[m_idx])\n",
    "\n",
    "                for i, lr in enumerate(min_dist_values):\n",
    "                    plt.plot(perplexities, comp_RI[type_counter][df_idx][:, i, m_idx], \n",
    "                             label=f'min distance={md}', color = coloring[m_idx][i])         \n",
    "\n",
    "            # Add plot visuals\n",
    "            plt.xlabel('Perplexity')\n",
    "            plt.ylabel('Average Adjusted Rand Index')\n",
    "            plt.title(f'{imp_types[type_counter]} for {plot_title[0]} at Time Point {plot_title[1]} and Metric {plot_title[2]}')\n",
    "            plt.legend(title = 'Minimum Distance')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/UMAP/{file_name[type_counter]}/Gridsearch_plots_figure_{file_name[type_counter]}_{plot_title[1]}_{fig_num}.png')\n",
    "        \n",
    "        if fig_num % 2 == 0: # Even figures (Hc + pwMS datasets)\n",
    "            type_counter += 1\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750b474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the hps to use in gridsearch\n",
    "param_dict = {\n",
    "    'perplexities': [2, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "    'min_dist': [0.0, 0.1, 0.5, 0.8],\n",
    "    'metrics': ['euclidean', 'cosine', 'canberra']\n",
    "}\n",
    "\n",
    "# Run the UMAP gridsearch\n",
    "time_gridsearch = UMAP_gridsearch(time_set_ls, param_dict)\n",
    "comp_gridsearch = UMAP_gridsearch(complete_set_ls, param_dict)\n",
    "\n",
    "# Make the table with best hps and corresponding ARI score\n",
    "best_gridsearch_df = make_gridsearch_tbl(time_gridsearch, comp_gridsearch, param_dict)\n",
    "\n",
    "# Plot the gridsearch runs\n",
    "plot_UMAP_gridsearch(time_gridsearch, comp_gridsearch, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e8b41",
   "metadata": {},
   "source": [
    "# Step 3: Apply UMAP with Gridsearch Results (+ plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2f132-362e-43af-b555-731bb96e4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_plot_UMAP(time_arrays, time_ls, comp_arrays, comp_ls, best_GS_df):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    time_arrays : (nested lists of arrays) nested lists with arrays of the UMAP embeddings for the time seperated datasets\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the time seperated datasets\n",
    "    comp_arrays : (nested lists of arrays) nested lists with arrays of the UMAP embeddings for the time combined datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the time combined datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    OUTPUT: 3 figures of 3 by 2 subplots\n",
    "    DESCRIPTION: creates 2 dimensional plots of the UMAP embedded dataframes\n",
    "    \"\"\"\n",
    "    # Make list of file names for saving, and list to order the plots within the figure\n",
    "    file_name = ['all_imp', 'type_imp', 'neighbor_imp']\n",
    "    ordered_ls = [0,1,0,2,3,1]\n",
    "    GS_ints = [0,1,12,2,3,13]\n",
    "    \n",
    "    # Make n main figures (imputation types)\n",
    "    for fig_num in range(1, len(time_ls) + 1):\n",
    "        plt.figure(figsize=(24, 12))\n",
    "\n",
    "        # Make m main plots\n",
    "        num_plots = len(time_arrays[0]) + len(comp_arrays[0])\n",
    "        for plot_num, df_ind in enumerate(ordered_ls):\n",
    "            plt.subplot(2, int(num_plots/2), plot_num + 1)\n",
    "            \n",
    "            # Assign a df and array to plot\n",
    "            label_df = comp_ls[fig_num - 1][df_ind] if plot_num == len(GS_ints)/2 - 1 or plot_num == len(GS_ints) - 1 else time_ls[fig_num - 1][df_ind]\n",
    "            plotting_array = comp_arrays[fig_num - 1][df_ind] if plot_num == len(GS_ints)/2 - 1 or plot_num == len(GS_ints) - 1 else time_arrays[fig_num - 1][df_ind]\n",
    "            \n",
    "            # Define the plot colours & label colum\n",
    "            label_col = [col for col in label_df.columns if col.startswith('MS')]\n",
    "            color_map = {0: 'pink', 1: 'orange', 2: 'purple'} if label_col[0] == 'MStype' else {0: 'green', 1: 'purple'}\n",
    "            legend_labels = {0: 'PPMS', 1: 'SPMS', 2: 'RRMS'} if label_col[0] == 'MStype' else {0: 'HC', 1: 'MS'}\n",
    "            mapped_colors = label_df['MStype'].map(color_map) if label_col[0] == 'MStype' else label_df['MS'].map(color_map)  \n",
    "            \n",
    "            # Make the plots\n",
    "            for category, color in color_map.items():\n",
    "                indices = label_df[label_col[0]] == category\n",
    "                plt.scatter(plotting_array[indices, 0], plotting_array[indices, 1], \n",
    "                            c = color, label = legend_labels[category], alpha=0.7)\n",
    "            \n",
    "            # Make plot labels\n",
    "            df_name = best_GS_df.iloc[GS_ints[plot_num], 0]\n",
    "            perplexity = best_GS_df.iloc[GS_ints[plot_num], 1]\n",
    "            min_distance = best_GS_df.iloc[GS_ints[plot_num], 2]\n",
    "            metric = best_GS_df.iloc[GS_ints[plot_num], 3]\n",
    "            plt.xlabel('UMAP Component 1')\n",
    "            plt.ylabel('UMAP Component 2')\n",
    "            plt.title(f'{df_name} (Perplexity={perplexity}, Min Distance={min_distance}, Metric={metric})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)  \n",
    "        \n",
    "        GS_ints = [x + 2 if i == len(GS_ints)/2 - 1 or i == len(GS_ints) - 1 else x + 4 for i, x in enumerate(GS_ints)]\n",
    "\n",
    "        # Make figures and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/UMAP/{file_name[fig_num - 1]}/best_param_UMAP_plots_{file_name[fig_num - 1]}_multicolor.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ca4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_UMAP(time_ls, comp_ls, best_GS_df, plot_param):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    time_ls : (nested lists of dataframes) nested lists with dataframe for the unique sessions datasets\n",
    "    comp_ls : (nested lists of dataframes) nested lists with dataframe for the combined sessions datasets\n",
    "    best_GS_df : (dataframe) dataframe of the gridsearch outcomes\n",
    "    plot_param : (Boolean) True/False make a 2-dimensional plot of the UMAP embeddings\n",
    "    OUTPUT: 2 lists of nested dfs, 6 lists of nested floats\n",
    "    DESCRIPTION: Applies tSNE fitting to each of the dataframes in the given list of nested dataframes, based on\n",
    "    its optimal perplexity and learning rate values. Plots the ouput arrays of the UMAP fittings if plot_param is\n",
    "    True.\n",
    "    \"\"\"\n",
    "    # Initialise output lists\n",
    "    output_time_ls, output_comp_ls = [], []\n",
    "    \n",
    "    # Needed to iterate through best_GS_df \n",
    "    counter = 0\n",
    "\n",
    "    # Iterate through the unique sessions dataset\n",
    "    for sublist in time_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, perplexity, learning rate and label column name for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            perplexity = best_GS_df.iloc[counter, 1]\n",
    "            min_dist = best_GS_df.iloc[counter, 2]\n",
    "            metric = best_GS_df.iloc[counter, 3]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "\n",
    "            # Run the UMAP model\n",
    "            umap_model = UMAP(n_components = 2, n_neighbors = perplexity, min_dist = min_dist, \n",
    "                              metric = metric, random_state = 42)\n",
    "            umap_array = umap_model.fit_transform(norm_df)\n",
    " \n",
    "            type_list.append(umap_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        output_time_ls.append(type_list)\n",
    "        \n",
    "\n",
    "    for sublist in comp_ls:\n",
    "        type_list = []\n",
    "        \n",
    "        for df in sublist:\n",
    "            # Get name, perplexity, learning rate and label column name for the df\n",
    "            df_name = best_GS_df.iloc[counter, 0]\n",
    "            perplexity = best_GS_df.iloc[counter, 1]\n",
    "            min_dist = best_GS_df.iloc[counter, 2]\n",
    "            metric = best_GS_df.iloc[counter, 3]\n",
    "            label_col = [col for col in df.columns if col.startswith('MS')]\n",
    "\n",
    "            # Remove target variables and normalise the df\n",
    "            norm_df = df.drop(columns = ['EDSS', 'BL_Avg_cognition', 'index'] + label_col , axis = 1)\n",
    "            norm_df = StandardScaler().fit_transform(norm_df) \n",
    "\n",
    "            # Run the UMAP model\n",
    "            umap_model = UMAP(n_components = 2, n_neighbors = perplexity, min_dist = min_dist, \n",
    "                              metric = metric, random_state = 42)\n",
    "            umap_array = umap_model.fit_transform(norm_df)\n",
    "\n",
    "            type_list.append(umap_array)\n",
    "\n",
    "            # Update the counter\n",
    "            counter += 1\n",
    "\n",
    "        output_comp_ls.append(type_list)\n",
    "                       \n",
    "    # Plotting condition (if true, plots are generated)\n",
    "    if plot_param:\n",
    "        group_plot_UMAP(output_time_ls, time_ls, output_comp_ls, comp_ls, best_GS_df)\n",
    "     \n",
    "    return output_time_ls, output_comp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297cfcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run step 3 (apply_UMAP)\n",
    "time_UMAP_arrays, comp_UMAP_arrays = apply_UMAP(time_set_ls, complete_set_ls, best_gridsearch_df, True)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a2625-c171-4c5f-8646-a54837d74fc6",
   "metadata": {},
   "source": [
    "# Step 4: Save the UMAP embedded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6b50c-3b2d-496e-8649-307a3f6e062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_UMAP_embedings(ls_ls_umap_array, ls_ls_matching_df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    ls_ls_umap_array : (nested list of np.array) nested list of array of the UMAP embedding\n",
    "    ls_ls_matching_df : (nested list of pd.dataframe) nested list of original pre embedding dataframe\n",
    "    OUTPUT:\n",
    "    DESCRIPTION: Exports the tSNE embeddings as dataframes with the same index/ subjects ID as their original dataset. \n",
    "    \"\"\"\n",
    "    imp_file = ['all', 'type', 'neighbor']\n",
    "    imp_df = ['ia', 'it', 'in']\n",
    "\n",
    "    for imp_idx, imp_ls in enumerate(ls_ls_umap_array):\n",
    "        for emb_idx, umap_emb in enumerate(imp_ls):\n",
    "            # Make a dataframe from the array\n",
    "            output_df = pd.DataFrame(umap_emb, columns = [f'UMAP{i+1}' for i in range(umap_emb.shape[1])])\n",
    "\n",
    "            # Reintroduce the participant ID (index)\n",
    "            output_df['index'] = ls_ls_matching_df[imp_idx][emb_idx]['index'].reset_index(drop = True)\n",
    "            columns = ['index'] + [col for col in output_df.columns if col != 'index']\n",
    "            \n",
    "            # Reorder the columns such that index is first\n",
    "            output_df = output_df[columns]\n",
    "\n",
    "            # Check for time split list or not\n",
    "            if len(ls_ls_matching_df[0]) > 3:\n",
    "                sub_type = 'MStrain_' if emb_idx < 2 else 'ALLtrain_'\n",
    "                year = '00' if emb_idx % 2 == 0 else '05'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/UMAP/{imp_file[imp_idx]}/UMAP_{sub_type}{imp_df[imp_idx]}{year}.xlsx', index=False)\n",
    "            \n",
    "            else:\n",
    "                sub_type = 'MStrain_' if emb_idx == 0 else 'ALLtrain_'\n",
    "\n",
    "                # Save the new df as an excel file\n",
    "                output_df.to_excel(f'output/UMAP/{imp_file[imp_idx]}/UMAP_{sub_type}{imp_df[imp_idx]}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9346a10-9572-4cfd-88de-f50dee1c2e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the export_UMAP_embedings function for the unique sessions and the combined sessions.\n",
    "export_UMAP_embedings(time_UMAP_arrays, time_set_ls)\n",
    "export_UMAP_embedings(comp_UMAP_arrays, complete_set_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
